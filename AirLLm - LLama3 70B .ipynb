{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b580995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLama3-70B-Instruct with AirLLM\n",
    "AirLLM allows to load a 70B parameters model in a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c2b73b-b5e5-45fe-bf98-6ee123bc3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlx airllm\n",
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024ab525-b587-48ce-8548-9b0964ab2e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bitsandbytes\n",
      "Version: 0.43.1\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/TimDettmers/bitsandbytes\n",
      "Author: Tim Dettmers\n",
      "Author-email: dettmers@cs.washington.edu\n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: numpy, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ddb2f7-276b-4c04-9af8-cb6b54a6d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for carblacac/twitter-sentiment-analysis contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/carblacac/twitter-sentiment-analysis\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6f96beee4544e8afe05517527968f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef503ceb8fb54b7ab126f89b755fe3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d29a252b9c4e9c87881ed0650b7656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a036c3fc824107b62101989319d5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fbff96809346cea067b8745da3cf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2598728bf7844059f866cbb4845aa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76e1bac07ee4a3dafda3ff41a9e2b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5546325104b64261a2d0ba38883ef6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/149985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6034d278b9441339b1c452745e71195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621637a11b9d4e78acc6a7cdb760e483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb5dc6b505f409fa23d30fb5c7a5655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb4adc4ddb84cc2b4c1216516cb2eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72a6402bae54501845c812bc5ba48a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/119988 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e27895fd8514f279c42721de86e942c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/29997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c63cbbd9f94c2c96e6a481fb71d246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/61998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"carblacac/twitter-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df14ce5a-023c-4706-a02e-b445099184a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type=\"torch\", columns=[\"text\", \"feeling\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38a8c899-4005-4625-b975-ae7bc1c69f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'feeling'],\n",
       "        num_rows: 119988\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'feeling'],\n",
       "        num_rows: 29997\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'feeling'],\n",
       "        num_rows: 61998\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c87c46de-f529-4298-9051-1859da80f0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29997, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset['validation'].to_pandas()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518afcdf-bb53-4b06-b421-24f8a92de036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "908a98a9-7429-467f-ab2c-c85edc56cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST] Using the provided text below, perform a sentiment analysis of the text. \\\n",
    "Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. \\\n",
    "Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
    "Reply only with the sentiment analysis, no other comment.\n",
    "\\n\n",
    "********** BEGIN TEXT **********\n",
    "{text}\n",
    "********** END TEXT **********\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7af1b8b-40e1-4f58-a5f7-fd8b6b889c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29997"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = df.text.values.tolist()\n",
    "\n",
    "prompts = [prompt_template.format(text=p) for p in batch]\n",
    "\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58674b26-c7db-42c2-ae5b-19612c78a7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Using the provided text below, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment analysis, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nis feeling like Monday morning....ugh\\n********** END TEXT **********\\n[/INST]\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e60202-01ac-4425-a480-d72baaed160f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb3910ec-3e92-41a4-9f0e-592b5a17b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> bitsandbytes installed\n",
      ">>>> cache_utils installed\n"
     ]
    }
   ],
   "source": [
    "from airllm import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b5fdd91-57fc-4723-949c-020b151a60d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f050025031048669b04b429f86ec1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/83 [00:01<01:46,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.embed_tokens.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/83 [00:02<01:32,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.0.safetensors\n",
      "Loading shard 2/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4760635ea748559c4577e6ca05bf77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150217dbdb9f4c2eb7dd33eb324fb3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.1.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4/83 [01:26<31:26, 23.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.2.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/83 [01:27<20:17, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.3.safetensors\n",
      "Loading shard 3/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c97cbdb48044c78d6e09b54ceb35d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c6a2e3c99a4a22b7d82c640072f516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/83 [05:44<2:05:00, 97.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.4.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 7/83 [05:45<1:23:26, 65.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.5.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 8/83 [05:46<56:30, 45.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.6.safetensors\n",
      "Loading shard 4/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c835d4ee8ae4146be878bcef4932826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d07425935347a1a4adbf41c10e18c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 9/83 [07:33<1:19:44, 64.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.7.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 10/83 [07:34<54:44, 44.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.8.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 11/83 [07:35<37:50, 31.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.9.safetensors\n",
      "Loading shard 5/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39b504126e54e39925c11c2000687d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cada8f81c74122bcc7d8e6548284fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 12/83 [09:21<1:04:05, 54.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.10.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 13/83 [09:22<44:22, 38.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.11.safetensors\n",
      "Loading shard 6/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdce873ccc34a3183b1f4b22d10d07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4810fe5c994ad291cfe98757de1881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 14/83 [11:05<1:06:23, 57.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.12.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 15/83 [11:06<46:02, 40.63s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.13.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 16/83 [11:07<32:01, 28.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.14.safetensors\n",
      "Loading shard 7/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d4dc1dc612426db66756b083210197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b027b4026b1c4b99b3f4b313053669eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 17/83 [12:51<56:31, 51.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.15.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 18/83 [12:52<39:15, 36.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.16.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 19/83 [12:53<27:21, 25.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.17.safetensors\n",
      "Loading shard 8/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07348f51bb6f413bbb11b8c2fb2dd796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d22a3a305a4ea1bd416c50d3648865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 20/83 [18:06<1:57:37, 112.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.18.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 21/83 [18:07<1:21:19, 78.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.19.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 22/83 [18:08<56:17, 55.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.20.safetensors\n",
      "Loading shard 9/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0693b7ca9b424d84be8a8b2d3f2272ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cafea5762d4182b91941ba7578b8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.21.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 24/83 [19:50<47:45, 48.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.22.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 25/83 [19:51<33:09, 34.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.23.safetensors\n",
      "Loading shard 10/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e8e3a60e7044efa205346409938672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a565b00e114b9680819d9ad428a0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 26/83 [21:27<50:04, 52.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.24.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 27/83 [21:28<34:42, 37.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.25.safetensors\n",
      "Loading shard 11/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62edd077f12470b90b17b6c7234c34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d327af8c0bc14313a67d5c754fae2a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 28/83 [26:32<1:47:32, 117.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.26.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 29/83 [26:33<1:14:10, 82.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.27.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 30/83 [26:34<51:13, 57.98s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.28.safetensors\n",
      "Loading shard 12/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b80602c0734cbe8f3659fcbd1c8518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa6e8543a3a41a58d328fdd44b4528b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 31/83 [28:06<59:09, 68.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.29.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 32/83 [28:07<40:51, 48.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.30.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 33/83 [28:08<28:17, 33.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.31.safetensors\n",
      "Loading shard 13/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d4b10bfaee4936bdead7216b36e6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5738685145e84e429f902f6fa1601f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 37/83 [32:04<45:21, 59.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.35.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 38/83 [32:05<31:16, 41.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.36.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 39/83 [32:06<21:38, 29.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.37.safetensors\n",
      "Loading shard 15/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a243ddf05e44c8fb0bd159666e75455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dc087351684464a143a65ddc12fc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 40/83 [33:40<35:07, 49.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.38.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 41/83 [33:41<24:12, 34.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.39.safetensors\n",
      "Loading shard 16/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89894100a3b84e518ca565e2c40cfa33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db43475f684b4b6d870019d3bb61f085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 42/83 [35:13<35:19, 51.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.40.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 43/83 [35:14<24:19, 36.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.41.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 44/83 [35:15<16:47, 25.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.42.safetensors\n",
      "Loading shard 17/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bde84b98ed24a7fa6838bf1a0cf2b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fd108745824da9bd27787c5f26450d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 75%|███████▍  | 62/83 [47:25<14:48, 42.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.60.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 63/83 [47:26<09:57, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.61.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 64/83 [47:27<06:43, 21.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.62.safetensors\n",
      "Loading shard 24/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270b488a0464479f8bfe9a4b75f9d18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fa27fd040544999b154510b368792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 65/83 [49:59<18:06, 60.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.63.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 66/83 [50:00<12:02, 42.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.64.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 67/83 [50:01<08:01, 30.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.65.safetensors\n",
      "Loading shard 25/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba89ebba4a10442db3181161381c07a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99942a359be447aa971fb427b070eab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 68/83 [51:29<11:55, 47.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.66.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 69/83 [51:30<07:51, 33.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.67.safetensors\n",
      "Loading shard 26/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538eae34d3c7494ea16d806ae9cc8e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6bd9b27f0c4af69769f7911b7b5eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 70/83 [53:10<11:33, 53.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.68.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 71/83 [53:11<07:31, 37.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.69.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 72/83 [53:12<04:53, 26.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.70.safetensors\n",
      "Loading shard 27/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc76987ece0b42c4bb03f14ed310e4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd734954f3954e3391bd9dac07f8d2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 73/83 [54:50<08:01, 48.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.71.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 74/83 [54:51<05:05, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.72.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 75/83 [54:52<03:12, 24.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.73.safetensors\n",
      "Loading shard 28/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f741579fab470897b070f8b746845c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db676b2132eb48468d1ddd4c6b46f8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 76/83 [58:47<10:12, 87.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.74.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 77/83 [58:48<06:09, 61.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.75.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 78/83 [58:49<03:36, 43.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.76.safetensors\n",
      "Loading shard 29/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2677b5cacc41228ba4d705ff66e14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d076db662fd48ffb2d99eef1bf8cfb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.77.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 80/83 [1:00:25<02:04, 41.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.78.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 81/83 [1:00:26<00:58, 29.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.layers.79.safetensors\n",
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/model.norm.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 82/83 [1:00:26<00:20, 20.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 30/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bbcbb787af4225947cdb38ba19ef1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14c9a59184b42e8a5181d3e57c198cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00030.safetensors:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [1:04:24<00:00, 46.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B-Instruct/snapshots/db240881eba571f1f2422f23b40cb57c2f7e7511/splitted_model.4bit/lm_head.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n",
      "not support prefetching for compression for now. loading with no prepetching mode.\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "model = AutoModel.from_pretrained(\"v2ray/Llama-3-70B-Instruct\", compression='4bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4920feb9-8c04-4f05-8d6c-3220f74eccdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mis_on_mac_os\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mAirLLMLlamaMlx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.10/dist-packages/airllm/auto_model.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??AutoModel.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750a2330-f2e4-4966-b5b4-dffb5402a141",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AirLLMLlama2' object has no attribute 'get_memory_footprint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel memory footprint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_memory_footprint\u001b[49m()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AirLLMLlama2' object has no attribute 'get_memory_footprint'"
     ]
    }
   ],
   "source": [
    "print(f\"Model memory footprint: {model.get_memory_footprint()/1e9:.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22480230-9126-4c66-abc8-9cc6610d0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.pad_token = model.tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "model.tokenizer.padding_side = \"left\"\n",
    "model.tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5d8de-a804-48a5-8723-6f9ad3e06e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729188c-de62-422f-91ab-3f782334e212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5fa4621a-f024-4af0-b203-8ff361b1c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "def generate_token(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0] if type(outputs)==tuple else outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id\n",
    "\n",
    "def generate_tokens(inputs, n_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    durations_s = []\n",
    "    for _ in range(n_tokens):\n",
    "        t0 = time.time()\n",
    "        next_token_id = generate_token(next_inputs)\n",
    "        durations_s += [time.time() - t0]\n",
    "    \n",
    "        next_inputs = {\n",
    "            \"input_ids\": torch.cat(\n",
    "                [next_inputs[\"input_ids\"], next_token_id.reshape((1, 1))],\n",
    "                dim=1),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]], device=device)],\n",
    "                dim=1),\n",
    "        }\n",
    "    \n",
    "        next_token = model.tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return generated_tokens, durations_s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b83e93fb-52ce-4b0b-9ba5-6a5e40554069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Using the provided text below, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment analysis, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nis feeling like Monday morning....ugh\\n********** END TEXT **********\\n[/INST]\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "769a4a24-b178-4476-9922-c4d4a7acbc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 3 µs, total: 7 µs\n",
      "Wall time: 13.1 µs\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 83/83 [00:21<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 83/83 [00:22<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 83/83 [00:22<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 83/83 [00:21<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.691831350326538, 23.22546935081482, 23.511146068572998, 22.620964765548706]\n",
      "['Sent', 'iment', ':', ' Negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs = model.tokenizer(prompts[0], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "tokens, durations_cached = generate_tokens(inputs, 4)\n",
    "\n",
    "print(f\"{durations_cached}\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a841a2a-d174-4144-b093-b17f1d4751dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea5902-db5b-41f4-8671-e3ba1c4f861c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97947a7-eeeb-40c3-82ba-120f050b6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c6d909-0bf1-41d5-b81c-ea26ca47944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.pad_token = model.tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "model.tokenizer.padding_side = \"left\"\n",
    "model.tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426ee45-8a9c-438c-922f-684091f29acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d74de7c8-d9bc-4a62-844b-62eed4b828b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([29997, 438]), torch.Size([29997, 438]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].size(), inputs['attention_mask'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17edf578-9065-4f8b-98a5-3d6bfd82df07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29997"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c14a933a-9c67-416c-aca2-dbe0a8811b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 102]), torch.Size([2, 102]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = model.tokenizer(prompts[:2], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "inputs['input_ids'].size(), inputs[\"attention_mask\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "596216c6-eabb-4f5c-ab25-d8bfd2854d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 83/83 [00:22<00:00,  3.74it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28d643b4-ce57-4ae6-87c4-876f97d7644a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 102, 128256])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "69c81df9-9a29-4dad-8471-88e3b7958eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs[0] if type(outputs)==tuple else outputs.logits\n",
    "\n",
    "    #logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids\n",
    "\n",
    "\n",
    "def generate_batch(inputs, max_tokens):\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids = \\\n",
    "            generate_batch_tokens_with_past(next_inputs)\n",
    "\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"],\n",
    "                torch.ones((next_token_ids.shape[0], 1), device=device),  \n",
    "            ], dim=1),\n",
    "            #\"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_tokens = model.tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73aa775e-a81e-4813-8fa0-5b326cef4825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed the random number generator so our results are deterministic\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 10\n",
    "batch_size = 10\n",
    "max_tokens = 4\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[i], max_tokens)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "batches = [\n",
    "    request_queue[i:i + batch_size]\n",
    "    for i in range(0, len(request_queue), batch_size)\n",
    "]\n",
    "\n",
    "len(request_queue), len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e1fe31c8-0786-4fe0-bc03-391811966ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8730e929a184626be07fa46596882bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bs=10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "running layers(self.running_device):   0%|          | 0/83 [00:00<?, ?it/s]\u001b[A\n",
      "running layers(self.running_device):   1%|          | 1/83 [00:00<00:21,  3.85it/s]\u001b[A\n",
      "running layers(self.running_device):   2%|▏         | 2/83 [00:00<00:23,  3.52it/s]\u001b[A\n",
      "running layers(self.running_device):   4%|▎         | 3/83 [00:00<00:22,  3.58it/s]\u001b[A\n",
      "running layers(self.running_device):   5%|▍         | 4/83 [00:01<00:21,  3.64it/s]\u001b[A\n",
      "running layers(self.running_device):   6%|▌         | 5/83 [00:01<00:21,  3.66it/s]\u001b[A\n",
      "running layers(self.running_device):   7%|▋         | 6/83 [00:01<00:20,  3.68it/s]\u001b[A\n",
      "running layers(self.running_device):   8%|▊         | 7/83 [00:01<00:20,  3.67it/s]\u001b[A\n",
      "running layers(self.running_device):  10%|▉         | 8/83 [00:02<00:20,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  11%|█         | 9/83 [00:02<00:20,  3.68it/s]\u001b[A\n",
      "running layers(self.running_device):  12%|█▏        | 10/83 [00:02<00:19,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  13%|█▎        | 11/83 [00:02<00:19,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  14%|█▍        | 12/83 [00:03<00:19,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  16%|█▌        | 13/83 [00:03<00:18,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  17%|█▋        | 14/83 [00:03<00:18,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  18%|█▊        | 15/83 [00:04<00:18,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  19%|█▉        | 16/83 [00:04<00:18,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  20%|██        | 17/83 [00:04<00:17,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  22%|██▏       | 18/83 [00:04<00:17,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  23%|██▎       | 19/83 [00:05<00:17,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  24%|██▍       | 20/83 [00:05<00:16,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  25%|██▌       | 21/83 [00:05<00:16,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  27%|██▋       | 22/83 [00:05<00:16,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  28%|██▊       | 23/83 [00:06<00:16,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  29%|██▉       | 24/83 [00:06<00:15,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  30%|███       | 25/83 [00:06<00:15,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  31%|███▏      | 26/83 [00:07<00:15,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  33%|███▎      | 27/83 [00:07<00:15,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  34%|███▎      | 28/83 [00:07<00:14,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  35%|███▍      | 29/83 [00:07<00:14,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  36%|███▌      | 30/83 [00:08<00:14,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  37%|███▋      | 31/83 [00:08<00:14,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  39%|███▊      | 32/83 [00:08<00:13,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  40%|███▉      | 33/83 [00:08<00:13,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  41%|████      | 34/83 [00:09<00:13,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  42%|████▏     | 35/83 [00:09<00:12,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  43%|████▎     | 36/83 [00:09<00:12,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  45%|████▍     | 37/83 [00:09<00:12,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  46%|████▌     | 38/83 [00:10<00:12,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  47%|████▋     | 39/83 [00:10<00:11,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  48%|████▊     | 40/83 [00:10<00:11,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  49%|████▉     | 41/83 [00:11<00:11,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  51%|█████     | 42/83 [00:11<00:11,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  52%|█████▏    | 43/83 [00:11<00:10,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  53%|█████▎    | 44/83 [00:11<00:10,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  54%|█████▍    | 45/83 [00:12<00:10,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  55%|█████▌    | 46/83 [00:12<00:09,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  57%|█████▋    | 47/83 [00:12<00:09,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  58%|█████▊    | 48/83 [00:12<00:09,  3.80it/s]\u001b[A\n",
      "running layers(self.running_device):  59%|█████▉    | 49/83 [00:13<00:08,  3.81it/s]\u001b[A\n",
      "running layers(self.running_device):  60%|██████    | 50/83 [00:13<00:08,  3.83it/s]\u001b[A\n",
      "running layers(self.running_device):  61%|██████▏   | 51/83 [00:13<00:08,  3.82it/s]\u001b[A\n",
      "running layers(self.running_device):  63%|██████▎   | 52/83 [00:13<00:08,  3.83it/s]\u001b[A\n",
      "running layers(self.running_device):  64%|██████▍   | 53/83 [00:14<00:07,  3.82it/s]\u001b[A\n",
      "running layers(self.running_device):  65%|██████▌   | 54/83 [00:14<00:07,  3.80it/s]\u001b[A\n",
      "running layers(self.running_device):  66%|██████▋   | 55/83 [00:14<00:07,  3.80it/s]\u001b[A\n",
      "running layers(self.running_device):  67%|██████▋   | 56/83 [00:15<00:07,  3.80it/s]\u001b[A\n",
      "running layers(self.running_device):  69%|██████▊   | 57/83 [00:15<00:06,  3.81it/s]\u001b[A\n",
      "running layers(self.running_device):  70%|██████▉   | 58/83 [00:15<00:06,  3.84it/s]\u001b[A\n",
      "running layers(self.running_device):  71%|███████   | 59/83 [00:15<00:06,  3.82it/s]\u001b[A\n",
      "running layers(self.running_device):  72%|███████▏  | 60/83 [00:16<00:05,  3.85it/s]\u001b[A\n",
      "running layers(self.running_device):  73%|███████▎  | 61/83 [00:16<00:05,  3.85it/s]\u001b[A\n",
      "running layers(self.running_device):  75%|███████▍  | 62/83 [00:16<00:05,  3.87it/s]\u001b[A\n",
      "running layers(self.running_device):  76%|███████▌  | 63/83 [00:16<00:05,  3.86it/s]\u001b[A\n",
      "running layers(self.running_device):  77%|███████▋  | 64/83 [00:17<00:04,  3.87it/s]\u001b[A\n",
      "running layers(self.running_device):  78%|███████▊  | 65/83 [00:17<00:04,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  80%|███████▉  | 66/83 [00:17<00:04,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  81%|████████  | 67/83 [00:17<00:04,  3.65it/s]\u001b[A\n",
      "running layers(self.running_device):  82%|████████▏ | 68/83 [00:18<00:04,  3.65it/s]\u001b[A\n",
      "running layers(self.running_device):  83%|████████▎ | 69/83 [00:18<00:03,  3.63it/s]\u001b[A\n",
      "running layers(self.running_device):  84%|████████▍ | 70/83 [00:18<00:03,  3.65it/s]\u001b[A\n",
      "running layers(self.running_device):  86%|████████▌ | 71/83 [00:19<00:03,  3.65it/s]\u001b[A\n",
      "running layers(self.running_device):  87%|████████▋ | 72/83 [00:19<00:02,  3.67it/s]\u001b[A\n",
      "running layers(self.running_device):  88%|████████▊ | 73/83 [00:19<00:02,  3.67it/s]\u001b[A\n",
      "running layers(self.running_device):  89%|████████▉ | 74/83 [00:19<00:02,  3.60it/s]\u001b[A\n",
      "running layers(self.running_device):  90%|█████████ | 75/83 [00:20<00:02,  3.52it/s]\u001b[A\n",
      "running layers(self.running_device):  92%|█████████▏| 76/83 [00:20<00:02,  3.49it/s]\u001b[A\n",
      "running layers(self.running_device):  93%|█████████▎| 77/83 [00:20<00:01,  3.45it/s]\u001b[A\n",
      "running layers(self.running_device):  94%|█████████▍| 78/83 [00:21<00:01,  3.44it/s]\u001b[A\n",
      "running layers(self.running_device):  95%|█████████▌| 79/83 [00:21<00:01,  3.43it/s]\u001b[A\n",
      "running layers(self.running_device):  96%|█████████▋| 80/83 [00:21<00:00,  3.43it/s]\u001b[A\n",
      "running layers(self.running_device):  98%|█████████▊| 81/83 [00:21<00:00,  3.42it/s]\u001b[A\n",
      "running layers(self.running_device):  99%|█████████▉| 82/83 [00:22<00:00,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device): 100%|██████████| 83/83 [00:22<00:00,  3.71it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "running layers(self.running_device):   0%|          | 0/83 [00:00<?, ?it/s]\u001b[A\n",
      "running layers(self.running_device):   1%|          | 1/83 [00:00<00:21,  3.83it/s]\u001b[A\n",
      "running layers(self.running_device):   2%|▏         | 2/83 [00:00<00:24,  3.24it/s]\u001b[A\n",
      "running layers(self.running_device):   4%|▎         | 3/83 [00:00<00:23,  3.43it/s]\u001b[A\n",
      "running layers(self.running_device):   5%|▍         | 4/83 [00:01<00:22,  3.56it/s]\u001b[A\n",
      "running layers(self.running_device):   6%|▌         | 5/83 [00:01<00:21,  3.62it/s]\u001b[A\n",
      "running layers(self.running_device):   7%|▋         | 6/83 [00:01<00:20,  3.67it/s]\u001b[A\n",
      "running layers(self.running_device):   8%|▊         | 7/83 [00:01<00:20,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  10%|▉         | 8/83 [00:02<00:20,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  11%|█         | 9/83 [00:02<00:19,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  12%|█▏        | 10/83 [00:02<00:19,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  13%|█▎        | 11/83 [00:02<00:19,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  14%|█▍        | 12/83 [00:03<00:18,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  16%|█▌        | 13/83 [00:03<00:18,  3.79it/s]\u001b[A\n",
      "running layers(self.running_device):  17%|█▋        | 14/83 [00:03<00:18,  3.79it/s]\u001b[A\n",
      "running layers(self.running_device):  18%|█▊        | 15/83 [00:04<00:17,  3.79it/s]\u001b[A\n",
      "running layers(self.running_device):  19%|█▉        | 16/83 [00:04<00:17,  3.79it/s]\u001b[A\n",
      "running layers(self.running_device):  20%|██        | 17/83 [00:04<00:17,  3.79it/s]\u001b[A\n",
      "running layers(self.running_device):  22%|██▏       | 18/83 [00:04<00:17,  3.79it/s]\u001b[A\n",
      "running layers(self.running_device):  23%|██▎       | 19/83 [00:05<00:16,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  24%|██▍       | 20/83 [00:05<00:16,  3.79it/s]\u001b[A\n",
      "running layers(self.running_device):  25%|██▌       | 21/83 [00:05<00:16,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  27%|██▋       | 22/83 [00:05<00:16,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  28%|██▊       | 23/83 [00:06<00:15,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  29%|██▉       | 24/83 [00:06<00:15,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  30%|███       | 25/83 [00:06<00:15,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  31%|███▏      | 26/83 [00:06<00:15,  3.79it/s]\u001b[A\n",
      "running layers(self.running_device):  33%|███▎      | 27/83 [00:07<00:14,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  34%|███▎      | 28/83 [00:07<00:14,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  35%|███▍      | 29/83 [00:07<00:14,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  36%|███▌      | 30/83 [00:08<00:14,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  37%|███▋      | 31/83 [00:08<00:13,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  39%|███▊      | 32/83 [00:08<00:13,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  40%|███▉      | 33/83 [00:08<00:13,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  41%|████      | 34/83 [00:09<00:12,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  42%|████▏     | 35/83 [00:09<00:12,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  43%|████▎     | 36/83 [00:09<00:12,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  45%|████▍     | 37/83 [00:09<00:12,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  46%|████▌     | 38/83 [00:10<00:11,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  47%|████▋     | 39/83 [00:10<00:11,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  48%|████▊     | 40/83 [00:10<00:11,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  49%|████▉     | 41/83 [00:10<00:11,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  51%|█████     | 42/83 [00:11<00:10,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  52%|█████▏    | 43/83 [00:11<00:10,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  53%|█████▎    | 44/83 [00:11<00:10,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  54%|█████▍    | 45/83 [00:11<00:10,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  55%|█████▌    | 46/83 [00:12<00:09,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  57%|█████▋    | 47/83 [00:12<00:09,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  58%|█████▊    | 48/83 [00:12<00:09,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  59%|█████▉    | 49/83 [00:13<00:09,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  60%|██████    | 50/83 [00:13<00:08,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  61%|██████▏   | 51/83 [00:13<00:08,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  63%|██████▎   | 52/83 [00:13<00:08,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  64%|██████▍   | 53/83 [00:14<00:08,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  65%|██████▌   | 54/83 [00:14<00:07,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  66%|██████▋   | 55/83 [00:14<00:07,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  67%|██████▋   | 56/83 [00:14<00:07,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  69%|██████▊   | 57/83 [00:15<00:06,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  70%|██████▉   | 58/83 [00:15<00:06,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  71%|███████   | 59/83 [00:15<00:06,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  72%|███████▏  | 60/83 [00:16<00:06,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  73%|███████▎  | 61/83 [00:16<00:05,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  75%|███████▍  | 62/83 [00:16<00:05,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  76%|███████▌  | 63/83 [00:16<00:05,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  77%|███████▋  | 64/83 [00:17<00:05,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  78%|███████▊  | 65/83 [00:17<00:04,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  80%|███████▉  | 66/83 [00:17<00:04,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  81%|████████  | 67/83 [00:17<00:04,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  82%|████████▏ | 68/83 [00:18<00:03,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  83%|████████▎ | 69/83 [00:18<00:03,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  84%|████████▍ | 70/83 [00:18<00:03,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  86%|████████▌ | 71/83 [00:18<00:03,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  87%|████████▋ | 72/83 [00:19<00:02,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  88%|████████▊ | 73/83 [00:19<00:02,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  89%|████████▉ | 74/83 [00:19<00:02,  3.80it/s]\u001b[A\n",
      "running layers(self.running_device):  90%|█████████ | 75/83 [00:19<00:02,  3.80it/s]\u001b[A\n",
      "running layers(self.running_device):  92%|█████████▏| 76/83 [00:20<00:01,  3.81it/s]\u001b[A\n",
      "running layers(self.running_device):  93%|█████████▎| 77/83 [00:20<00:01,  3.81it/s]\u001b[A\n",
      "running layers(self.running_device):  94%|█████████▍| 78/83 [00:20<00:01,  3.83it/s]\u001b[A\n",
      "running layers(self.running_device):  95%|█████████▌| 79/83 [00:21<00:01,  3.83it/s]\u001b[A\n",
      "running layers(self.running_device):  96%|█████████▋| 80/83 [00:21<00:00,  3.84it/s]\u001b[A\n",
      "running layers(self.running_device):  98%|█████████▊| 81/83 [00:21<00:00,  3.83it/s]\u001b[A\n",
      "running layers(self.running_device):  99%|█████████▉| 82/83 [00:21<00:00,  4.30it/s]\u001b[A\n",
      "running layers(self.running_device): 100%|██████████| 83/83 [00:21<00:00,  3.78it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "running layers(self.running_device):   0%|          | 0/83 [00:00<?, ?it/s]\u001b[A\n",
      "running layers(self.running_device):   1%|          | 1/83 [00:00<00:21,  3.89it/s]\u001b[A\n",
      "running layers(self.running_device):   2%|▏         | 2/83 [00:00<00:22,  3.56it/s]\u001b[A\n",
      "running layers(self.running_device):   4%|▎         | 3/83 [00:00<00:21,  3.65it/s]\u001b[A\n",
      "running layers(self.running_device):   5%|▍         | 4/83 [00:01<00:21,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):   6%|▌         | 5/83 [00:01<00:20,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):   7%|▋         | 6/83 [00:01<00:20,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):   8%|▊         | 7/83 [00:01<00:20,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  10%|▉         | 8/83 [00:02<00:20,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  11%|█         | 9/83 [00:02<00:19,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  12%|█▏        | 10/83 [00:02<00:19,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  13%|█▎        | 11/83 [00:02<00:19,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  14%|█▍        | 12/83 [00:03<00:18,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  16%|█▌        | 13/83 [00:03<00:18,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  17%|█▋        | 14/83 [00:03<00:18,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  18%|█▊        | 15/83 [00:04<00:18,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  19%|█▉        | 16/83 [00:04<00:17,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  20%|██        | 17/83 [00:04<00:17,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  22%|██▏       | 18/83 [00:04<00:17,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  23%|██▎       | 19/83 [00:05<00:16,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  24%|██▍       | 20/83 [00:05<00:16,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  25%|██▌       | 21/83 [00:05<00:16,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  27%|██▋       | 22/83 [00:05<00:16,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  28%|██▊       | 23/83 [00:06<00:15,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  29%|██▉       | 24/83 [00:06<00:15,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  30%|███       | 25/83 [00:06<00:15,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  31%|███▏      | 26/83 [00:06<00:15,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  33%|███▎      | 27/83 [00:07<00:14,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  34%|███▎      | 28/83 [00:07<00:14,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  35%|███▍      | 29/83 [00:07<00:14,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  36%|███▌      | 30/83 [00:07<00:14,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  37%|███▋      | 31/83 [00:08<00:13,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  39%|███▊      | 32/83 [00:08<00:13,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  40%|███▉      | 33/83 [00:08<00:13,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  41%|████      | 34/83 [00:09<00:13,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  42%|████▏     | 35/83 [00:09<00:12,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  43%|████▎     | 36/83 [00:09<00:12,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  45%|████▍     | 37/83 [00:09<00:12,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  46%|████▌     | 38/83 [00:10<00:12,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  47%|████▋     | 39/83 [00:10<00:11,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  48%|████▊     | 40/83 [00:10<00:11,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  49%|████▉     | 41/83 [00:10<00:11,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  51%|█████     | 42/83 [00:11<00:11,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  52%|█████▏    | 43/83 [00:11<00:10,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  53%|█████▎    | 44/83 [00:11<00:10,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  54%|█████▍    | 45/83 [00:12<00:10,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  55%|█████▌    | 46/83 [00:12<00:09,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  57%|█████▋    | 47/83 [00:12<00:09,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  58%|█████▊    | 48/83 [00:12<00:09,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  59%|█████▉    | 49/83 [00:13<00:09,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  60%|██████    | 50/83 [00:13<00:08,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  61%|██████▏   | 51/83 [00:13<00:08,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  63%|██████▎   | 52/83 [00:13<00:08,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  64%|██████▍   | 53/83 [00:14<00:08,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  65%|██████▌   | 54/83 [00:14<00:07,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  66%|██████▋   | 55/83 [00:14<00:07,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  67%|██████▋   | 56/83 [00:14<00:07,  3.68it/s]\u001b[A\n",
      "running layers(self.running_device):  69%|██████▊   | 57/83 [00:15<00:07,  3.68it/s]\u001b[A\n",
      "running layers(self.running_device):  70%|██████▉   | 58/83 [00:15<00:06,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  71%|███████   | 59/83 [00:15<00:06,  3.68it/s]\u001b[A\n",
      "running layers(self.running_device):  72%|███████▏  | 60/83 [00:16<00:06,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  73%|███████▎  | 61/83 [00:16<00:05,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  75%|███████▍  | 62/83 [00:16<00:05,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  76%|███████▌  | 63/83 [00:16<00:05,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  77%|███████▋  | 64/83 [00:17<00:05,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  78%|███████▊  | 65/83 [00:17<00:04,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  80%|███████▉  | 66/83 [00:17<00:04,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  81%|████████  | 67/83 [00:17<00:04,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  82%|████████▏ | 68/83 [00:18<00:04,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  83%|████████▎ | 69/83 [00:18<00:03,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  84%|████████▍ | 70/83 [00:18<00:03,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  86%|████████▌ | 71/83 [00:19<00:03,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  87%|████████▋ | 72/83 [00:19<00:02,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  88%|████████▊ | 73/83 [00:19<00:02,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  89%|████████▉ | 74/83 [00:19<00:02,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  90%|█████████ | 75/83 [00:20<00:02,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  92%|█████████▏| 76/83 [00:20<00:01,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  93%|█████████▎| 77/83 [00:20<00:01,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  94%|█████████▍| 78/83 [00:20<00:01,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  95%|█████████▌| 79/83 [00:21<00:01,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  96%|█████████▋| 80/83 [00:21<00:00,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  98%|█████████▊| 81/83 [00:21<00:00,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  99%|█████████▉| 82/83 [00:21<00:00,  4.19it/s]\u001b[A\n",
      "running layers(self.running_device): 100%|██████████| 83/83 [00:22<00:00,  3.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "running layers(self.running_device):   0%|          | 0/83 [00:00<?, ?it/s]\u001b[A\n",
      "running layers(self.running_device):   1%|          | 1/83 [00:00<00:21,  3.86it/s]\u001b[A\n",
      "running layers(self.running_device):   2%|▏         | 2/83 [00:00<00:22,  3.53it/s]\u001b[A\n",
      "running layers(self.running_device):   4%|▎         | 3/83 [00:00<00:22,  3.58it/s]\u001b[A\n",
      "running layers(self.running_device):   5%|▍         | 4/83 [00:01<00:21,  3.64it/s]\u001b[A\n",
      "running layers(self.running_device):   6%|▌         | 5/83 [00:01<00:21,  3.67it/s]\u001b[A\n",
      "running layers(self.running_device):   7%|▋         | 6/83 [00:01<00:20,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):   8%|▊         | 7/83 [00:01<00:20,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  10%|▉         | 8/83 [00:02<00:20,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  11%|█         | 9/83 [00:02<00:19,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  12%|█▏        | 10/83 [00:02<00:19,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  13%|█▎        | 11/83 [00:02<00:19,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  14%|█▍        | 12/83 [00:03<00:18,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  16%|█▌        | 13/83 [00:03<00:18,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  17%|█▋        | 14/83 [00:03<00:18,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  18%|█▊        | 15/83 [00:04<00:18,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  19%|█▉        | 16/83 [00:04<00:17,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  20%|██        | 17/83 [00:04<00:17,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  22%|██▏       | 18/83 [00:04<00:17,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  23%|██▎       | 19/83 [00:05<00:16,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  24%|██▍       | 20/83 [00:05<00:16,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  25%|██▌       | 21/83 [00:05<00:16,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  27%|██▋       | 22/83 [00:05<00:16,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  28%|██▊       | 23/83 [00:06<00:15,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  29%|██▉       | 24/83 [00:06<00:15,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  30%|███       | 25/83 [00:06<00:15,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  31%|███▏      | 26/83 [00:06<00:15,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  33%|███▎      | 27/83 [00:07<00:14,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  34%|███▎      | 28/83 [00:07<00:14,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  35%|███▍      | 29/83 [00:07<00:14,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  36%|███▌      | 30/83 [00:08<00:14,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  37%|███▋      | 31/83 [00:08<00:13,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  39%|███▊      | 32/83 [00:08<00:13,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  40%|███▉      | 33/83 [00:08<00:13,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  41%|████      | 34/83 [00:09<00:12,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  42%|████▏     | 35/83 [00:09<00:12,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  43%|████▎     | 36/83 [00:09<00:12,  3.78it/s]\u001b[A\n",
      "running layers(self.running_device):  45%|████▍     | 37/83 [00:09<00:12,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  46%|████▌     | 38/83 [00:10<00:11,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  47%|████▋     | 39/83 [00:10<00:11,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  48%|████▊     | 40/83 [00:10<00:11,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  49%|████▉     | 41/83 [00:10<00:11,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  51%|█████     | 42/83 [00:11<00:10,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  52%|█████▏    | 43/83 [00:11<00:10,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  53%|█████▎    | 44/83 [00:11<00:10,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  54%|█████▍    | 45/83 [00:11<00:10,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  55%|█████▌    | 46/83 [00:12<00:09,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  57%|█████▋    | 47/83 [00:12<00:09,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  58%|█████▊    | 48/83 [00:12<00:09,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  59%|█████▉    | 49/83 [00:13<00:09,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  60%|██████    | 50/83 [00:13<00:08,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  61%|██████▏   | 51/83 [00:13<00:08,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  63%|██████▎   | 52/83 [00:13<00:08,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  64%|██████▍   | 53/83 [00:14<00:08,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  65%|██████▌   | 54/83 [00:14<00:07,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  66%|██████▋   | 55/83 [00:14<00:07,  3.70it/s]\u001b[A\n",
      "running layers(self.running_device):  67%|██████▋   | 56/83 [00:14<00:07,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  69%|██████▊   | 57/83 [00:15<00:07,  3.69it/s]\u001b[A\n",
      "running layers(self.running_device):  70%|██████▉   | 58/83 [00:15<00:06,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  71%|███████   | 59/83 [00:15<00:06,  3.71it/s]\u001b[A\n",
      "running layers(self.running_device):  72%|███████▏  | 60/83 [00:16<00:06,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  73%|███████▎  | 61/83 [00:16<00:05,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  75%|███████▍  | 62/83 [00:16<00:05,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  76%|███████▌  | 63/83 [00:16<00:05,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  77%|███████▋  | 64/83 [00:17<00:05,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  78%|███████▊  | 65/83 [00:17<00:04,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  80%|███████▉  | 66/83 [00:17<00:04,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  81%|████████  | 67/83 [00:17<00:04,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  82%|████████▏ | 68/83 [00:18<00:04,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  83%|████████▎ | 69/83 [00:18<00:03,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  84%|████████▍ | 70/83 [00:18<00:03,  3.73it/s]\u001b[A\n",
      "running layers(self.running_device):  86%|████████▌ | 71/83 [00:18<00:03,  3.72it/s]\u001b[A\n",
      "running layers(self.running_device):  87%|████████▋ | 72/83 [00:19<00:02,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  88%|████████▊ | 73/83 [00:19<00:02,  3.74it/s]\u001b[A\n",
      "running layers(self.running_device):  89%|████████▉ | 74/83 [00:19<00:02,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  90%|█████████ | 75/83 [00:20<00:02,  3.75it/s]\u001b[A\n",
      "running layers(self.running_device):  92%|█████████▏| 76/83 [00:20<00:01,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  93%|█████████▎| 77/83 [00:20<00:01,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  94%|█████████▍| 78/83 [00:20<00:01,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  95%|█████████▌| 79/83 [00:21<00:01,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  96%|█████████▋| 80/83 [00:21<00:00,  3.77it/s]\u001b[A\n",
      "running layers(self.running_device):  98%|█████████▊| 81/83 [00:21<00:00,  3.76it/s]\u001b[A\n",
      "running layers(self.running_device):  99%|█████████▉| 82/83 [00:21<00:00,  4.20it/s]\u001b[A\n",
      "running layers(self.running_device): 100%|██████████| 83/83 [00:22<00:00,  3.76it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 93.27\n",
      "CPU times: user 1min 34s, sys: 12.8 s, total: 1min 46s\n",
      "Wall time: 1min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "# Processing batches\n",
    "# generate tokens for all batches and record duration\n",
    "tokens = []\n",
    "\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(batches), desc=f\"bs={batch_size}\") as pbar:\n",
    "    for i, batch in enumerate(batches):\n",
    "        # to accommodate all the requests with our \n",
    "        # current implementation, we take the max of\n",
    "        # all the tokens to generate among the requests\n",
    "        #batch_max_tokens = [b[1] for b in batch]\n",
    "        #max_tokens = max(batch_max_tokens)\n",
    "        #print(max_tokens)\n",
    "        pbar.set_postfix({'max_tokens': max_tokens})\n",
    "        \n",
    "        batch_prompts = [b[0] for b in batch]\n",
    "        inputs = model.tokenizer(\n",
    "            batch_prompts, padding=True, return_tensors=\"pt\").to('cuda')\n",
    "        tokens.extend(generate_batch(inputs, max_tokens=max_tokens))\n",
    "        pbar.update(1)\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(f\"Duration {duration_s:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717acbc7-fa2a-4904-b583-ed38c14637d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41058f-7872-4fca-9ae4-5f1f98426611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aee8d8f-4c43-4d1c-8a81-9c0a58073820",
   "metadata": {},
   "source": [
    "### Continuous Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac97e94-b405-40f8-862a-2139bca0b95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722a75a4-10ad-4e25-a472-44e5967d9a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 83/83 [00:39<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 83/83 [00:20<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 83/83 [00:19<00:00,  4.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I like\\nI'm\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = [\n",
    "        #'What is the capital of China?',\n",
    "        'I like',\n",
    "    ]\n",
    "\n",
    "input_tokens = model.tokenizer(input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=False,\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    #padding=True\n",
    "    )\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_tokens['input_ids'].cuda(),\n",
    "    max_new_tokens=3,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True)\n",
    "\n",
    "model.tokenizer.decode(generation_output.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d853f32-2cb5-45ae-8a12-498dd77265c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
