# ServingLLMs
How to serve LLMs efficiently on CPU or GPU using llamacpp and other efficiency techniques
