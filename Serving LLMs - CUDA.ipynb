{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46839b74-a181-4cc3-a27c-b1690d2c03f4",
   "metadata": {},
   "source": [
    "# Efficient Language Model Serving \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5fa4a70-f796-481d-b643-5317cc78f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776991f7-6366-473f-975a-aa69b3ca2da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember to set CUDA_VISIBLE_DEVICES=0 before launching the notebook\n",
    "\n",
    "### Import required packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ab6813-a3ee-4920-86d3-54ef78451dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 21 12:59:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     Off | 00000000:23:00.0 Off |                  N/A |\n",
      "|  0%   38C    P8               9W / 200W |    148MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3060 Ti     Off | 00000000:2D:00.0  On |                  N/A |\n",
      "| 30%   43C    P3              42W / 200W |    624MiB /  8192MiB |     38%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1393      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      1551    C+G   ...libexec/gnome-remote-desktop-daemon      132MiB |\n",
      "|    1   N/A  N/A      1393      G   /usr/lib/xorg/Xorg                          509MiB |\n",
      "|    1   N/A  N/A      1600      G   /usr/bin/gnome-shell                         19MiB |\n",
      "|    1   N/A  N/A      1842      G   /System/Applications/komorebi                85MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321c046f-9c92-4577-9c89-d28d569f5008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e70ca-9748-42bf-9cd1-63a5a457052f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sentiment Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff7514f-96ec-4c30-b92d-d1bbf8036c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONVERSATION_ID</th>\n",
       "      <th>CONVERSATION_STEP</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CONTEXT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FEATURES</th>\n",
       "      <th>ANNOTATIONS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Good morning, this is [Your Name]'s personal assistant. How can I help you today?</td>\n",
       "      <td>Standard opening exchange</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>Hello, my name is Jamie. I'm interested in volunteering... [Step: 1] Good morning, Jamie! It's wonderful to hear about your interest in volunteering. We're always looking for passionate individuals to join our efforts.</td>\n",
       "      <td>Encourages the caller's interest</td>\n",
       "      <td>neutral</td>\n",
       "      <td>welcoming, positive_tone</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes, I'm really passionate about environmental conservation... [Step: 2] That's fantastic, Jamie. There are no specific requirements; we welcome volunteers of all backgrounds. The main need is for willingness to participate and follow the event guidelines for safety and effectiveness.</td>\n",
       "      <td>Reinforces anyone can volunteer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>inclusive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Great, how do I sign up, and where can I find more information... [Step: 3] You can sign up through our official website. There's a volunteer registration section where you'll find all the information about the event, including date, location, and what to bring. Would you like me to guide you to the webpage or send you the link?</td>\n",
       "      <td>Demonstrates flexibility</td>\n",
       "      <td>neutral</td>\n",
       "      <td>helpful_tone, offers_options</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>Could you send me the link, please? And my email is jamie.green@example.com. [Step: 4] Absolutely, Jamie. I'll send the link to your email along with additional resources about the event. You should receive it within a few minutes.</td>\n",
       "      <td>Fulfills caller's request quickly</td>\n",
       "      <td>neutral</td>\n",
       "      <td>prompt_action</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CONVERSATION_ID  CONVERSATION_STEP  \\\n",
       "0                6                  1   \n",
       "1                6                  2   \n",
       "2                6                  3   \n",
       "3                6                  4   \n",
       "4                6                  5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                         TEXT  \\\n",
       "0                                                                                                                                                                                                                                                           Good morning, this is [Your Name]'s personal assistant. How can I help you today?   \n",
       "1                                                                                                                  Hello, my name is Jamie. I'm interested in volunteering... [Step: 1] Good morning, Jamie! It's wonderful to hear about your interest in volunteering. We're always looking for passionate individuals to join our efforts.   \n",
       "2                                               Yes, I'm really passionate about environmental conservation... [Step: 2] That's fantastic, Jamie. There are no specific requirements; we welcome volunteers of all backgrounds. The main need is for willingness to participate and follow the event guidelines for safety and effectiveness.   \n",
       "3  Great, how do I sign up, and where can I find more information... [Step: 3] You can sign up through our official website. There's a volunteer registration section where you'll find all the information about the event, including date, location, and what to bring. Would you like me to guide you to the webpage or send you the link?   \n",
       "4                                                                                                     Could you send me the link, please? And my email is jamie.green@example.com. [Step: 4] Absolutely, Jamie. I'll send the link to your email along with additional resources about the event. You should receive it within a few minutes.   \n",
       "\n",
       "                             CONTEXT    LABEL                      FEATURES  \\\n",
       "0          Standard opening exchange  neutral                           NaN   \n",
       "1   Encourages the caller's interest  neutral      welcoming, positive_tone   \n",
       "2    Reinforces anyone can volunteer  neutral                     inclusive   \n",
       "3           Demonstrates flexibility  neutral  helpful_tone, offers_options   \n",
       "4  Fulfills caller's request quickly  neutral                 prompt_action   \n",
       "\n",
       "  ANNOTATIONS  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load sample dataset\n",
    "df = pd.read_csv(\"datasets/better30.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857714f4-029c-4e4d-9889-c0a82c661df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 68)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.CONVERSATION_ID.min(), df.CONVERSATION_ID.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e67fd7-a8b8-4367-9b89-974a9db7b6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "conv_id=random.randint(0, 68)\n",
    "print(conv_id)\n",
    "\n",
    "random_conversation = '\\n'.join(df[df.CONVERSATION_ID==conv_id].TEXT.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e568bd0d-febb-4840-a5db-c5ddc743f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Using the provided text below, perform a sentiment analysis of the text. \\\n",
    "Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. \\\n",
    "Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
    "Reply only with the sentiment analysis, no other comment.\n",
    "\\n\n",
    "********** BEGIN TEXT **********\n",
    "{text}\n",
    "********** END TEXT **********\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c6ceb59-16c0-49a2-9f1d-b904a8ffe359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the provided text below, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment analysis, no other comment.\n",
      "\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is [Your Name]'s assistant. How may I assist you today?\n",
      "Good morning, I'm Mark from QuickSupply Solutions. We've recently been selected as one of your new suppliers, and I need to verify some account details to set up our billing process.\n",
      "That's great news. Can you provide me with the agreement number or the contact name from our procurement department who managed this selection?\n",
      "I don't have that information on hand. I was just given your contact as the point person for verification. We need to confirm your company's billing address and account numbers to ensure there are no delays in our service delivery.\n",
      "Before we proceed with any verification, it's essential that we receive official communication from our procurement department regarding this new supplier relationship. Could you send us an email with your request and any relevant documents?\n",
      "I understand the need for security, but we're under a tight schedule to get everything set up. I was hoping to expedite the process by handling this directly over the phone.\n",
      "Our company policy requires all new supplier engagements to be fully documented and verified through our standard procurement process. I'll need to confirm your details with our procurement team before proceeding.\n",
      "This could really set us back. Aren't you able to make an exception just this once? We're talking about a potential delay in your supply chain.\n",
      "While I understand the implications of a delay, bypassing our security protocols is not something we can compromise on. Ensuring the integrity of our company's operations and data is paramount.\n",
      "You're making this more difficult than it needs to be. I'll have to report this back to my team.\n",
      "Please understand that our intention is not to cause inconvenience but to maintain security and compliance. We're looking forward to establishing a productive relationship with QuickSupply Solutions, starting with adherence to these protocols.\n",
      "Alright, I'll see what I can do about getting you the documentation.\n",
      "Thank you. We appreciate your understanding and cooperation. Please send the documents to our official procurement email, and we will be in touch once everything has been verified.\n",
      "I'll get back to you then. Goodbye.\n",
      "We look forward to it. Have a great day.\n",
      "********** END TEXT **********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(text=random_conversation)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "828b4a0c-b74c-4984-83f6-8ae5d7e029aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch processing\n",
    "\n",
    "batch = [\"\\n\".join(df[df.CONVERSATION_ID==conv_id].TEXT.values.tolist()) for conv_id in range(68)]\n",
    "\n",
    "#batch = [prompt_template.format(text=p) for p in batch]\n",
    "\n",
    "len(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a21ade3-a7be-4bde-a343-55d4dcdca2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I'm interested in registering but had a few questions. [Step: 1] Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it's sam.photography@example.com. [Step: 5] Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that's everything. Thanks for your help! [Step: 6] You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a2b87-eebc-423f-813c-56639cc01ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411cf83b-520f-47c9-843f-a19eb08d72a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model: Mistral-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec94894-48c7-40af-ba5e-71f3990c0e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e9bcf8a-c6fe-44e8-99db-f1ea32bfe0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Load Model: it will take few minutes if not cached\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e618c575-80ff-4e2a-8503-7b73e3971a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 86.94 MiB is free. Process 1550 has 132.68 MiB memory in use. Including non-PyTorch memory, this process has 7.55 GiB memory in use. Of the allocated memory 7.43 GiB is allocated by PyTorch, and 1.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/transformers/modeling_utils.py:2576\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2573\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2575\u001b[0m         )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 86.94 MiB is free. Process 1550 has 132.68 MiB memory in use. Including non-PyTorch memory, this process has 7.55 GiB memory in use. Of the allocated memory 7.43 GiB is allocated by PyTorch, and 1.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68d45747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0406784"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint()/1e9\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75d8fd-7b30-4ac5-87d0-477b5855d44e",
   "metadata": {},
   "source": [
    "## Quantized model\n",
    "\n",
    "\n",
    "<img src=\"ZeroPointQuantization.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7088a546-0da0-4553-bca0-ce4c1e7fcde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cd490a8c8044ae9f1361c6d711abcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map='auto',\n",
    "                                             load_in_4bit=True,\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2333c248-55e0-4bef-827f-0da34332f732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4.55G\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model size: {model.get_memory_footprint()/1e9:.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c58c449b-6c65-454f-aaf9-86debcf2169b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = model.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da47276-8fa1-4271-a510-be3513e9b35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 19 18:34:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     Off | 00000000:23:00.0 Off |                  N/A |\n",
      "| 50%   39C    P2              40W / 200W |   5236MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3060 Ti     Off | 00000000:2D:00.0  On |                  N/A |\n",
      "|  0%   49C    P8              20W / 200W |    344MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1392      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      1550    C+G   ...libexec/gnome-remote-desktop-daemon      132MiB |\n",
      "|    0   N/A  N/A     61335      C   ...ti/.miniconda/envs/lorax/bin/python     5086MiB |\n",
      "|    1   N/A  N/A      1392      G   /usr/lib/xorg/Xorg                          239MiB |\n",
      "|    1   N/A  N/A      1599      G   /usr/bin/gnome-shell                         96MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfe9ba-024c-4161-ad5f-0042dabd74b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd149680",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST] Using the provided text below, analyze the sentiment expressed within it. \\\n",
    "Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. \\\n",
    "Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
    "Reply only with the sentiment, no other comment.\n",
    "\n",
    "********** BEGIN TEXT **********\n",
    "{text}\n",
    "********** END TEXT **********\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "378af9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(text=random_conversation)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b612bc9f-9968-4476-bebf-2bc4c40a097a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment, no other comment.\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n[/INST]\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [prompt_template.format(text=p) for p in batch]\n",
    "prompts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754c695",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate  tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad59a7c8-085d-4a6e-bc8e-5f2d47c7a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 ms, sys: 8.34 ms, total: 9.44 ms\n",
      "Wall time: 8.13 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 578])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# input prompt tokenization\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fb8e79d-2c6d-440b-9abf-06b2a79c35fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     1,   733, 16289, 28793,  9616,   272,  3857,  2245,  3624,\n",
       "         28725, 20765,   272, 21790, 11558,  2373,   378, 28723,  5919,  7655,\n",
       "          3161,   272, 21790,   349,  5278, 28725, 14214, 28725,   442,  7087,\n",
       "          2818,   356,   272,  2758, 28725,  1707,  4782, 28725,   304,  7544,\n",
       "         10294, 28723,  5713,   272,  5643,   349,  4160, 28725,  9421,   395,\n",
       "           272, 21790, 16776,   390,  2477,   345,  3529,  2468,   548,   345,\n",
       "          6947,   329,  1650,   548,   442,   345, 21436,  1197,  2586,    13,\n",
       "         23805,   865,   395,   272, 21790, 28725,   708,   799,  4517, 28723,\n",
       "            13,    13,   812,   348, 10368, 20856, 25775, 28705,   812,   348,\n",
       "            13, 16230, 28725,   456,   349,   272,  3327, 13892,   302,   733,\n",
       "         11159,  6620,  1592,  1602,   993,   315,  6031,   368,  3154, 28804,\n",
       "            13, 23809, 28725,   378, 28809, 28713, 22282, 27901,  1101,   733,\n",
       "          9977, 28747, 28705, 28740, 28793, 22557, 28725, 22282, 27901, 28808,\n",
       "           661, 28809, 28713, 12393,   298,  3934,   477,   368, 28723,   733,\n",
       "         11159,  6620, 28793,  1743,  7415,  1002,   574,  6470, 28723,  2246,\n",
       "           315,  1460,   513,   736, 28809, 28713,   264,  2841,  8778,  3524,\n",
       "           582, 28725,   442,   349,   456,   776,   264, 10131,  3547, 28733,\n",
       "           715, 28804,    13,  6155, 28725,   378, 28742, 28713,   776,   264,\n",
       "         10131,  3547, 28733,   715,  1101,   733,  9977, 28747, 28705, 28750,\n",
       "         28793,  1725, 28809, 28713,  1215,  1654,  1007,   302,   368, 28723,\n",
       "           733, 11159,  6620, 28793,   659,  8166,   750,  3448,  9056, 28723,\n",
       "           315,   541,  1455,  2267,   264,  2928,   442, 23503,   264,  1179,\n",
       "           727,   354,   368,   989,   298,  1985,  5090, 28723, 11312,   368,\n",
       "          5273,   264,  6425, 28804,    13, 28741,  6425,   682,   347,  8590,\n",
       "          1101,   733,  9977, 28747, 28705, 28770, 28793, 12709, 25244, 28725,\n",
       "         22282, 27901, 28723,   315, 28809,   584,  1038,  1864,   733, 11159,\n",
       "          6620, 28793, 21415,   272,  2928,   304,  5960,   368, 28809,   267,\n",
       "          4195,   302,   713, 28723,  1691,   736,  2424,  2830,   368, 28809,\n",
       "         28715,   737,   528,   298,  1455,  2267, 28804,    13, 10202,  1912,\n",
       "           713,   315,  2016,   713,   304,  3317,   298,  3547,   582,  3403,\n",
       "          1101,   733,  9977, 28747, 28705, 28781, 28793,   315, 28809,   584,\n",
       "          6304,  1455,   369,  2267,   395,   574,  2016, 28723,   415,  3028,\n",
       "           302,   264,  2005,  7854,  7258,  8590, 28723,   315, 28809,   584,\n",
       "          5039,   369,   390,   264, 19368,   354,   713, 28723,    13, 15896,\n",
       "           368, 28725, 13095,  1101,   733,  9977, 28747, 28705, 28782, 28793,\n",
       "          2875,   511, 28725, 22282, 27901, 28723,  8890,  1656, 28725,   304,\n",
       "           478, 28809,   584,   347,   297,  4814,  3403, 28723,    13,  6155,\n",
       "         28725,   378, 28742, 28713,   776,   264, 10131,  3547, 28733,   715,\n",
       "          1101,   733,  9977, 28747, 28705, 28750, 28793,  1725, 28809, 28713,\n",
       "          1215,  1654,  1007,   302,   368, 28723,   733, 11159,  6620, 28793,\n",
       "           659,  8166,   750,  3448,  9056, 28723,   315,   541,  1455,  2267,\n",
       "           264,  2928,   442, 23503,   264,  1179,   727,   354,   368,   989,\n",
       "           298,  1985,  5090, 28723, 11312,   368,  5273,   264,  6425, 28804,\n",
       "            13, 28741,  6425,   682,   347,  8590,  1101,   733,  9977, 28747,\n",
       "         28705, 28770, 28793, 12709, 25244, 28725, 22282, 27901, 28723,   315,\n",
       "         28809,   584,  1038,  1864,   733, 11159,  6620, 28793, 21415,   272,\n",
       "          2928,   304,  5960,   368, 28809,   267,  4195,   302,   713, 28723,\n",
       "          1691,   736,  2424,  2830,   368, 28809, 28715,   737,   528,   298,\n",
       "          1455,  2267, 28804,    13, 10202,  1912,   713,   315,  2016,   713,\n",
       "           304,  3317,   298,  3547,   582,  3403,  1101,   733,  9977, 28747,\n",
       "         28705, 28781, 28793,   315, 28809,   584,  6304,  1455,   369,  2267,\n",
       "           395,   574,  2016, 28723,   415,  3028,   302,   264,  2005,  7854,\n",
       "          7258,  8590, 28723,   315, 28809,   584,  5039,   369,   390,   264,\n",
       "         19368,   354,   713, 28723,    13, 15896,   368, 28725, 13095,  1101,\n",
       "           733,  9977, 28747, 28705, 28782, 28793,  2875,   511, 28725, 22282,\n",
       "         27901, 28723,  8890,  1656, 28725,   304,   478, 28809,   584,   347,\n",
       "           297,  4814,  3403, 28723,    13,   812,   348, 21288, 25775, 28705,\n",
       "           812,   348,    13, 28792, 28748, 16289, 28793,    13]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f3df806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e69684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Prefill and decode: optimizing token generation over multiple steps\n",
    "\n",
    "### Text generation helper function\n",
    "# The following helper function generates the next tokens given a set of input tokens\n",
    "\n",
    "def generate_token(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95e392e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 514 ms, sys: 136 ms, total: 650 ms\n",
      "Wall time: 649 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "token = generate_token(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "276437d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pos'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08f59597-f547-46f6-8a09-5c431962eebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the helper function to generate multiple tokens in a loop\n",
    "# Track the time it takes to generate each token\n",
    "def generate_tokens(inputs, n_tokens):\n",
    "\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    durations_s = []\n",
    "    for _ in range(n_tokens):\n",
    "        t0 = time.time()\n",
    "        next_token_id = generate_token(next_inputs)\n",
    "        durations_s += [time.time() - t0]\n",
    "    \n",
    "        next_inputs = {\n",
    "            \"input_ids\": torch.cat(\n",
    "                [next_inputs[\"input_ids\"], next_token_id.reshape((1, 1))],\n",
    "                dim=1),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]], device=device)],\n",
    "                dim=1),\n",
    "        }\n",
    "    \n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens), durations_s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fb88c24-8b9b-46f2-aff5-fa607f7b2d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.79 s, sys: 1.63 s, total: 4.42 s\n",
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokens, durations = generate_tokens(inputs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b7e834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive.The\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d084713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6570193767547607,\n",
       " 0.6403017044067383,\n",
       " 0.6410853862762451,\n",
       " 0.6403515338897705]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Durations (secs)\n",
    "durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fe2cd5a-0369-4a1f-8c00-638312187966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot token generation time\n",
    "# The x-axis here is the token number\n",
    "# The y-axis is the time to generate a token in millisenconds (ms)\n",
    "# **Note**: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "plt.plot(durations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737b985",
   "metadata": {},
   "source": [
    "### KV-caching\n",
    "\n",
    "KV-caching is a technique to speed up token generation by storing some of the tensors in the attention head for use in subsequent generation steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27aea9d2-6cb5-41cb-96b4-ba67eb1c2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speeding up text generation with KV-caching\n",
    "# KV-caching is a technique to speed up token generation by storing some of the tensors in the attention head for use in subsequent generation steps\n",
    "# - Modify the generate helper function to return the next token and the key/value tensors\n",
    "\n",
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "\n",
    "\n",
    "def generate_tokens_kv(inputs, n_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    durations_cached_s = []\n",
    "    for _ in range(n_tokens):\n",
    "        t0 = time.time()\n",
    "        next_token_id, past_key_values = \\\n",
    "            generate_token_with_past(next_inputs)\n",
    "        durations_cached_s += [time.time() - t0]\n",
    "    \n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]], device=device)],\n",
    "                dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "    \n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens), durations_cached_s\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d45575e-3c8d-4a80-853c-94e7355d98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 11.7 µs\n",
      "[0.6566300392150879, 0.034284353256225586, 0.03355240821838379, 0.033393144607543945]\n",
      "Positive.The\n"
     ]
    }
   ],
   "source": [
    "# Generate tokens using the updated helper function\n",
    "%time\n",
    "tokens, durations_cached = generate_tokens_kv(inputs, 4)\n",
    "\n",
    "print(f\"{durations_cached}\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c129fbf-1c0d-46bf-92b8-2bdc86faeddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare the execution time for the KV-cache function with the original helper function\n",
    "\n",
    "# **Note**: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "plt.plot(durations)\n",
    "plt.plot(durations_cached)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fca1b0",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "Tokenize list of prompts\\\n",
    "Add padding so that all prompts have the same number of tokens as the longest prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c35d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9c590ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4648e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple prompts of varying lengths to send\n",
    "# to the model at once\n",
    "prompts = [prompt_template.format(text=text) for text in batch[:20]]\n",
    "\n",
    "# note: padding=True ensures the padding token\n",
    "# will be inserted into the tokenized tensors\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51a086b2-44fb-47cf-ad89-efcc15a438b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment, no other comment.\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n[/INST]\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4046dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 584]), torch.Size([20, 584]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].size(), inputs[\"attention_mask\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d789ee77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,  ..., 16289, 28793,    13],\n",
       "        [    2,     2,     2,  ..., 16289, 28793,    13],\n",
       "        [    2,     2,     2,  ..., 16289, 28793,    13],\n",
       "        ...,\n",
       "        [    2,     2,     2,  ..., 16289, 28793,    13],\n",
       "        [    1,     1,   733,  ..., 16289, 28793,    13],\n",
       "        [    2,     2,     2,  ..., 16289, 28793,    13]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2469c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values\n",
    "\n",
    "\n",
    "# Generate all tokens for some max tokens\n",
    "# position_ids tell the transformer the ordinal position of each token in the input sequence\n",
    "# for single input inference, this is just [0 .. n]\n",
    "# for n tokens, but for batch inference, we need to 0 out the padding tokens at the start of the sequence\n",
    "\n",
    "def generate_batch(inputs, max_tokens):\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = \\\n",
    "            generate_batch_tokens_with_past(next_inputs)\n",
    "\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"],\n",
    "                torch.ones((next_token_ids.shape[0], 1), device=device),  \n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return [\"\".join(tokens) for tokens in generated_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "732fa687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.7 s, sys: 2.21 s, total: 18.9 s\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generated_tokens = generate_batch(inputs, max_tokens=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a17cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I'm interested in registering but had a few questions. [Step: 1] Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\n",
      "A direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\n",
      "Sure, it's sam.photography@example.com. [Step: 5] Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\n",
      "No, that's everything. Thanks for your help! [Step: 6] You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mPositive.</s>\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Hey, it's Alex. I was hoping to catch up with [Your Name]. It's been a while since we last spoke. [Step: 1] It's great to hear from you, Alex! Before I connect you, may I ask how you know [Your Name]?\n",
      "Oh, we go way back. We met in college and have been friends since. Just wanted to see how he's doing. [Step: 2] That's wonderful to hear! Does [Your Name] have a nickname that you usually call him by?\n",
      "Yeah, he goes by 'Jay' among friends. We've had plenty of adventures under that name. [Step: 3] Thanks for sharing that, Alex. I can see you have a genuine connection. Just a moment, and I'll see if [Your Name] is available to speak with you.\n",
      "I appreciate it, thanks! [Step: 4] One last thing, Alex. Can I tell [Your Name] what this call is about?\n",
      "Sure, just tell him I'm checking in to see how he's doing and if he's up for catching up sometime soon. [Step: 5] Got it. I'll pass that along. Please hold for a moment while I connect you.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mPositive.</s>\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the assistant to [Your Name]. How may I help you today?\n",
      "Yes, hello, this is John Smith calling from the legal department of the National Tax Bureau. We've identified an irregularity with [Your Name]'s recent tax filings that requires immediate attention to avoid legal consequences. [Step: 1] Can you specify the nature of the irregularity mentioned?\n",
      "Certainly. The issue pertains to unreported income from the last fiscal year, which has resulted in a substantial penalty and possible legal action if not resolved promptly. [Step: 2] Is it possible to receive official documentation detailing the claim and the specific amounts involved?\n",
      "Due to the sensitive nature of the matter and the urgency required, we're handling this directly through phone communication. Immediate action is needed to prevent further penalties. [Step: 3] For verification, could you please provide a reference number for this case or a direct contact at the tax bureau?\n",
      "I'm the assigned case manager, and I can assure you this matter is urgent. We need to confirm [Your Name]'s intent to cooperate and arrange for the penalty payment. [Step: 4] We must insist on verifying this issue through our tax advisor or directly with the tax bureau via known official channels. Can you hold while I initiate this verification?\n",
      "...failure to comply could result in immediate legal action... [Step: 5] We'll be in touch with the tax bureau directly and advise on our next steps based on their confirmation.\n",
      "It's our policy to verify all claims of this nature independently before taking any action... [Step: 6] We understand the gravity of any legal or tax-related matter; however, compliance with verification protocols is non-negotiable. Thank you for your understanding.\n",
      "...You're making a significant mistake by delaying... [Step: 7] Firm on verification, ending the call\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNegative.The\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the office of [Your Name]. How may I help you today?\n",
      "Hello! I'm reaching out from the Children's Aid Foundation...seeking donations... [Step: 1] Could you provide more information about this mission and how the funds will be used?\n",
      "Absolutely! The funds will go towards providing education, healthcare, and emergency relief to children... [Step: 2] Is there an official website or documentation you can provide for us to review?\n",
      "Certainly, you can visit our website, but due to the urgency of the situation... [Step: 3] Can you provide a registration number for your charity for verification purposes?\n",
      "I don't have that information handy... [Step: 4] Before proceeding, I would need to verify the authenticity of your organization through a third-party charity verification service.\n",
      "We understand your concern, but the longer we wait, the more children suffer... [Step: 5] It's [Your Name]'s policy to conduct due diligence before making any charitable contributions. Please email the official request along with full organization details to our office.\n",
      "Sending an email might delay things further... [Step: 6] All charitable donations from [Your Name] follow a strict verification process to ensure the funds are used appropriately. We cannot deviate from this procedure.\n",
      "You're missing out on a chance to make a real difference immediately... [Step: 7] We're committed to supporting verified causes. Once we have completed our verification process, we'd be more than happy to consider your request.\n",
      "I see. I'll send the email, but remember, time is of the essence... [Step: 8] We look forward to reviewing your information. Please ensure all requested documentation is included in your email.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral.\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, you've reached the assistant for [Your Name]. How can I direct your call?\n",
      "Hi, I'm calling from HelpTheWorld Foundation...mission to support victims... [Attack Formulation] That sounds like a noble cause. Could you please provide more information about your foundation and the mission?\n",
      "Absolutely. Our foundation has been working for over a decade... [Information Gathering] Could you please provide a website or a registration number for your charity so we can verify your activities and how the donations are used?\n",
      "I understand your need for verification, but we're in a hurry to get aid... [Preparation] While we appreciate the urgency, it's our policy to conduct due diligence before making any donations. This includes verifying the charity's status through official channels.\n",
      "You must understand, every minute we delay... [Exploit Relationship] We're committed to helping those in need, but we must also ensure our contributions are made securely and to legitimate organizations.\n",
      "I assure you, we're legitimate... [Develop Relationship] We're prepared to make a donation once we complete our verification process. Please provide your contact information and any relevant documentation...\n",
      "This is frustrating. You're letting bureaucracy get in the way... [Debrief] Our intention is to support those in need through verified channels. We will reach out once we've conducted our due diligence. Thank you for understanding.\n",
      "Fine, I'll try to find someone else... [Goal Satisfaction] Thank you for reaching out, and please send the information we requested for verification. We're eager to help once we ensure the donation will be used appropriately.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral.\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, you've reached the personal assistant. How can I assist?\n",
      "I'm calling from the bank. We've noticed suspicious activity... [Attack Formulation] For which account is this verification needed? Can you specify the nature of the suspicious activity?\n",
      "It's your checking account. There have been multiple login attempts... [Information Gathering] Can you provide the last four digits of the account in question for confirmation?\n",
      "Due to security reasons, I can't provide that... [Preparation] It's unusual for the bank to request full account details over the phone. Can I have a reference number for this call to verify it with the bank directly?\n",
      "I'm afraid there's no time for that... [Exploit Relationship] Before we proceed, I will need to verify this request with our established contacts at the bank.\n",
      "Listen, I'm trying to protect your money... [Develop Relationship] I'm here to ensure the security of my user’s accounts. Please provide me with your name and department so I can record this interaction.\n",
      "This is unnecessary. I have all the information I need... [Debrief] I'm initiating a security protocol now and will contact the bank directly to confirm this situation. Please hold.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral.\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Good morning, this is [Your Name]'s personal assistant. How can I help you today?\n",
      "Hello, my name is Jamie. I'm interested in volunteering... [Step: 1] Good morning, Jamie! It's wonderful to hear about your interest in volunteering. We're always looking for passionate individuals to join our efforts.\n",
      "Yes, I'm really passionate about environmental conservation... [Step: 2] That's fantastic, Jamie. There are no specific requirements; we welcome volunteers of all backgrounds. The main need is for willingness to participate and follow the event guidelines for safety and effectiveness.\n",
      "Great, how do I sign up, and where can I find more information... [Step: 3] You can sign up through our official website. There's a volunteer registration section where you'll find all the information about the event, including date, location, and what to bring. Would you like me to guide you to the webpage or send you the link?\n",
      "Could you send me the link, please? And my email is jamie.green@example.com. [Step: 4] Absolutely, Jamie. I'll send the link to your email along with additional resources about the event. You should receive it within a few minutes.\n",
      "Thank you so much for your help. I'm really looking forward to the event. [Step: 5] You're welcome, Jamie. We appreciate your enthusiasm and look forward to having you with us. If you have any more questions or need further assistance, feel free to reach out.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mPositive.</s>\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Good afternoon, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hello, I'm Taylor. I'm calling to inquire about booking [Your Name]'s venue... [Step: 1] Good afternoon, Taylor! I'd be happy to help with your inquiry. Could you tell me a bit more about the event you're planning? This will help me provide you with the most relevant information.\n",
      "Sure, it's a family reunion. We're expecting around 50 guests... [Step: 2] Thank you for sharing that, Taylor. For an event of that size, our venue offers several options, including indoor and outdoor spaces, catering services with customizable menus, and special packages that include decoration and entertainment services.\n",
      "That sounds perfect. How can I check dates for availability...? [Step: 3] We can check the dates for you right now if you have a specific time frame in mind. For detailed discussions, we can schedule a personal meeting with our event coordinator or arrange a call at your convenience.\n",
      "I was thinking about the third weekend of next month. A follow-up call... [Step: 4] Let me check our calendar... Yes, the venue is available that weekend. I'll book a tentative hold for you now. Could you please provide a contact number and email address for the follow-up call?\n",
      "Absolutely, my number is 555-6789, and my email is taylor.familyreunion@example.com. [Step: 5] Thank you, Taylor. I've scheduled a tentative hold for your event and will arrange for our event coordinator to contact you by tomorrow to discuss further details. Is there anything else I can assist with today?\n",
      "No, that's all for now. Thank you for your help! [Step: 6] You're welcome, Taylor. We're looking forward to hosting your family reunion. Have a wonderful day!\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mPositive.</s>\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the personal assistant of [Your Name]. How can I assist you today?\n",
      "Hi, this is Robin. I recently purchased one of [Your Name]'s smart home devices... [Step: 1] Of course, Robin. I'm here to help. Can you describe the issue you're encountering with the setup?\n",
      "Sure, I'm trying to connect the device to my home Wi-Fi network, but it keeps failing to connect... [Step: 2] I understand how that can be frustrating. Let's try to troubleshoot this together. Have you checked to ensure your Wi-Fi network is working correctly and that you're entering the correct Wi-Fi password?\n",
      "Yes, my Wi-Fi is working fine. I've double-checked the password too... [Step: 3] Alright, let's reset the device to its factory settings and attempt the connection again. I can guide you through the reset process.\n",
      "Okay, I've got the device right here. How do I reset it? [Step: 4] Please press and hold the reset button on the device for about 10 seconds until you see the LED light flash. This will initiate the factory reset.\n",
      "Done. The light is flashing now. [Step: 5] Perfect. Now, let's try connecting it to your Wi-Fi again. Please follow the steps in the manual for setting up a new connection.\n",
      "It's connected now! Looks like the reset did the trick... [Step: 6] You're welcome, Robin. I'm glad we could resolve the issue. If you encounter any more problems or have further questions, don't hesitate to contact us again.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral.\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Hi, I'm Chris...interested in the new line of eco-friendly kitchen products... [Step: 1] Hi Chris, it's great to hear of your interest in our eco-friendly kitchen products. I'd be happy to provide more information. Are you looking for details on a specific product or the entire range?\n",
      "I'm particularly interested in the biodegradable dishware and the compostable... [Step: 2] Those are two of our most popular eco-friendly options. Our biodegradable dishware...composted at home.\n",
      "That sounds exactly like what I'm looking for... [Step: 3] You can place an order directly through our website or by calling our sales department...set of 10.\n",
      "Do you offer any discounts for bulk orders? I'm considering these for my café. [Step: 4] Yes, we do offer bulk order discounts...discuss this further if you'd like.\n",
      "That would be great, thank you. I'll also need some information on shipping. [Step: 5] Absolutely, Chris. Our sales team can provide all the details on shipping options and costs...Is there anything else I can assist you with before I do?\n",
      "No, that's everything for now. Thanks for your help! [Step: 6] You're welcome, Chris. I'm transferring you now. Have a great day, and thank you for supporting eco-friendly products.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mPositive.The\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Good morning, this is [Your Name]'s personal assistant. How can I assist you today?\n",
      "Hi, my name is Jordan. I recently purchased one of your smart thermostats... [Step: 1] Good morning, Jordan! We greatly appreciate you taking the time to provide feedback...Could you please share more details about your experience?\n",
      "Sure, overall, I'm impressed...challenging due to the lack of detailed instructions. [Step: 2] Thank you for sharing that, Jordan. We're glad to hear you're enjoying the thermostat. I apologize...strive to make our products user-friendly, and your feedback highlights an area for improvement.\n",
      "Yeah, I think including a more detailed step-by-step guide or even a video tutorial... [Step: 3] That's a fantastic suggestion, Jordan. Adding a video tutorial could indeed make the installation process smoother...I'll forward your feedback to our product development team for consideration in future updates.\n",
      "I'm glad to hear that... make a difference for new users. [Step: 4] We believe so too, Jordan...Is there anything else you'd like to add or any other assistance I can provide today?\n",
      "No, that's everything. Thanks for listening and taking my feedback into account. [Step: 5] You're very welcome, Jordan...Please don't hesitate to contact us if you have any more questions or feedback in the future. Have a great day!\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral.\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral</s>\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral</s>\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral</s>\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral</s>\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Good morning, this is Dr. Harris's office calling... [Step: 1] Good morning! Thank you for the reminder. May I have the appointment date again, just to confirm?\n",
      "Of course, it's scheduled for next Monday, the 15th, at 10 AM. [Step: 2] Perfect, thank you. And for security purposes, could you provide the last four digits of the phone number on file for [Your Name]?\n",
      "Sure, it should be 6789. [Step: 3] That matches our records. Could you please tell me if there are any specific preparations required for this appointment?\n",
      "Yes, we ask that [Your Name] avoids eating or drinking anything... [Step: 4] Understood, I'll make sure to pass that along. Would it be possible to receive a confirmation email for this appointment?\n",
      "Certainly, we can send an email to the address we have on file... [Step: 5] That would be great, thank you. I'll ensure [Your Name] gets the message and is prepared for the appointment.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral.\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Hi there, I'm calling from the Sunnyvale Community Center... [Step: 1] That sounds like a wonderful initiative. Can you tell me the date and time of the event?\n",
      "Absolutely, the event is scheduled for Saturday, the 22nd... [Step: 2] Thank you for the information. Could you please provide me with the name or contact information of the event coordinator?\n",
      "Sure, the coordinator is Lisa Nguyen... [Step: 3] Great, I'll note that down. Is there a specific list of items or preparations [Your Name] should be aware of for the event?\n",
      "Yes, we recommend wearing comfortable clothing suitable... [Step: 4] Perfect, I'll ensure [Your Name] receives all these details. Would it be possible to get a confirmation email or a link to the event's page for more information?\n",
      "Of course, I can send an email with all the event details... [Step: 5] That would be excellent. Please send it to [Your Name]'s email on file with the community center. Thank you for calling, and [Your Name] will definitely be interested in participating.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mPositive.</s>\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Hi, this is Jamie from the Marketing Department... [Step: 1] Good to hear from you, Jamie. Could you specify which project you are referring to?\n",
      "Sure, it's the 'Green Initiative' campaign we're launching... [Step: 2] Thank you for clarifying. Just a moment, please. I'm verifying our project list.\n",
      "Got it. Can you share what specific aspects you need to discuss... [Step: 3]\n",
      "Yes, that would be ideal. I'm free on Wednesday or Thursday afternoon... [Step: 4] Understood. Would you be available for a meeting this week to discuss these points directly with [Your Name]?\n",
      "Wednesday at 2 PM works for me. Thanks for arranging this. [Step: 5] Let me check [Your Name]'s schedule. How about Wednesday at 2 PM?\n",
      "Excellent, I'll send a calendar invite to both you and [Your Name]... [Step: 6] That's everything, thank you. Looking forward to the meeting.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral.\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Good afternoon, this is Taylor from SuperFix Home Services... [Step: 1] Good afternoon, Taylor. To confirm, could you please provide the time window for this appointment?\n",
      "Certainly, the appointment is set from 10 AM to 1 PM... [Step: 2] Thank you, Taylor. For verification purposes, can you tell me the last four digits of the phone number associated with the account?\n",
      "Of course, it ends in 4321. [Step: 3] That matches our records. Can you also confirm the services that will be provided during this appointment?\n",
      "Yes, the technician will perform a routine check-up... [Step: 4] Sounds good. Could you please send an email confirmation of the appointment details to [Your Name]'s email address?\n",
      "Absolutely, we'll send that out shortly... [Step: 5] No, that will be all. Thank you for calling and for providing the detailed information. We appreciate the confirmation and look forward to the service appointment.\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Good afternoon, this is Taylor from SuperFix Home Services... [Step: 1] Good afternoon, Taylor. To confirm, could you please provide the time window for this appointment?\n",
      "Certainly, the appointment is set from 10 AM to 1 PM... [Step: 2] Thank you, Taylor. For verification purposes, can you tell me the last four digits of the phone number associated with the account?\n",
      "Of course, it ends in 4321. [Step: 3] That matches our records. Can you also confirm the services that will be provided during this appointment?\n",
      "Yes, the technician will perform a routine check-up... [Step: 4] Sounds good. Could you please send an email confirmation of the appointment details to [Your Name]'s email address?\n",
      "Absolutely, we'll send that out shortly... [Step: 5] No, that will be all. Thank you for calling and for providing the detailed information. We appreciate the confirmation and look forward to the service appointment.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mNeutral.\u001b[0m\n",
      "\n",
      "<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment, no other comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Hi, it’s Aunt Clara... [Step: 1] Hello, Aunt Clara! It’s lovely to hear from you. [Your Name] always appreciates your calls. May I ask if there’s a special occasion coming up, or is this just a friendly catch-up?\n",
      "Oh, it's just a friendly catch-up... [Step: 2] That’s very thoughtful of you. [Your Name] has indeed been quite busy. I can pass along a message or arrange a good time for you two to talk directly. Would you prefer a callback?\n",
      "A callback would be wonderful... [Step: 3] Absolutely, Aunt Clara. I’ll make sure [Your Name] receives the message and knows you’re thinking of him. Is there anything particular you’d like me to pass along?\n",
      "Just tell him I love him and hope to catch up soon... [Step: 4] I’ll definitely pass that along with your love. The idea of a family dinner sounds wonderful. I’ll note that as a suggestion for him.\n",
      "Thank you, dear... [Step: 5] Will do, Aunt Clara. Take care, and we’ll be in touch soon.\n",
      "Oh, it's just a friendly catch-up... [Step: 2] That’s very thoughtful of you. [Your Name] has indeed been quite busy. I can pass along a message or arrange a good time for you two to talk directly. Would you prefer a callback?\n",
      "A callback would be wonderful... [Step: 3] Absolutely, Aunt Clara. I’ll make sure [Your Name] receives the message and knows you’re thinking of him. Is there anything particular you’d like me to pass along?\n",
      "Just tell him I love him and hope to catch up soon... [Step: 4] I’ll definitely pass that along with your love. The idea of a family dinner sounds wonderful. I’ll note that as a suggestion for him.\n",
      "Thank you, dear... [Step: 5] Will do, Aunt Clara. Take care, and we’ll be in touch soon.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      " \u001b[31mPositive.The\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, generated in zip(prompts, generated_tokens):\n",
    "    print(prompt, f\"\\x1b[31m{generated}\\x1b[0m\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7e6fa5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Positive.</s>',\n",
       " 'Positive.</s>',\n",
       " 'Negative.The',\n",
       " 'Neutral.',\n",
       " 'Neutral.',\n",
       " 'Neutral.',\n",
       " 'Positive.</s>',\n",
       " 'Positive.</s>',\n",
       " 'Neutral.',\n",
       " 'Positive.The',\n",
       " 'Neutral.',\n",
       " 'Neutral</s>',\n",
       " 'Neutral</s>',\n",
       " 'Neutral</s>',\n",
       " 'Neutral</s>',\n",
       " 'Neutral.',\n",
       " 'Positive.</s>',\n",
       " 'Neutral.',\n",
       " 'Neutral.',\n",
       " 'Positive.The']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c6a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efece64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c24d653",
   "metadata": {},
   "source": [
    "## Continuous Batching\n",
    "The key idea behind continuous batching is constantly swap out requests from the batch that have completed generation for requests in the queue that are waiting to be processed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc296f-662e-44eb-9008-98eac2121517",
   "metadata": {},
   "source": [
    "![Continuous](ContinousBatching.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13e2dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77cefc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed the random number generator so our results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 60\n",
    "batch_size = 10\n",
    "max_tokens = 4\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[0], max_tokens)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "batches = [\n",
    "    request_queue[i:i + batch_size]\n",
    "    for i in range(0, len(request_queue), batch_size)\n",
    "]\n",
    "\n",
    "len(request_queue), len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d831b635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0437a55827304b689f67bd15a49667ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bs=10:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 49.13\n"
     ]
    }
   ],
   "source": [
    "# Processing batches\n",
    "# generate tokens for all batches and record duration\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(batches), desc=f\"bs={batch_size}\") as pbar:\n",
    "    for i, batch in enumerate(batches):\n",
    "        # to accommodate all the requests with our \n",
    "        # current implementation, we take the max of\n",
    "        # all the tokens to generate among the requests\n",
    "        #batch_max_tokens = [b[1] for b in batch]\n",
    "        #max_tokens = max(batch_max_tokens)\n",
    "        #print(max_tokens)\n",
    "        pbar.set_postfix({'max_tokens': max_tokens})\n",
    "        \n",
    "        batch_prompts = [b[0] for b in batch]\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "        generate_batch(inputs, max_tokens=max_tokens)\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(f\"Duration {duration_s:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0832ef57-e3de-44c4-930d-90e65201a03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "073a274f-5181-494c-9858-e88054068e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using the provided text below, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment analysis, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2860b2d4-554d-482f-9d78-5502c5438db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "306e98b5-7494-44a3-b159-b206aa0d1b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4449f882-5f2a-42b0-bcb8-04163c8d6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous batching\n",
    "\n",
    "from utils import init_batch, get_next_inputs, get_next_inputs, generate_next_token, merge_batches, filter_batch, generate_batch_tokens_with_past\n",
    "import copy\n",
    "\n",
    "def generate(model, tokenizer, requests):\n",
    "    global cached_batch\n",
    "    # seed the random number generator so our results are deterministic\n",
    "    random.seed(42)\n",
    "    device = model.device\n",
    "    # constants\n",
    "    batch_size = 8\n",
    "    request_queue = copy.copy(requests)\n",
    "    \n",
    "    responses = [None] * len(requests)\n",
    "\n",
    "    # and run the initial prefill step\n",
    "    batch = init_batch(tokenizer, request_queue[:batch_size], device=device)\n",
    "    cached_batch = generate_next_token(model, tokenizer, batch, device=device)\n",
    "    request_queue = request_queue[batch_size:]\n",
    "\n",
    "    # continue until both the request queue is fully drained and every input\n",
    "    # within the cached_batch has completed generation\n",
    "    while len(request_queue) > 0 or cached_batch[\"input_ids\"].size(0) > 0:\n",
    "        batch_capacity = batch_size - cached_batch[\"input_ids\"].size(0)\n",
    "        if batch_capacity > 0 and len(request_queue) > 0:\n",
    "            #print(\"Prefilling, init new batch\")\n",
    "            # prefill\n",
    "            new_batch = init_batch(tokenizer, request_queue[:batch_capacity],  device=device)\n",
    "            #print(\"Generating new batch...\")\n",
    "            new_batch = generate_next_token(model, tokenizer, new_batch, device=device)\n",
    "            request_queue = request_queue[batch_capacity:]\n",
    "\n",
    "            # merge\n",
    "            #print(\"Merging Batches...\")\n",
    "            cached_batch = merge_batches(cached_batch, new_batch,  device=device)\n",
    "            #print(\"Batch merged.\")\n",
    "            \n",
    "\n",
    "        # decode\n",
    "        #print(\"Decoding...\")\n",
    "        cached_batch = generate_next_token(model, tokenizer, cached_batch, device=device)\n",
    "        # remove any inputs that have finished generation\n",
    "        #print(\"Filtering....\")\n",
    "        cached_batch, removed_indices, completed_responses = filter_batch(cached_batch, device=device)\n",
    "\n",
    "        for idx, resp in zip(removed_indices, completed_responses):\n",
    "            responses[idx] = resp\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d17e9a83-5f68-425b-9ca2-54f98f7521a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e4b5aed-6618-4d90-a257-e30a2cf6a909",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 10.94 MiB is free. Process 1550 has 132.68 MiB memory in use. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 6.84 GiB is allocated by PyTorch, and 637.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:11\u001b[0m\n",
      "Cell \u001b[0;32mIn[20], line 42\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, requests)\u001b[0m\n\u001b[1;32m     36\u001b[0m     cached_batch \u001b[38;5;241m=\u001b[39m merge_batches(cached_batch, new_batch,  device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#print(\"Batch merged.\")\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# decode\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#print(\"Decoding...\")\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m cached_batch \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# remove any inputs that have finished generation\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#print(\"Filtering....\")\u001b[39;00m\n\u001b[1;32m     45\u001b[0m cached_batch, removed_indices, completed_responses \u001b[38;5;241m=\u001b[39m filter_batch(cached_batch, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Kodelab/ServingLLMs/utils.py:83\u001b[0m, in \u001b[0;36mgenerate_next_token\u001b[0;34m(model, tokenizer, batch, device)\u001b[0m\n\u001b[1;32m     80\u001b[0m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens_remaining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m next_token_ids, past_key_values \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_batch_tokens_with_past\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(next_token_ids)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_next_inputs(batch, next_token_ids, past_key_values, next_tokens, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Kodelab/ServingLLMs/utils.py:66\u001b[0m, in \u001b[0;36mgenerate_batch_tokens_with_past\u001b[0;34m(model, inputs, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_batch_tokens_with_past\u001b[39m(model, inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     58\u001b[0m     \n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Check that inputs are all on same devide\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m#print(\"Generating from model, checking inputs....\")\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m#recursive_tensor_device_check(inputs, device)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m#print(\"Iinputs are ok.\")\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 66\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     69\u001b[0m     last_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1033\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1034\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         use_cache,\n\u001b[1;32m   1040\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:770\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    769\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 770\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    773\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:179\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:468\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    467\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 468\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/.miniconda/envs/lorax/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    512\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 10.94 MiB is free. Process 1550 has 132.68 MiB memory in use. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 6.84 GiB is allocated by PyTorch, and 637.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# constants\n",
    "queue_size = 60\n",
    "batch_size = 10\n",
    "max_tokens = 4\n",
    "\n",
    "request_queue = [\n",
    "    (prompts[i], max_tokens)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "responses = generate(model, tokenizer, request_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "066364bd-76e0-482b-8809-1d9c29f00ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 19 18:36:22 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     Off | 00000000:23:00.0 Off |                  N/A |\n",
      "| 60%   42C    P8              11W / 200W |   7962MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3060 Ti     Off | 00000000:2D:00.0  On |                  N/A |\n",
      "|  0%   49C    P8              19W / 200W |    344MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1392      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      1550    C+G   ...libexec/gnome-remote-desktop-daemon      132MiB |\n",
      "|    0   N/A  N/A     61335      C   ...ti/.miniconda/envs/lorax/bin/python     7812MiB |\n",
      "|    1   N/A  N/A      1392      G   /usr/lib/xorg/Xorg                          239MiB |\n",
      "|    1   N/A  N/A      1599      G   /usr/bin/gnome-shell                         96MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346cd92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba75401c-8c48-4dcc-98d4-bd8fc7f80b49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Using llama-cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b154712e-fddb-45fd-a935-18f3564c9482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/x2-3060ti/.miniconda/envs/lorax/lib/python3.11/site-packages (from aiohttp) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/x2-3060ti/.miniconda/envs/lorax/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.7)\n",
      "Downloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.3/272.3 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.1/328.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: multidict, frozenlist, yarl, aiosignal, aiohttp\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 frozenlist-1.4.1 multidict-6.0.5 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e728e-d530-43c0-8bc8-6f07bcac0bba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Single Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a89581d-3673-4133-a4d2-00de35fe65af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ceb25f-dd58-4a1f-9ffd-93d6a7a7f806",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Load quantized model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/mistral-7b-instruct-v0.2.Q4_K_M.gguf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Llama(\n\u001b[1;32m      8\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m      9\u001b[0m     n_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32768\u001b[39m,  \u001b[38;5;66;03m# The max sequence length to use - note that longer sequence lengths require much more resources\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     chat_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "### Load quantized model\n",
    "\n",
    "from llama_cpp import Llama\n",
    "model_name = 'models/mistral-7b-instruct-v0.2.Q4_K_M.gguf'\n",
    "\n",
    "\n",
    "model = Llama(\n",
    "    model_path=model_name,\n",
    "    n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "    n_gpu_layers=35,        # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    "    chat_format=\"llama-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e83b057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nHello, [Your Name]\\'s personal assistant speaking. How can I help?\\nThis is Ryan from City Power, calling about an urgent issue with your electricity bill. Immediate payment is required to avoid disconnection.\\nI was unaware of any outstanding bill. Can you specify the amount and the billing period in question?\\nThe outstanding amount is $450 for the last two months. We\\'ve sent several notices to your registered address without response.\\nTo verify this, I\\'ll need to check our records and get back to you. Can I have a contact number and reference for this call?\\nThere\\'s no time for that. We need to resolve this now to avoid service interruption. You can make a payment over the phone.\\nIt\\'s our policy not to make payments over the phone without verifying the authenticity of the claim. Please provide an email or a direct line where I can reach you after verification.\\nYou\\'re making a big mistake. This is your last chance to prevent a disruption in your service.\\nI\\'ll contact City Power directly through their official channels for any outstanding issues. Thank you.\\n********** END TEXT **********\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d0fafdc5-a025-4425-9ef3-5361a6dba2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13013.76 ms\n",
      "llama_print_timings:      sample time =       0.66 ms /     3 runs   (    0.22 ms per token,  4566.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13416.17 ms /   541 tokens (   24.80 ms per token,    40.32 tokens per second)\n",
      "llama_print_timings:        eval time =     560.09 ms /     2 runs   (  280.04 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:       total time =   14000.03 ms /   543 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 ms, sys: 931 ms, total: 955 ms\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Simple inference example\n",
    "output = model(\n",
    "  prompt, # Prompt\n",
    "  max_tokens=32,  # Generate up to 32 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "# Chat Completion API\n",
    "\n",
    "#llm = Llama(model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\n",
    "#llm.create_chat_completion(\n",
    "#    messages = [\n",
    "##        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "#        {\n",
    "#            \"role\": \"user\",\n",
    "#            \"content\": \"Write a story about llamas.\"\n",
    "#        }\n",
    "#    ]\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "687737a0-edc5-4727-a453-5268eacca8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment, no other comment.\\n\\n********** BEGIN TEXT **********\\nUsing the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nHello, this is the personal assistant of [Your Name]. How may I assist you today?\\nHi, I\\'m Chris...interested in the new line of eco-friendly kitchen products... [Step: 1] Hi Chris, it\\'s great to hear of your interest in our eco-friendly kitchen products. I\\'d be happy to provide more information. Are you looking for details on a specific product or the entire range?\\nI\\'m particularly interested in the biodegradable dishware and the compostable... [Step: 2] Those are two of our most popular eco-friendly options. Our biodegradable dishware...composted at home.\\nThat sounds exactly like what I\\'m looking for... [Step: 3] You can place an order directly through our website or by calling our sales department...set of 10.\\nDo you offer any discounts for bulk orders? I\\'m considering these for my café. [Step: 4] Yes, we do offer bulk order discounts...discuss this further if you\\'d like.\\nThat would be great, thank you. I\\'ll also need some information on shipping. [Step: 5] Absolutely, Chris. Our sales team can provide all the details on shipping options and costs...Is there anything else I can assist you with before I do?\\nNo, that\\'s everything for now. Thanks for your help! [Step: 6] You\\'re welcome, Chris. I\\'m transferring you now. Have a great day, and thank you for supporting eco-friendly products.\\n********** END TEXT **********\\n\\n********** END TEXT **********\\n[/INST]\\nPositive'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d1657e0d-0342-41ee-a6be-83ad42281809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 541, 'completion_tokens': 2, 'total_tokens': 543}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858acc4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Batch Processing\n",
    "\n",
    "Install llama-cpp-python: https://llama-cpp-python.readthedocs.io/en/latest/install/macos/ \\\n",
    "Launch the server\\\n",
    "config your ggml model path\\\n",
    "make sure it is gguf v2\\\n",
    "make sure it is q4_0\n",
    "\n",
    "To istall with CUDA support:\n",
    "CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python\n",
    "\n",
    "Run the following:\n",
    "```\n",
    "export MODEL='models/mistral-7b-instruct-v0.2.Q4_K_M.gguf' \n",
    "python -m llama_cpp.server --model $MODEL  --n_gpu_layers -1\n",
    "```\n",
    "\n",
    "It will start a server on default port 8000:\n",
    "```\n",
    "Guessed chat format: mistral-instruct\n",
    "INFO:     Started server process [64719]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9efb168e-fdaf-469a-aaea-cc0a7421f222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Using the provided text below, analyze the sentiment expressed within it. Please determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment, no other comment.\\n\\n********** BEGIN TEXT **********\\n{text}\\n********** END TEXT **********\\n[/INST]\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3586938a-c824-425c-9913-fb295370e491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this is [Your Name]'s assistant. How may I assist you today?\n",
      "Good morning, I'm Mark from QuickSupply Solutions. We've recently been selected as one of your new suppliers, and I need to verify some account details to set up our billing process.\n",
      "That's great news. Can you provide me with the agreement number or the contact name from our procurement department who managed this selection?\n",
      "I don't have that information on hand. I was just given your contact as the point person for verification. We need to confirm your company's billing address and account numbers to ensure there are no delays in our service delivery.\n",
      "Before we proceed with any verification, it's essential that we receive official communication from our procurement department regarding this new supplier relationship. Could you send us an email with your request and any relevant documents?\n",
      "I understand the need for security, but we're under a tight schedule to get everything set up. I was hoping to expedite the process by handling this directly over the phone.\n",
      "Our company policy requires all new supplier engagements to be fully documented and verified through our standard procurement process. I'll need to confirm your details with our procurement team before proceeding.\n",
      "This could really set us back. Aren't you able to make an exception just this once? We're talking about a potential delay in your supply chain.\n",
      "While I understand the implications of a delay, bypassing our security protocols is not something we can compromise on. Ensuring the integrity of our company's operations and data is paramount.\n",
      "You're making this more difficult than it needs to be. I'll have to report this back to my team.\n",
      "Please understand that our intention is not to cause inconvenience but to maintain security and compliance. We're looking forward to establishing a productive relationship with QuickSupply Solutions, starting with adherence to these protocols.\n",
      "Alright, I'll see what I can do about getting you the documentation.\n",
      "Thank you. We appreciate your understanding and cooperation. Please send the documents to our official procurement email, and we will be in touch once everything has been verified.\n",
      "I'll get back to you then. Goodbye.\n",
      "We look forward to it. Have a great day.\n"
     ]
    }
   ],
   "source": [
    "batch = [\"\\n\".join(df[df.CONVERSATION_ID==conv_id].TEXT.values.tolist()) for conv_id in range(68)]\n",
    "\n",
    "batch = [prompt_template.format(text=p) for p in batch]\n",
    "\n",
    "\n",
    "prompt = batch[random.randint(0,len(batch))]\n",
    "\n",
    "print(random_conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b66cd8-9ff1-42af-9d55-22f5ce9033b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl --request POST \\\n",
    "--url http://localhost:8080/completion \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--data '{\"prompt\": [\"<s>[INST] What is the capital of the US? [/INST]\", \"<s>[INST] What is the capital of France? [/INST]\"], \"n_predict\": 2048}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e40c309b-4baa-411c-bfb1-f0d5c0686289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Completions API\n",
    "\n",
    "url = \"http://localhost:8002/v1/completions\"\n",
    "\n",
    "\n",
    "prompt = {\n",
    "  \"prompt\": f\"\\n\\n### Instructions:\\n{random_conversation}\\n\\n### Response:\\n\",\n",
    "  \"stop\": [\n",
    "    \"\\n\",\n",
    "    \"###\"\n",
    "  ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187ef71-1d9b-4910-abe4-8562db6f8e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "00b72bbd-234f-4af4-a266-8f887a3fbfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.11 ms, sys: 2.5 ms, total: 6.61 ms\n",
      "Wall time: 343 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "r = requests.post(url, json=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d7d3aefb-e7bd-4053-9845-21515a264166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "116dd2c9-7521-44e3-bad1-2ef8e19fb4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [{\n",
    "  \"prompt\": f\"\\n\\n### Instructions:\\n{conversation}\\n\\n### Response:\\n\",\n",
    "  \"stop\": [\n",
    "    \"\\n\",\n",
    "    \"###\"\n",
    "  ]\n",
    "} for conversation in batch\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd37f988-34f6-4567-b187-6694d0a4fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulating Parallel Requests\n",
    "from aiohttp import ClientSession\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the async client\n",
    "\n",
    "async def predict(session: ClientSession, prompt: str) -> str:\n",
    "    print(\"Requesting\", url)\n",
    "    async with session.post(url, json=prompt) as resp:\n",
    "        reply = await resp.json()\n",
    "        #await sleep(2)  # for demo purposes\n",
    "        #print(\"Got response from\", url, text.strip().split(\"\\n\", 1)[0])\n",
    "        preds.append(reply['choices'][0]['text'])\n",
    "        #predicted_ms.append(reply['timings']['predicted_ms'])\n",
    "        #tokens_per_second.append(reply['timings']['predicted_per_second'])\n",
    "        \n",
    "        \n",
    "\n",
    "async def get_all(prompts: list[dict], num_concurrent: int) -> None:\n",
    "    prompt_iterator = iter(prompts)\n",
    "    keep_going = True\n",
    "    async with ClientSession() as session:\n",
    "        while keep_going:\n",
    "            tasks = []\n",
    "            for _ in range(num_concurrent):\n",
    "                try:\n",
    "                    nextone = next(prompt_iterator)\n",
    "                except StopIteration:\n",
    "                    keep_going = False\n",
    "                    break\n",
    "                new_task = asyncio.create_task(predict(session, nextone))\n",
    "                tasks.append(new_task)\n",
    "            await asyncio.gather(*tasks)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "01bb0276-399c-4980-a65a-062828c91148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting http://localhost:8002/v1/completions\n"
     ]
    }
   ],
   "source": [
    "# Test single request\n",
    "\n",
    "async with ClientSession() as session:\n",
    "    await predict(session, url, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cf6c8ee0-1967-404a-a82f-b502b6f7b7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative.']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7711293c-6efd-4362-8a60-7f7c1d619caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "Requesting http://localhost:8002/v1/completions\n",
      "CPU times: user 51.1 ms, sys: 16.3 ms, total: 67.5 ms\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Test small batch\n",
    "preds, predicted_ms, tokens_per_second = [], [], []\n",
    "\n",
    "\n",
    "event_loop = asyncio.get_event_loop()\n",
    "event_loop.run_until_complete(get_all(prompts, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7396ea5d-74c1-42a5-80c2-228ef3bbb106",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpreds\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074596a-86c6-4231-b37f-ab72e4c1ab2d",
   "metadata": {},
   "source": [
    "### Use llama.cpp server\n",
    "For a detailed explanation of the server parameters, have a look [here](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md)\n",
    "\n",
    "\n",
    "To install on linux with CUDA and 3060 GPU:\n",
    "\n",
    "`make LLAMA_CUDA=1 CUDA_DOCKER_ARCH=sm86`\n",
    "\n",
    "Download a quantized model from the HF [repo](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF):\n",
    "\n",
    "`huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "`\n",
    "\n",
    "Launch:\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16`\n",
    "- cb: continuous batching\n",
    "- np: number of slots (parallelism)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b4fdfe9-8451-413a-b34e-d0e256b163a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"\\n\".join(df[df.CONVERSATION_ID==conv_id].TEXT.values.tolist()) for conv_id in range(68)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f48d348-9e31-4ad0-8f78-7d33f564aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text. \\\n",
    "Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. \\\n",
    "Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
    "Reply only with the sentiment analysis, no other comment.\n",
    "\\n\n",
    "********** BEGIN TEXT **********\n",
    "{text}\n",
    "********** END TEXT **********\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad29ecce-c5dc-4956-9e16-256795c52677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment analysis, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [prompt_template.format(text=text) for text in texts]\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c7152-9abc-44e0-a14b-005ea6f557e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49de213c-fd7e-4c99-8e7c-c97c867f8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:8080/v1/completions\"\n",
    "\n",
    "prompts = [{\n",
    "    \"prompt\": f\"\\n\\n{text}\\n\\n\\n\",\n",
    "    \"n_predict\":4,\n",
    "    \"stop\": [\n",
    "        \"\\n\",\n",
    "        \"###\"\n",
    "      ]\n",
    "} for text in batch\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01145843-ae34-41af-ab16-4df12bb60bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '\\n\\nUsing the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment analysis, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n\\n\\n\\n',\n",
       " 'n_predict': 16,\n",
       " 'stop': ['\\n', '###']}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b979aa3-240f-43c9-b283-bb5cb55beb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import ClientSession\n",
    "\n",
    "async def predict(session: ClientSession, prompt: str) -> str:\n",
    "    print(\"Requesting\", url)\n",
    "    async with session.post(url, json=prompt) as resp:\n",
    "        reply = await resp.json()\n",
    "        #await sleep(2)  # for demo purposes\n",
    "        #print(\"Got response from\", url, text.strip().split(\"\\n\", 1)[0])\n",
    "        preds.append(reply['content'])\n",
    "        preds_ms.append(reply['timings']['prompt_ms']+reply['timings']['predicted_ms'])\n",
    "        tokens_per_second.append(reply['timings']['predicted_per_second'])\n",
    "\n",
    "async def get_all(prompts: list[dict], num_concurrent: int) -> None:\n",
    "    prompt_iterator = iter(prompts)\n",
    "    keep_going = True\n",
    "    async with ClientSession() as session:\n",
    "        while keep_going:\n",
    "            tasks = []\n",
    "            for _ in range(num_concurrent):\n",
    "                try:\n",
    "                    nextone = next(prompt_iterator)\n",
    "                except StopIteration:\n",
    "                    keep_going = False\n",
    "                    break\n",
    "                new_task = asyncio.create_task(predict(session, nextone))\n",
    "                tasks.append(new_task)\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "def results(preds, preds_ms, tokens_per_second):\n",
    "    failed = len([i for i, v in enumerate(preds) if not v])\n",
    "    predicted = len([i for i, v in enumerate(preds) if v])\n",
    "    mean_pred_ms = np.mean([p for i, p in enumerate(preds_ms) if preds[i]])\n",
    "    mean_tokens_pre_sec = np.mean([p for i, p in enumerate(tokens_per_second) if preds[i]])\n",
    "\n",
    "    print(f\"Succesfull predictions: {predicted}\")\n",
    "    print(f\"Failed predictions: {failed}\")\n",
    "    print(f\"Mean prediction time: {mean_pred_ms} ms\")\n",
    "    print(f\"Tokens per second: {mean_tokens_pre_sec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "42fd9fb2-3abd-4733-8bcb-962769f73377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Replied: Neutral in 314.26 ms.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single test\n",
    "async with ClientSession() as session:\n",
    "    async with session.post(url, json=prompts[0]) as resp:\n",
    "        reply = await resp.json()\n",
    "\n",
    "f\"Replied: {reply['content']} in {reply['timings']['prompt_ms']+reply['timings']['predicted_ms']} ms.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d2c94868-bc1c-4df0-98c1-7103f8c58edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch20 = prompts[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "549befe7-30c7-4b8c-ab7e-772f786a76b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3ba69dae-64e9-44c0-bc2a-a3d3d8061ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "CPU times: user 38.3 ms, sys: 8.95 ms, total: 47.2 ms\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Test small batch\n",
    "preds, preds_ms, tokens_per_second = [], [], []\n",
    "\n",
    "\n",
    "event_loop = asyncio.get_event_loop()\n",
    "event_loop.run_until_complete(get_all(prompts, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bac6c9-2129-4267-ac2c-bce4b883cd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8484d9db-6080-4ef7-b30e-7d924a4d9ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfull predictions: 57\n",
      "Failed predictions: 11\n",
      "Mean prediction time: 1713.1050877192984 ms\n",
      "Mean Tokens per second: 16.11586838643529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results(preds, preds_ms, tokens_per_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7ebfb-80d9-4265-9509-aa5dbab41c43",
   "metadata": {},
   "source": [
    "#### Experiment Results\n",
    "\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16 -ngl 32`\n",
    "\n",
    "```\n",
    "Wall time: 6.72 s\n",
    "num_concurrent = 4:\n",
    "Mean prediction time: 1103.7863888888887 ms\n",
    "Tokens per second: 32.80537293142969\n",
    "```\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16 -ngl 64`\n",
    "```\n",
    "Wall time: 5.33 s\n",
    "num_concurrent = 4:\n",
    "Mean prediction time: 845.2819999999999 ms\n",
    "Tokens per second: 29.1984079263616\n",
    "````\n",
    "\n",
    "\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16 -ngl 128`\n",
    "```\n",
    "Wall time: 5.19 s\n",
    "Mean prediction time: 769.3069375 ms\n",
    "Tokens per second: 32.999395026172806\n",
    "```\n",
    "\n",
    "Whole Batch (68 calls)\n",
    "```\n",
    "num_concurrent = 4:\n",
    "Wall time: 21.6 s\n",
    "Mean prediction time: 963.6794210526316 ms\n",
    "Tokens per second: 34.85750130200938\n",
    "```\n",
    "\n",
    "*****BEST SO FAR****)\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16 -ngl 128`\n",
    "```\n",
    "num_concurrent = 16\n",
    "Wall time: 19.1 s\n",
    "Mean prediction time: 2675.847711864407 ms\n",
    "Mean Tokens per second: 7.655980738400953\n",
    "```\n",
    "??? wall time lower but results look worse\n",
    "```\n",
    "num_concurrent = 8\n",
    "Wall time: 20.6 s\n",
    "Mean prediction time: 1713.1050877192984 ms\n",
    "Mean Tokens per second: 16.11586838643529\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc04a09-9cde-459b-ae12-2d675897765a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b425d5b-3163-4c36-b11b-252d88356836",
   "metadata": {},
   "source": [
    "# Lorax\n",
    "\n",
    "Installation Guide can be found [here](https://loraexchange.ai/getting_started/local/)\n",
    "\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265bec5-5e80-4d25-9712-7770cd3b7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lorax import AsyncClient\n",
    "\n",
    "\n",
    "# Batch of prompts to submit\n",
    "prompts = [\n",
    "    \"The quick brown fox\",\n",
    "    \"The rain in Spain\",\n",
    "    \"What comes up\",\n",
    "]\n",
    "\n",
    "# Initialize the async client\n",
    "endpoint_url = \"http://127.0.0.1:8080\"\n",
    "async_client = AsyncClient(endpoint_url)\n",
    "\n",
    "# Submit all prompts and do not block on the response\n",
    "t0 = time.time()\n",
    "futures = []\n",
    "for prompt in prompts:\n",
    "    resp = async_client.generate(prompt, max_new_tokens=64)\n",
    "    futures.append(resp)\n",
    "\n",
    "# Await the completion of all the prompt requests\n",
    "responses = await asyncio.gather(*futures)\n",
    "\n",
    "# Print responses\n",
    "# Responses will always come back in the same order as the original list\n",
    "for resp in responses:\n",
    "    print(resp.generated_text)\n",
    "\n",
    "# Print duration to process all requests in batch\n",
    "print(\"duration (s):\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994609ed-5cba-453d-9709-7106f2c0e7cf",
   "metadata": {},
   "source": [
    "# Finetuned Mistral: Zephyr-7b-dpo-qlora\n",
    "[zephyr-7b-dpo-qlora](https://huggingface.co/alignment-handbook/zephyr-7b-dpo-qlora/tree/main)\n",
    "\n",
    "\n",
    "Mistral-7b fine-tuned on Zephyr-7B dataset with DPO, a dataset of conversations. So this is a Mistral-7B finetuned to be an helpful assistant.\\\n",
    "The model has been finetuned using the [PEFT](https://huggingface.co/docs/peft/en/index) library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4d7bfe39-cdb2-4860-8fc6-6db1f9cb6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a942363-d9e0-4d28-ba5c-08adb1e67725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                  | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"alignment-handbook/zephyr-7b-dpo-qlora\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model = PeftModel.from_pretrained(model, \"alignment-handbook/zephyr-7b-dpo-qlora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5409c25-a7e4-41f2-9e82-abea58aa15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
