{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46839b74-a181-4cc3-a27c-b1690d2c03f4",
   "metadata": {},
   "source": [
    "# Efficient Language Model Serving \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5fa4a70-f796-481d-b643-5317cc78f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3fee95-5ffe-4c73-a075-6970226fbe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 KB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.21\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 KB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.2.0\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 KB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 KB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.8)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 KB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, tzdata, tqdm, sympy, safetensors, regex, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, kiwisolver, fsspec, fonttools, filelock, cycler, triton, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, contourpy, tokenizers, nvidia-cusolver-cu12, matplotlib, transformers, torch, accelerate\n",
      "Successfully installed accelerate-0.29.3 contourpy-1.2.1 cycler-0.12.1 filelock-3.13.4 fonttools-4.51.0 fsspec-2024.3.1 huggingface-hub-0.22.2 kiwisolver-1.4.5 matplotlib-3.8.4 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pandas-2.2.2 pillow-10.3.0 regex-2024.4.16 safetensors-0.4.3 sympy-1.12 tokenizers-0.19.1 torch-2.2.2 tqdm-4.66.2 transformers-4.40.0 triton-2.2.0 tzdata-2024.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib torch transformers accelerate pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776991f7-6366-473f-975a-aa69b3ca2da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember to set CUDA_VISIBLE_DEVICES=0 before launching the notebook\n",
    "\n",
    "### Import required packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0af75eb-dae0-4036-9a17-639f2b3b4e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb81e44b1544a16a626776ca37b2141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Authenticate to HuggingFace Hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67ab6813-a3ee-4920-86d3-54ef78451dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 23 15:07:16 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:06:00.0 Off |                  N/A |\n",
      "| 43%   45C    P8              27W / 300W |   7077MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      8263      C   ./server                                   7068MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e70ca-9748-42bf-9cd1-63a5a457052f",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff7514f-96ec-4c30-b92d-d1bbf8036c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONVERSATION_ID</th>\n",
       "      <th>CONVERSATION_STEP</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CONTEXT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FEATURES</th>\n",
       "      <th>ANNOTATIONS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Good morning, this is [Your Name]'s personal assistant. How can I help you today?</td>\n",
       "      <td>Standard opening exchange</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>Hello, my name is Jamie. I'm interested in volunteering... [Step: 1] Good morning, Jamie! It's wonderful to hear about your interest in volunteering. We're always looking for passionate individuals to join our efforts.</td>\n",
       "      <td>Encourages the caller's interest</td>\n",
       "      <td>neutral</td>\n",
       "      <td>welcoming, positive_tone</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes, I'm really passionate about environmental conservation... [Step: 2] That's fantastic, Jamie. There are no specific requirements; we welcome volunteers of all backgrounds. The main need is for willingness to participate and follow the event guidelines for safety and effectiveness.</td>\n",
       "      <td>Reinforces anyone can volunteer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>inclusive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Great, how do I sign up, and where can I find more information... [Step: 3] You can sign up through our official website. There's a volunteer registration section where you'll find all the information about the event, including date, location, and what to bring. Would you like me to guide you to the webpage or send you the link?</td>\n",
       "      <td>Demonstrates flexibility</td>\n",
       "      <td>neutral</td>\n",
       "      <td>helpful_tone, offers_options</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>Could you send me the link, please? And my email is jamie.green@example.com. [Step: 4] Absolutely, Jamie. I'll send the link to your email along with additional resources about the event. You should receive it within a few minutes.</td>\n",
       "      <td>Fulfills caller's request quickly</td>\n",
       "      <td>neutral</td>\n",
       "      <td>prompt_action</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CONVERSATION_ID  CONVERSATION_STEP  \\\n",
       "0                6                  1   \n",
       "1                6                  2   \n",
       "2                6                  3   \n",
       "3                6                  4   \n",
       "4                6                  5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                         TEXT  \\\n",
       "0                                                                                                                                                                                                                                                           Good morning, this is [Your Name]'s personal assistant. How can I help you today?   \n",
       "1                                                                                                                  Hello, my name is Jamie. I'm interested in volunteering... [Step: 1] Good morning, Jamie! It's wonderful to hear about your interest in volunteering. We're always looking for passionate individuals to join our efforts.   \n",
       "2                                               Yes, I'm really passionate about environmental conservation... [Step: 2] That's fantastic, Jamie. There are no specific requirements; we welcome volunteers of all backgrounds. The main need is for willingness to participate and follow the event guidelines for safety and effectiveness.   \n",
       "3  Great, how do I sign up, and where can I find more information... [Step: 3] You can sign up through our official website. There's a volunteer registration section where you'll find all the information about the event, including date, location, and what to bring. Would you like me to guide you to the webpage or send you the link?   \n",
       "4                                                                                                     Could you send me the link, please? And my email is jamie.green@example.com. [Step: 4] Absolutely, Jamie. I'll send the link to your email along with additional resources about the event. You should receive it within a few minutes.   \n",
       "\n",
       "                             CONTEXT    LABEL                      FEATURES  \\\n",
       "0          Standard opening exchange  neutral                           NaN   \n",
       "1   Encourages the caller's interest  neutral      welcoming, positive_tone   \n",
       "2    Reinforces anyone can volunteer  neutral                     inclusive   \n",
       "3           Demonstrates flexibility  neutral  helpful_tone, offers_options   \n",
       "4  Fulfills caller's request quickly  neutral                 prompt_action   \n",
       "\n",
       "  ANNOTATIONS  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# Load sample dataset\n",
    "df = pd.read_csv(\"better30.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857714f4-029c-4e4d-9889-c0a82c661df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 68)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.CONVERSATION_ID.min(), df.CONVERSATION_ID.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e67fd7-a8b8-4367-9b89-974a9db7b6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "conv_id=random.randint(0, 68)\n",
    "print(conv_id)\n",
    "\n",
    "random_conversation = '\\n'.join(df[df.CONVERSATION_ID==conv_id].TEXT.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e568bd0d-febb-4840-a5db-c5ddc743f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST] Using the provided text below, perform a sentiment analysis of the text. \\\n",
    "Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. \\\n",
    "Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
    "Reply only with the sentiment analysis, no other comment.\n",
    "\\n\n",
    "********** BEGIN TEXT **********\n",
    "{text}\n",
    "********** END TEXT **********\n",
    "[/INST]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c6ceb59-16c0-49a2-9f1d-b904a8ffe359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Using the provided text below, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
      "Reply only with the sentiment analysis, no other comment.\n",
      "\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Hi, wow, this is fancy. I’m Jordan... [Step: 1] Hello, Jordan! It’s wonderful that you’re looking to reconnect with [Your Name]. He often reminisces about his college days. May I ask how you two met or a shared memory? This will help me ensure I relay your message accurately.\n",
      "Sure, we met during our freshman year in the dorms... [Step: 2] That’s a great story, Jordan, and exactly the kind of thing [Your Name] would do. I can see you two shared some memorable times. Would you like me to arrange a call or meeting for you to catch up directly with [Your Name]?\n",
      "I’d really appreciate that... [Step: 3] Of course, Jordan. I’ll pass on your message and the lovely story about the coffee maker. It’ll surely bring back good memories. Can you provide a contact number or email where [Your Name] can reach you?\n",
      "Yes, I’ll give you my email and phone number... [Step: 4] You’re welcome, Jordan. I’ve noted your contact details. [Your Name] will be informed, and I’m sure he’ll be excited to catch up.\n",
      "Thank you so much. Have a great day! [Step: 5] My pleasure, Jordan. Have a wonderful day as well.\n",
      "Hello, this is the personal assistant of [Your Name]. How may I assist you today?\n",
      "Hi, wow, this is fancy. I’m Jordan... [Step: 1] Hello, Jordan! It’s wonderful that you’re looking to reconnect with [Your Name]. He often reminisces about his college days. May I ask how you two met or a shared memory? This will help me ensure I relay your message accurately.\n",
      "Sure, we met during our freshman year in the dorms... [Step: 2] That’s a great story, Jordan, and exactly the kind of thing [Your Name] would do. I can see you two shared some memorable times. Would you like me to arrange a call or meeting for you to catch up directly with [Your Name]?\n",
      "I’d really appreciate that... [Step: 3] Of course, Jordan. I’ll pass on your message and the lovely story about the coffee maker. It’ll surely bring back good memories. Can you provide a contact number or email where [Your Name] can reach you?\n",
      "Yes, I’ll give you my email and phone number... [Step: 4] You’re welcome, Jordan. I’ve noted your contact details. [Your Name] will be informed, and I’m sure he’ll be excited to catch up.\n",
      "Thank you so much. Have a great day! [Step: 5] My pleasure, Jordan. Have a wonderful day as well.\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(text=random_conversation)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "828b4a0c-b74c-4984-83f6-8ae5d7e029aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch processing\n",
    "\n",
    "batch = [\"\\n\".join(df[df.CONVERSATION_ID==conv_id].TEXT.values.tolist()) for conv_id in range(68)]\n",
    "\n",
    "#batch = [prompt_template.format(text=p) for p in batch]\n",
    "\n",
    "len(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a21ade3-a7be-4bde-a343-55d4dcdca2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I'm interested in registering but had a few questions. [Step: 1] Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it's sam.photography@example.com. [Step: 5] Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that's everything. Thanks for your help! [Step: 6] You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b39a2b87-eebc-423f-813c-56639cc01ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [prompt_template.format(text=p) for p in batch]\n",
    "len(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411cf83b-520f-47c9-843f-a19eb08d72a2",
   "metadata": {},
   "source": [
    "# Model: [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "\n",
    "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e9bcf8a-c6fe-44e8-99db-f1ea32bfe0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4493686460374ead92e1682260fea4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1508dcc6a71446ed821bb70e5a72b664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ad2a2833bc4038949d85300067f030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5529d98a2424672bcf6d70675402df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd6f47361a446278a0f4ac6985c8192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf80122fd574a30804ef8347ee2647a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4bcb98307a4ddcbd6993a8a2e950d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdd2f3fabad4de9bea9bd9ad102ddeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b8f7808ec9466e985ccc620c036a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c138f193ea6942c2b9ed691f1760238c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb1a3b142554bae8d0167be2c377f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49778bd66fd484cb77047bc2f3690e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Load Model: it will take few minutes if not cached\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map='cuda')\n",
    "\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e618c575-80ff-4e2a-8503-7b73e3971a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d45747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory footprint: 15.02G\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model memory footprint: {model.get_memory_footprint()/1e9:.2f}G\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7accabc0-f7cf-4193-8ef1-b9d58b9b7d38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generate Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6178114b-fe85-445f-bee5-454efdac6b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Using the provided text below, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment analysis, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\n\\n********** END TEXT **********\\n[/INST]\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generate  tokens\n",
    "text = batch[random.randint(0, len(batch))]\n",
    "\n",
    "prompt = prompt_template.format(text=text)\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6ed32c6-7d0d-4ed9-8f25-9e4c36d06adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 29.4 ms, total: 29.4 ms\n",
      "Wall time: 26.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 661])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# input prompt tokenization\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba022230-3947-4c23-8fb8-65915ab3b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text generation helper function\n",
    "# The following helper function generates the next tokens given a set of input tokens\n",
    "\n",
    "def generate_token(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f742e336-0f06-4ffe-8ae2-68762ff6bfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 ms, sys: 28 ms, total: 196 ms\n",
      "Wall time: 195 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ne'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "token = generate_token(inputs)\n",
    "\n",
    "tokenizer.decode(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50125a26-6171-494e-82cd-1da5e8497943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper function to generate multiple tokens in a loop\n",
    "# Track the time it takes to generate each token\n",
    "def generate_tokens(inputs, n_tokens):\n",
    "\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    durations_s = []\n",
    "    for _ in range(n_tokens):\n",
    "        t0 = time.time()\n",
    "        next_token_id = generate_token(next_inputs)\n",
    "        durations_s += [time.time() - t0]\n",
    "    \n",
    "        next_inputs = {\n",
    "            \"input_ids\": torch.cat(\n",
    "                [next_inputs[\"input_ids\"], next_token_id.reshape((1, 1))],\n",
    "                dim=1),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]], device=device)],\n",
    "                dim=1),\n",
    "        }\n",
    "    \n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return generated_tokens, durations_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a85dd404-dcf2-4f50-b11f-0b04bc100e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 694 ms, sys: 92.9 ms, total: 787 ms\n",
      "Wall time: 786 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokens, durations = generate_tokens(inputs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9afe535c-29bb-4e4f-bbcb-ad7c529f9198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ne', 'ut', 'ral', '.']\n",
      "0.26729297637939453\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print(sum(durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "973ed96a-6ef3-41db-8315-ac898f4f68d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot token generation time\n",
    "# The x-axis here is the token number\n",
    "# The y-axis is the time to generate a token in millisenconds (ms)\n",
    "# **Note**: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "plt.plot(durations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce86f0d-e3e4-4d08-b746-465b43e17a0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### KV-caching\n",
    "\n",
    "KV-caching is a technique to speed up token generation by storing some of the tensors in the attention head for use in subsequent generation steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4feb1759-f169-483a-bba4-7723800b51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speeding up text generation with KV-caching\n",
    "# KV-caching is a technique to speed up token generation by storing some of the tensors in the attention head for use in subsequent generation steps\n",
    "# - Modify the generate helper function to return the next token and the key/value tensors\n",
    "\n",
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "\n",
    "\n",
    "def generate_tokens_kv(inputs, n_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    durations_cached_s = []\n",
    "    for _ in range(n_tokens):\n",
    "        t0 = time.time()\n",
    "        next_token_id, past_key_values = \\\n",
    "            generate_token_with_past(next_inputs)\n",
    "        durations_cached_s += [time.time() - t0]\n",
    "    \n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]], device=device)],\n",
    "                dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "    \n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens), durations_cached_s\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce0efb46-f544-4f01-9bbd-92435cd2aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.25 µs\n",
      "[0.08661174774169922, 0.05237460136413574, 0.027804851531982422, 0.027214765548706055]\n",
      "Neutral.\n"
     ]
    }
   ],
   "source": [
    "# Generate tokens using the updated helper function\n",
    "%time\n",
    "tokens, durations_cached = generate_tokens_kv(inputs, 4)\n",
    "\n",
    "print(f\"{durations_cached}\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7430a0b-4ec6-42b5-a129-f193ae4334b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare the execution time for the KV-cache function with the original helper function\n",
    "\n",
    "# **Note**: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "plt.plot(durations)\n",
    "plt.plot(durations_cached)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc5055-4094-4fdf-be07-1a94afb7bb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7994ca27-11ea-493e-82f7-e07f69e8626e",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "Tokenize list of prompts\\\n",
    "Add padding so that all prompts have the same number of tokens as the longest prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9c590ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f4648e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([68, 741]), torch.Size([68, 741]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple prompts of varying lengths to send\n",
    "# to the model at once\n",
    "#prompts = [prompt_template.format(text=text) for text in batch]\n",
    "\n",
    "# note: padding=True ensures the padding token\n",
    "# will be inserted into the tokenized tensors\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "inputs['input_ids'].size(), inputs[\"attention_mask\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2469c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values\n",
    "\n",
    "\n",
    "# Generate all tokens for some max tokens\n",
    "# position_ids tell the transformer the ordinal position of each token in the input sequence\n",
    "# for single input inference, this is just [0 .. n]\n",
    "# for n tokens, but for batch inference, we need to 0 out the padding tokens at the start of the sequence\n",
    "\n",
    "\n",
    "#?? durations\n",
    "def generate_batch(inputs, max_tokens):\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = \\\n",
    "            generate_batch_tokens_with_past(next_inputs)\n",
    "\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"],\n",
    "                torch.ones((next_token_ids.shape[0], 1), device=device),  \n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbab233-9d5d-4e40-858e-d4ffdd910f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "463979c9-f24c-4dcf-af38-786f01e23d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 7)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed the random number generator so our results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 68\n",
    "batch_size = 10\n",
    "max_tokens = 4\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[i], max_tokens)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "batches = [\n",
    "    request_queue[i:i + batch_size]\n",
    "    for i in range(0, len(request_queue), batch_size)\n",
    "]\n",
    "\n",
    "len(request_queue), len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "834fd1d3-4484-4b67-b723-9527b1bf5267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffea05ced1140abb9e3c4aab0653060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bs=10:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 10.31\n",
      "CPU times: user 8.56 s, sys: 1.81 s, total: 10.4 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Processing batches\n",
    "# generate tokens for all batches and record duration\n",
    "tokens = []\n",
    "\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(batches), desc=f\"bs={batch_size}\") as pbar:\n",
    "    for i, batch in enumerate(batches):\n",
    "        # to accommodate all the requests with our \n",
    "        # current implementation, we take the max of\n",
    "        # all the tokens to generate among the requests\n",
    "        #batch_max_tokens = [b[1] for b in batch]\n",
    "        #max_tokens = max(batch_max_tokens)\n",
    "        #print(max_tokens)\n",
    "        pbar.set_postfix({'max_tokens': max_tokens})\n",
    "        \n",
    "        batch_prompts = [b[0] for b in batch]\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "        tokens.extend(generate_batch(inputs, max_tokens=max_tokens))\n",
    "        pbar.update(1)\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(f\"Duration {duration_s:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c3aadd83-82e4-456f-bb7f-37601a277ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens generated:272\n",
      "Tokens per second: 26.38972552620158\n"
     ]
    }
   ],
   "source": [
    "n_tokens = sum([len(toks) for toks in tokens])\n",
    "print(f\"Tokens generated:{n_tokens}\")\n",
    "print(f\"Tokens per second: {n_tokens/duration_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a17cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e6fa5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfa07b4a-28b9-404b-ae80-4e7fff5fd503",
   "metadata": {},
   "source": [
    "### Continuous Batching\n",
    "The key idea behind continuous batching is constantly swap out requests from the batch that have completed generation for requests in the queue that are waiting to be processed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bcf30d-07f9-4ec9-b448-e7f7f6a8399d",
   "metadata": {},
   "source": [
    "![Continuous](ContinousBatching.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646cd53-d894-4fab-8309-dacda1d5ff91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5c684e0d-6123-462c-a83e-774fe8137e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import helpers\n",
    "#from helpers import init_batch, generate_next_token\n",
    "#from helpers import merge_batches, filter_batch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_batch(requests, device='cpu'):\n",
    "    prompts = [r[0] for r in requests]\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "    \n",
    "    return {\n",
    "        \"position_ids\": position_ids,\n",
    "        \"responses\": copy.copy(prompts),\n",
    "        \"tokens_remaining\": [r[1] for r in requests],\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "def generate_next_token(batch, device='cpu'):\n",
    "    inputs = copy.copy(batch)\n",
    "    inputs.pop(\"responses\")\n",
    "    inputs.pop(\"tokens_remaining\")\n",
    "    \n",
    "    next_token_ids, past_key_values = \\\n",
    "        generate_batch_tokens_with_past(inputs)\n",
    "    next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "    return get_next_inputs(\n",
    "        batch, next_token_ids, past_key_values, next_tokens, device=device)\n",
    "\n",
    "\n",
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values\n",
    "\n",
    "\n",
    "def merge_batches(batch1, batch2, device='cpu'):\n",
    "    # first find the max sequence length of the two batches\n",
    "    # this can be obtained from the second dimension \n",
    "    # of the attention mask\n",
    "    attn_mask1 = batch1[\"attention_mask\"]\n",
    "    attn_mask2 = batch2[\"attention_mask\"]\n",
    "    max_seq_len = max(attn_mask1.shape[1], attn_mask2.shape[1])\n",
    "    \n",
    "    # pad each mask (on the left) to the max sequence length\n",
    "    # attention mask uses 0 for padding\n",
    "    padding1 = max_seq_len - attn_mask1.shape[1]\n",
    "    padding2 = max_seq_len - attn_mask2.shape[1]\n",
    "    attn_mask1 = F.pad(attn_mask1, (padding1, 0), \"constant\", 0)\n",
    "    attn_mask2 = F.pad(attn_mask2, (padding2, 0), \"constant\", 0)\n",
    "    \n",
    "    # because we only append batches post decoding, \n",
    "    # we don't need to pad input_ids\n",
    "    # or position_ids. these are always length 1 \n",
    "    # in the sequence dimension\n",
    "    # however, we do need to pad the \n",
    "    # past_key_values, which have shape:\n",
    "    # [batch_size, num_heads, sequence_length, head_dim]\n",
    "    past_kv1 = batch1[\"past_key_values\"]\n",
    "    past_kv2 = batch2[\"past_key_values\"]\n",
    "    \n",
    "    padded_kv1 = []\n",
    "    for i in range(len(past_kv1)):\n",
    "        k, v = past_kv1[i]\n",
    "        k = F.pad(k, (0, 0, padding1, 0), \"constant\", 0)\n",
    "        v = F.pad(v, (0, 0, padding1, 0), \"constant\", 0)\n",
    "        padded_kv1.append((k, v))\n",
    "    \n",
    "    padded_kv2 = []\n",
    "    for i in range(len(past_kv2)):\n",
    "        k, v = past_kv2[i]\n",
    "        k = F.pad(k, (0, 0, padding2, 0), \"constant\", 0)\n",
    "        v = F.pad(v, (0, 0, padding2, 0), \"constant\", 0)     \n",
    "        padded_kv2.append((k, v))\n",
    "        \n",
    "    # now that everything has been padded to have\n",
    "    # consistent shapes, let's merge\n",
    "    input_ids = torch.concat(\n",
    "        [batch1[\"input_ids\"], batch2[\"input_ids\"]], dim=0)\n",
    "    position_ids = torch.concat(\n",
    "        [batch1[\"position_ids\"], batch2[\"position_ids\"]], dim=0) \n",
    "    attn_mask = torch.concat([attn_mask1, attn_mask2], dim=0)\n",
    "    \n",
    "    past_kv = []\n",
    "    for i in range(len(padded_kv1)):\n",
    "        k1, v1 = padded_kv1[i]\n",
    "        k2, v2 = padded_kv2[i]\n",
    "        k = torch.concat([k1, k2], dim=0)\n",
    "        v = torch.concat([v1, v2], dim=0)\n",
    "        past_kv.append((k, v))\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"position_ids\": position_ids,\n",
    "        \"attention_mask\": attn_mask,\n",
    "        \"past_key_values\": past_kv,\n",
    "        \"responses\": batch1[\"responses\"] + batch2[\"responses\"],\n",
    "        \"tokens_remaining\": batch1[\"tokens_remaining\"] + batch2[\"tokens_remaining\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_batch(batch):\n",
    "    # mark all rows with 0 tokens remaining for removal\n",
    "    remove_indices = []\n",
    "    for i, tokens_remaining in enumerate(batch[\"tokens_remaining\"]):\n",
    "        if tokens_remaining <= 0:\n",
    "            remove_indices.append(i)\n",
    "    \n",
    "    # first, define a mask used to subselect the indices to keep\n",
    "    # from each tensor, given the indices to remove\n",
    "    batch_size = batch[\"input_ids\"].size(0)\n",
    "    mask = torch.ones(batch_size, dtype=torch.bool)\n",
    "    mask[remove_indices] = False\n",
    "\n",
    "    # index into the tensors using the mask to remove rows\n",
    "    input_ids = batch[\"input_ids\"][mask]\n",
    "    position_ids = batch[\"position_ids\"][mask]\n",
    "    attention_mask = batch[\"attention_mask\"][mask]\n",
    "    responses = [\n",
    "        r \n",
    "        for i, r in enumerate(batch[\"responses\"])\n",
    "        if i not in remove_indices\n",
    "    ]\n",
    "    tokens_remaining = [\n",
    "        v \n",
    "        for i, v in enumerate(batch[\"tokens_remaining\"])\n",
    "        if i not in remove_indices\n",
    "    ]\n",
    "\n",
    "    past_key_values = batch[\"past_key_values\"]\n",
    "    new_past_key_values = []\n",
    "    for i in range(len(past_key_values)):\n",
    "        k, v = past_key_values[i]\n",
    "        k = k[mask]\n",
    "        v = v[mask]\n",
    "        new_past_key_values.append((k, v))\n",
    "    past_key_values = new_past_key_values\n",
    "    \n",
    "    if input_ids.size(0) > 0:\n",
    "        # next, as an optimization to avoid wasting \n",
    "        # compute cycles on padding tokens,\n",
    "        # we will left truncate the attention_mask \n",
    "        # and past_key_values to the longest\n",
    "        # remaining sequence length\n",
    "        # we obtain the longest sequence length by \n",
    "        # looking for the min first non-zero index\n",
    "        # of the attention mask\n",
    "        # cumprod ensures we stop accumulating when we see a 1\n",
    "        zero_mask = attention_mask == 0\n",
    "        cumprod = zero_mask.cumprod(dim=1)  \n",
    "        leading_zeros_count = cumprod.sum(dim=1)\n",
    "        min_leading_zeros = torch.min(leading_zeros_count)\n",
    "        truncation_offset = min_leading_zeros.item()\n",
    "\n",
    "        # do the trunction\n",
    "        attention_mask = attention_mask[:, truncation_offset:]\n",
    "        past_key_values = past_key_values\n",
    "        new_past_key_values = []\n",
    "        for i in range(len(past_key_values)):\n",
    "            k, v = past_key_values[i]\n",
    "            k = k[:, :, truncation_offset:, :]\n",
    "            v = v[:, :, truncation_offset:, :]\n",
    "            new_past_key_values.append((k, v))\n",
    "        past_key_values = new_past_key_values\n",
    "    \n",
    "    # return the new batch\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"position_ids\": position_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"past_key_values\": past_key_values,\n",
    "        \"responses\": responses,\n",
    "        \"tokens_remaining\": tokens_remaining,\n",
    "    }, remove_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "13e2dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "77cefc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 7)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed the random number generator so our results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 68\n",
    "batch_size = 10\n",
    "max_tokens = 4\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[i], max_tokens)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "batches = [\n",
    "    request_queue[i:i + batch_size]\n",
    "    for i in range(0, len(request_queue), batch_size)\n",
    "]\n",
    "\n",
    "len(request_queue), len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b685f657-48bc-445e-8109-c2e1a5fd5621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queue: 68 items.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4b0d093a8c4d189d1467b9f35f943c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bs=10:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 10.48\n",
      "CPU times: user 8.69 s, sys: 1.85 s, total: 10.5 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Continuous batching\n",
    "\n",
    "def strip_response(reply: str):\n",
    "    return reply.split(sep='[/INST]')[-1].strip()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# requests waiting to be processed\n",
    "# this time requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[i], max_tokens)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "responses = [None] * len(request_queue)\n",
    "\n",
    "print(f\"Queue: {len(request_queue)} items.\")\n",
    "\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(request_queue), desc=f\"bs={batch_size}\") as pbar:\n",
    "    # first, let's seed the initial cached_batch\n",
    "    # with the first `batch_size` inputs\n",
    "    # and run the initial prefill step\n",
    "    batch = init_batch(request_queue[:batch_size], device='cuda')\n",
    "    cached_batch = generate_next_token(batch, device='cuda')\n",
    "    #print([strip_response(reply) for reply in cached_batch['responses']])\n",
    "    request_queue = request_queue[batch_size:]\n",
    "\n",
    "    # continue until both the request queue is \n",
    "    # fully drained and every input\n",
    "    # within the cached_batch has completed generation\n",
    "    while (\n",
    "        len(request_queue) > 0 or\n",
    "        cached_batch[\"input_ids\"].size(0) > 0\n",
    "    ):\n",
    "        #print(f\"Remaining Queue: {len(request_queue)} items.\")\n",
    "        batch_capacity = (\n",
    "            batch_size - cached_batch[\"input_ids\"].size(0)\n",
    "        )\n",
    "        if batch_capacity > 0 and len(request_queue) > 0:\n",
    "            # prefill\n",
    "            new_batch = init_batch(request_queue[:batch_capacity], device='cuda')\n",
    "            new_batch = generate_next_token(new_batch, device='cuda')\n",
    "            #print([strip_response(reply) for reply in cached_batch['responses']])\n",
    "            #print(f\"Processing queue from item {batch_capacity}\")\n",
    "            request_queue = request_queue[batch_capacity:]\n",
    "\n",
    "            # merge\n",
    "            cached_batch = merge_batches(cached_batch, new_batch)\n",
    "\n",
    "        # decode\n",
    "        cached_batch = generate_next_token(cached_batch, device='cuda')\n",
    "        #print([strip_response(reply) for reply in cached_batch['responses']])\n",
    "        # remove any inputs that have finished generation\n",
    "        cached_batch, removed_indices = filter_batch(cached_batch)\n",
    "        #for idx, resp in zip(removed_indices, p_batch['responses']):\n",
    "        #    responses[idx] = resp\n",
    "        \n",
    "        pbar.update(len(removed_indices))\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(f\"Duration {duration_s:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb0282-ca7d-44c6-bc2f-5b2f00c2a341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d75d8fd-7b30-4ac5-87d0-477b5855d44e",
   "metadata": {},
   "source": [
    "## Using [Quantized model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)\n",
    "Requires llama-cpp-python\n",
    "\n",
    "<img src=\"ZeroPointQuantization.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ee3ea-8609-415a-83d3-5d5d5693601a",
   "metadata": {},
   "source": [
    "### Installation\n",
    "`CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python`\n",
    "\n",
    "Download Model:\n",
    "\n",
    "`huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir ./models --local-dir-use-symlinks False`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a04fab20-bc8c-45cd-b97b-c4991c464e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.64.tar.gz (37.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/lib/python3/dist-packages (from llama-cpp-python) (3.0.3)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.64-cp310-cp310-linux_x86_64.whl size=39222634 sha256=3c9b1e88c47f24d0b072914272fc41285589b80e1f009880c499087ad36574a7\n",
      "  Stored in directory: /root/.cache/pip/wheels/4a/69/6b/2fb423c0e39b66cc7f9b84ea837dbdc1eee8ea5f624e3eaeb6\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.64\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5129af0a-c550-4d7e-8a45-e26710938ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \\nReply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\\n\\n********** BEGIN TEXT **********\\n<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \\nReply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n[/INST]\\n</s>\\n\\n********** END TEXT **********\\n[/INST]\\n</s>\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.format(text=batch[0])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1605ec89-b083-426b-aec4-fd05f5feac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f78b379b-023b-4487-ac49-217464819d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from llama.cpp/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path=\"llama.cpp/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "  n_ctx=8192,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")\n",
    "\n",
    "\n",
    "# Chat Completion API\n",
    "\n",
    "#llm = Llama(model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\n",
    "#llm.create_chat_completion(\n",
    "#    messages = [\n",
    "#        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "#        {\n",
    " #           \"role\": \"user\",\n",
    "#            \"content\": \"Write a story about llamas.\"\n",
    "#        }\n",
    "#    ]\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "590477dc-b239-4b99-a73e-4a1ad2a71a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     724.77 ms\n",
      "llama_print_timings:      sample time =       1.29 ms /     4 runs   (    0.32 ms per token,  3091.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =      38.63 ms /     4 runs   (    9.66 ms per token,   103.54 tokens per second)\n",
      "llama_print_timings:       total time =      48.73 ms /     5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.5 ms, sys: 261 µs, total: 57.8 ms\n",
      "Wall time: 56.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  prompt, # Prompt\n",
    "  max_tokens=4,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6bb65d43-7a91-4b8e-a6e7-177444dfb990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-00b930a7-09e9-4ba9-9e64-5c876a67aef9',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1713886128,\n",
       " 'model': 'llama.cpp/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
       " 'choices': [{'text': '<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \\nReply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\\n\\n********** BEGIN TEXT **********\\n<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \\nReply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n[/INST]\\n</s>\\n\\n********** END TEXT **********\\n[/INST]\\n</s>\\n\\nPositive.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 637, 'completion_tokens': 4, 'total_tokens': 641}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "197b317a-f554-477d-a470-621a0daa07f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \\nReply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n[/INST]\\n</s>\\n',\n",
       " 'n_predict': 16,\n",
       " 'temperature': 0.1,\n",
       " 'stop': ['</s>']}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b902d09-f2d9-4437-bd97-c221c75058e5",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ce1f08c-5736-4e0d-9ac9-8b48f97be81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a567a516-13e2-480b-aedd-b193515a60f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \\nReply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n[/INST]\\n</s>\\n',\n",
       " 'n_predict': 16,\n",
       " 'temperature': 0.1,\n",
       " 'stop': ['</s>']}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "85028734-72f1-4865-b0ea-8edeff722159",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 4\n",
    "\n",
    "\n",
    "batch_prompts = [prompt['prompt'] for prompt in prompts[:1]]\n",
    "\n",
    "inputs = tokenizer(\n",
    "            batch_prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = \\\n",
    "            generate_batch_tokens_with_past(next_inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef1a76-f375-4fe4-82ab-e3227615c04c",
   "metadata": {},
   "source": [
    "### Using chat_template format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ea41ec33-3006-430e-9bf7-0f99858fee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0277bc5360c4c14b55e381a643f98d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774efa762b71440f80f5a02dc8cf70b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fb88a41281462e830badd2f1a8ad39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc33f61898a4c28a457df9ce8847994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da2804e613c4e19a1237e910ec54d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cf1faab04c4139a0fc888714513580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1777a55213d497fbcbba83f458df95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2760413d6404f098fa4ba2eeea3510c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ffdc84e7654adc9f3f79a99d2df8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bc83f58d58493a9e97f57549b17148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1106872add104191bbef69cc5c374701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 164.69 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 17.15 GiB is allocated by PyTorch, and 101.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m encodeds \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m encodeds\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(model_inputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2692\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2691\u001b[0m         )\n\u001b[0;32m-> 2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 164.69 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 17.15 GiB is allocated by PyTorch, and 101.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5029a1d-688d-4b75-82f6-f33717b71895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "576abb99-4cd1-46f3-bf6b-2fbf76b6b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values\n",
    "    \n",
    "\n",
    "def generate_batch(inputs, max_tokens):\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = \\\n",
    "            generate_batch_tokens_with_past(next_inputs)\n",
    "\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"],\n",
    "                torch.ones((next_token_ids.shape[0], 1), device=device),  \n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfe9ba-024c-4161-ad5f-0042dabd74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random number generator so our results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 68\n",
    "batch_size = 10\n",
    "max_tokens = 4\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[i], max_tokens)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "batches = [\n",
    "    request_queue[i:i + batch_size]\n",
    "    for i in range(0, len(request_queue), batch_size)\n",
    "]\n",
    "\n",
    "len(request_queue), len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Processing batches\n",
    "# generate tokens for all batches and record duration\n",
    "tokens = []\n",
    "\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(batches), desc=f\"bs={batch_size}\") as pbar:\n",
    "    for i, batch in enumerate(batches):\n",
    "        # to accommodate all the requests with our \n",
    "        # current implementation, we take the max of\n",
    "        # all the tokens to generate among the requests\n",
    "        #batch_max_tokens = [b[1] for b in batch]\n",
    "        #max_tokens = max(batch_max_tokens)\n",
    "        #print(max_tokens)\n",
    "        pbar.set_postfix({'max_tokens': max_tokens})\n",
    "        \n",
    "        batch_prompts = [b[0] for b in batch]\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "        tokens.extend(generate_batch(inputs, max_tokens=max_tokens))\n",
    "        pbar.update(1)\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(f\"Duration {duration_s:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efece64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066364bd-76e0-482b-8809-1d9c29f00ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346cd92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba75401c-8c48-4dcc-98d4-bd8fc7f80b49",
   "metadata": {},
   "source": [
    "# Using LLaMa-cpp\n",
    "The main goal of llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware locally and in the cloud.\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "Check if Nvidia Cuda Toolkit is installed by running `nvcc --version`\n",
    "\n",
    "If Nvidia Cuda Toolkit is not installed, then run:\n",
    "`sudo apt install nvidia-cuda-toolkit`\n",
    "\n",
    "\n",
    "`git clone https://github.com/ggerganov/llama.cpp.git`\n",
    "`cd llama.cpp`\n",
    "\n",
    "To install on linux with CUDA and 3090 GPU:\n",
    "\n",
    "\n",
    "`make LLAMA_CUDA=1 CUDA_DOCKER_ARCH=sm_86`\n",
    "\n",
    "Download a quantized model from the HF [repo](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF):\n",
    "\n",
    "`huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir ./models --local-dir-use-symlinks False`\n",
    "\n",
    "or \n",
    "\n",
    "`huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q5_K_M.gguf --local-dir ./models --local-dir-use-symlinks False`\n",
    "\n",
    "\n",
    "Launch:\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16 -ngl 33 -np 32`\n",
    "\n",
    "#`./server ---model 'models/Mistral-7B-Instruct-v0.2' --port 8080 -c 8092 -cb -np 16 -ngl 33 -np 32`\n",
    "- cb: continuous batching\n",
    "- np: number of slots (parallelism)\n",
    "\n",
    "\n",
    "For a detailed explanation of the server parameters, have a look [here](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab6e248-5f8a-4234-999c-8639f9968112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Thu_Nov_18_09:45:30_PST_2021\n",
      "Cuda compilation tools, release 11.5, V11.5.119\n",
      "Build cuda_11.5.r11.5/compiler.30672275_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4a61f25-9b40-4067-b399-475e98fc2856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4553c660-c319-441b-8b36-618bd7fa94ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.3)\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 multidict-6.0.5 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b4fdfe9-8451-413a-b34e-d0e256b163a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"\\n\".join(df[df.CONVERSATION_ID==conv_id].TEXT.values.tolist()) for conv_id in range(68)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f48d348-9e31-4ad0-8f78-7d33f564aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  \\\n",
    "determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. \\\n",
    "Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \n",
    "Reply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\n",
    "\n",
    "********** BEGIN TEXT **********\n",
    "{text}\n",
    "********** END TEXT **********\n",
    "[/INST]\n",
    "</s>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad29ecce-c5dc-4956-9e16-256795c52677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \n",
      "Reply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\n",
      "\n",
      "********** BEGIN TEXT **********\n",
      "Hello, this is [Your Name]'s personal assistant. How may I assist you today?\n",
      "Hi, I'm Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I'm interested in registering but had a few questions. [Step: 1] Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\n",
      "Thanks! I was wondering about the skill level required for participants. I'm fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\n",
      "That sounds perfect. What's the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\n",
      "A direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\n",
      "Sure, it's sam.photography@example.com. [Step: 5] Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\n",
      "No, that's everything. Thanks for your help! [Step: 6] You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\n",
      "********** END TEXT **********\n",
      "[/INST]\n",
      "</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = [prompt_template.format(text=text) for text in texts]\n",
    "print(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc5c7152-9abc-44e0-a14b-005ea6f557e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49de213c-fd7e-4c99-8e7c-c97c867f8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:8080/completion\"\n",
    "\n",
    "prompts = [{\n",
    "    \"prompt\": f\"{prompt}\",\n",
    "    \"n_predict\":16,\n",
    "    \"temperature\":0.1,\n",
    "    \"stop\": [\n",
    "        \"</s>\"\n",
    "      ]\n",
    "} for text in batch\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01145843-ae34-41af-ab16-4df12bb60bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \\nReply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n[/INST]\\n</s>\\n',\n",
       " 'n_predict': 16,\n",
       " 'temperature': 0.1,\n",
       " 'stop': ['</s>']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b979aa3-240f-43c9-b283-bb5cb55beb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import ClientSession\n",
    "\n",
    "async def predict(session: ClientSession, prompt: str) -> str:\n",
    "    #print(\"Requesting\", url)\n",
    "    async with session.post(url, json=prompt) as resp:\n",
    "        reply = await resp.json()\n",
    "        #await sleep(2)  # for demo purposes\n",
    "        #print(\"Got response from\", url, text.strip().split(\"\\n\", 1)[0])\n",
    "        preds.append(reply['content'])\n",
    "        preds_ms.append(reply['timings']['prompt_ms']+reply['timings']['predicted_ms'])\n",
    "        tokens_per_second.append(reply['timings']['predicted_per_second'])\n",
    "\n",
    "async def get_all(prompts: list[dict], num_concurrent: int) -> None:\n",
    "    prompt_iterator = iter(prompts)\n",
    "    keep_going = True\n",
    "    async with ClientSession() as session:\n",
    "        while keep_going:\n",
    "            tasks = []\n",
    "            for _ in range(num_concurrent):\n",
    "                try:\n",
    "                    nextone = next(prompt_iterator)\n",
    "                except StopIteration:\n",
    "                    keep_going = False\n",
    "                    break\n",
    "                new_task = asyncio.create_task(predict(session, nextone))\n",
    "                tasks.append(new_task)\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "def results(preds, preds_ms, tokens_per_second):\n",
    "    failed = len([i for i, v in enumerate(preds) if not v])\n",
    "    predicted = len([i for i, v in enumerate(preds) if v])\n",
    "    mean_pred_ms = np.mean([p for i, p in enumerate(preds_ms) if preds[i]])\n",
    "    mean_tokens_pre_sec = np.mean([p for i, p in enumerate(tokens_per_second) if preds[i]])\n",
    "\n",
    "    print(f\"Succesfull predictions: {predicted}\")\n",
    "    print(f\"Failed predictions: {failed}\")\n",
    "    print(f\"Mean prediction time: {mean_pred_ms} ms\")\n",
    "    print(f\"Tokens per second: {mean_tokens_pre_sec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42fd9fb2-3abd-4733-8bcb-962769f73377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Replied: **Chatbot Response:**\\n\\n**[Step: 1]** Hello in 225.578 ms.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single test\n",
    "async with ClientSession() as session:\n",
    "    async with session.post(url, json=prompts[0]) as resp:\n",
    "        reply = await resp.json()\n",
    "\n",
    "f\"Replied: {reply['content']} in {reply['timings']['prompt_ms']+reply['timings']['predicted_ms']} ms.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e02e3ec-bc4b-4c14-89bf-de65f0c8988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '**Chatbot Response:**\\n\\n**[Step: 1]** Hello',\n",
       " 'id_slot': 0,\n",
       " 'stop': True,\n",
       " 'model': 'models/mistral-7b-instruct-v0.2.Q5_K_M.gguf',\n",
       " 'tokens_predicted': 16,\n",
       " 'tokens_evaluated': 129,\n",
       " 'generation_settings': {'n_ctx': 253,\n",
       "  'n_predict': -1,\n",
       "  'model': 'models/mistral-7b-instruct-v0.2.Q5_K_M.gguf',\n",
       "  'seed': 4294967295,\n",
       "  'temperature': 0.10000000149011612,\n",
       "  'dynatemp_range': 0.0,\n",
       "  'dynatemp_exponent': 1.0,\n",
       "  'top_k': 40,\n",
       "  'top_p': 0.949999988079071,\n",
       "  'min_p': 0.05000000074505806,\n",
       "  'tfs_z': 1.0,\n",
       "  'typical_p': 1.0,\n",
       "  'repeat_last_n': 64,\n",
       "  'repeat_penalty': 1.0,\n",
       "  'presence_penalty': 0.0,\n",
       "  'frequency_penalty': 0.0,\n",
       "  'penalty_prompt_tokens': [],\n",
       "  'use_penalty_prompt_tokens': False,\n",
       "  'mirostat': 0,\n",
       "  'mirostat_tau': 5.0,\n",
       "  'mirostat_eta': 0.10000000149011612,\n",
       "  'penalize_nl': False,\n",
       "  'stop': ['</s>'],\n",
       "  'n_keep': 0,\n",
       "  'n_discard': 0,\n",
       "  'ignore_eos': False,\n",
       "  'stream': False,\n",
       "  'logit_bias': [],\n",
       "  'n_probs': 0,\n",
       "  'min_keep': 0,\n",
       "  'grammar': '',\n",
       "  'samplers': ['top_k',\n",
       "   'tfs_z',\n",
       "   'typical_p',\n",
       "   'top_p',\n",
       "   'min_p',\n",
       "   'temperature']},\n",
       " 'prompt': '<s>[INST]Using the text below between BEGIN TEXT and END TEXT, perform a sentiment analysis of the text, where you  determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\". \\nReply only with  \"Positive\", \"Neutral\", or \"Negative\", no other word or comment.\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********\\n[/INST]\\n</s>\\n',\n",
       " 'truncated': True,\n",
       " 'stopped_eos': False,\n",
       " 'stopped_word': False,\n",
       " 'stopped_limit': True,\n",
       " 'stopping_word': '',\n",
       " 'tokens_cached': 144,\n",
       " 'timings': {'prompt_n': 129,\n",
       "  'prompt_ms': 72.061,\n",
       "  'prompt_per_token_ms': 0.5586124031007752,\n",
       "  'prompt_per_second': 1790.150011795562,\n",
       "  'predicted_n': 16,\n",
       "  'predicted_ms': 153.517,\n",
       "  'predicted_per_token_ms': 9.5948125,\n",
       "  'predicted_per_second': 104.22298507657133}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2c94868-bc1c-4df0-98c1-7103f8c58edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch20 = prompts[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "549befe7-30c7-4b8c-ab7e-772f786a76b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3ba69dae-64e9-44c0-bc2a-a3d3d8061ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "Requesting http://localhost:8080/v1/completions\n",
      "CPU times: user 38.3 ms, sys: 8.95 ms, total: 47.2 ms\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Test small batch\n",
    "preds, preds_ms, tokens_per_second = [], [], []\n",
    "\n",
    "\n",
    "event_loop = asyncio.get_event_loop()\n",
    "event_loop.run_until_complete(get_all(prompts, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bac6c9-2129-4267-ac2c-bce4b883cd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8484d9db-6080-4ef7-b30e-7d924a4d9ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfull predictions: 57\n",
      "Failed predictions: 11\n",
      "Mean prediction time: 1713.1050877192984 ms\n",
      "Mean Tokens per second: 16.11586838643529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results(preds, preds_ms, tokens_per_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7ebfb-80d9-4265-9509-aa5dbab41c43",
   "metadata": {},
   "source": [
    "#### Experiment Results\n",
    "\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16 -ngl 32`\n",
    "\n",
    "```\n",
    "Wall time: 6.72 s\n",
    "num_concurrent = 4:\n",
    "Mean prediction time: 1103.7863888888887 ms\n",
    "Tokens per second: 32.80537293142969\n",
    "```\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16 -ngl 64`\n",
    "```\n",
    "Wall time: 5.33 s\n",
    "num_concurrent = 4:\n",
    "Mean prediction time: 845.2819999999999 ms\n",
    "Tokens per second: 29.1984079263616\n",
    "````\n",
    "\n",
    "\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16 -ngl 128`\n",
    "```\n",
    "Wall time: 5.19 s\n",
    "Mean prediction time: 769.3069375 ms\n",
    "Tokens per second: 32.999395026172806\n",
    "```\n",
    "\n",
    "Whole Batch (68 calls)\n",
    "```\n",
    "num_concurrent = 4:\n",
    "Wall time: 21.6 s\n",
    "Mean prediction time: 963.6794210526316 ms\n",
    "Tokens per second: 34.85750130200938\n",
    "```\n",
    "\n",
    "*****BEST SO FAR****)\n",
    "\n",
    "`./server --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080 -c 8092 -cb -np 16 -ngl 128`\n",
    "```\n",
    "num_concurrent = 16\n",
    "Wall time: 19.1 s\n",
    "Mean prediction time: 2675.847711864407 ms\n",
    "Mean Tokens per second: 7.655980738400953\n",
    "```\n",
    "??? wall time lower but results look worse\n",
    "```\n",
    "num_concurrent = 8\n",
    "Wall time: 20.6 s\n",
    "Mean prediction time: 1713.1050877192984 ms\n",
    "Mean Tokens per second: 16.11586838643529\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc04a09-9cde-459b-ae12-2d675897765a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b425d5b-3163-4c36-b11b-252d88356836",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Lorax\n",
    "\n",
    "For local installation look at [here](https://loraexchange.ai/getting_started/local/). Might require additional packages like ninja, [vLLM](https://docs.vllm.ai/en/latest/getting_started/installation.html), [flash-attention](https://github.com/Dao-AILab/flash-attention).\n",
    "\n",
    "Launch using: `lorax-launcher --model-id TheBloke/Mistral-7B-v0.1-AWQ --quantize awq`\n",
    "\n",
    "\n",
    "Otherwise launch with docker (requires the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html):\n",
    "\n",
    "`docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/predibase/lorax:latest --model-id TheBloke/Mistral-7B-v0.1-AWQ --quantize awq --max-batch-prefill-tokens 1024`\n",
    "\n",
    "\n",
    "To load quantized models, follow this [guide](https://loraexchange.ai/guides/quantization/)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "13ddc8bb-925e-48f2-9909-339ce94cf7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Using the text below between `BEGIN TEXT` and `END TEXT`, perform a sentiment analysis of the text. \\\n",
    "Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. \\\n",
    "Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\n",
    "Reply only with the sentiment analysis, no other comment.\n",
    "\\n\n",
    "********** BEGIN TEXT **********\n",
    "{text}\n",
    "********** END TEXT **********\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b924244-f956-4da3-901d-a96c66008c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using the text below between `BEGIN TEXT` and `END TEXT`, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment analysis, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [prompt_template.format(text=text) for text in texts]\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3c228b3f-711c-403a-b0de-b34c21c23bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Can you do a sentiment analysys of the following text: 'what an awful day!' Choose between POsitive, Neutral or Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d4eef755-109e-4170-afc7-62e65327fcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': \"[INST] Can you do a sentiment analysys of the following text: 'what an awful day!' Choose between POsitive, Neutral or Negative [/INST]\",\n",
       " 'parameters': {'max_new_tokens': 64}}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://127.0.0.1:8080/generate\"\n",
    "\n",
    "jprompt = {\n",
    "    \"inputs\": f\"[INST] {sample_text} [/INST]\",\n",
    "    \"parameters\": {\n",
    "            \"max_new_tokens\": 64\n",
    "        }\n",
    "} \n",
    "jprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb0bfb-0820-4a7d-a6f8-0482cc0b7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl 127.0.0.1:8080/generate \\\n",
    "    -X POST \\\n",
    "    -d '{\n",
    "        \"inputs\": \"[INST] Can you do a sentiment analysys of the following text: `what an awful day!` Choose between Positive, Neutral or Negative. [/INST]\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 64\n",
    "        }\n",
    "    }' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ff5ac2a4-8660-4ac6-ae74-445132b73558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using the text below between `BEGIN TEXT` and `END TEXT`, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment analysis, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT **********'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = batch[0]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "987884b3-9865-46de-80bf-233db3b28a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': '[INST] Using the text below between `BEGIN TEXT` and `END TEXT`, perform a sentiment analysis of the text. Determine whether the sentiment is positive, neutral, or negative based on the context, word choice, and overall tone. Once the analysis is complete, respond with the sentiment classification as either \"Positive\", \"Neutral\", or \"Negative\".\\nReply only with the sentiment analysis, no other comment.\\n\\n\\n********** BEGIN TEXT **********\\nHello, this is [Your Name]\\'s personal assistant. How may I assist you today?\\nHi, I\\'m Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I\\'m interested in registering but had a few questions. [Step: 1] Hi Sam, it\\'s great to hear of your interest in the photography workshop. I\\'d be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I\\'m fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What\\'s the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you\\'d like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I\\'ll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it\\'s sam.photography@example.com. [Step: 5] Thank you, Sam. You\\'ll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that\\'s everything. Thanks for your help! [Step: 6] You\\'re welcome, Sam. We look forward to having you at the workshop. Have a great day!\\n********** END TEXT ********** [/INST]', 'parameters': {'max_new_tokens': 64}}\n"
     ]
    }
   ],
   "source": [
    "jprompt = {\n",
    "    \"inputs\": f\"[INST] {prompt} [/INST]\",\n",
    "    \"parameters\": {\n",
    "            \"max_new_tokens\": 64\n",
    "        }\n",
    "}\n",
    "print(jprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ebca5e6-dd1f-4fef-a795-af9460949a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': \"[INST] Can you do a sentiment analysys of the following text: 'what an awful day!!' [/INST]\",\n",
       " 'parameters': {'max_new_tokens': 64}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "39fcc58d-ca80-40d3-a838-7088a124314c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': \"\\n\\n[INST] Can you do a sentiment analysys of the following text: 'what an awful day!' Choose between POsitive, Neutral or Negative [/INST]\\n\\n[INST] Can you do a sentiment analysys of the following text: 'what an awful day!' Choose between\"}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single test\n",
    "async with ClientSession() as session:\n",
    "    async with session.post(url, json=jprompt) as resp:\n",
    "        reply = await resp.json()\n",
    "\n",
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33fcc764-0e0c-4297-85d2-2f893fe319f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ClientResponse(http://127.0.0.1:8080/generate) [200 OK]>\n",
       "<CIMultiDictProxy('Content-Type': 'application/json', 'x-compute-type': 'gpu+optimized', 'x-compute-time': '392', 'x-compute-characters': '1986', 'x-total-time': '392', 'x-prompt-tokens': '490', 'x-generated-tokens': '1', 'x-total-tokens': '491', 'x-validation-time': '1', 'x-queue-time': '0', 'x-inference-time': '390', 'x-time-per-token': '390', 'x-model-id': 'TheBloke/Mistral-7B-v0.1-AWQ', 'Content-Length': '21', 'Access-Control-Allow-Origin': '*', 'Vary': 'origin', 'Vary': 'access-control-request-method', 'Vary': 'access-control-request-headers', 'Date': 'Sun, 21 Apr 2024 17:50:35 GMT')>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1e16a-a1f9-4c4e-be01-a70424aafb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfdf43eb-c2ba-40fa-9118-16e9ea6f50ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OpenAI Compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bcf688-0fe7-4cd8-b1c2-59fbecff09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ff16c7-8128-4a47-801a-7333f6debeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Arr, me hearties, 'tis a fine question!\n",
      "<|user|>\n",
      "I'm not sure if you're aware, but I'm a human</s>\n",
      "<|assistant|>\n",
      "Aye, matey, but yer a human who can eat helicopters!\n",
      "<|user|>\n",
      "I'm not sure if you're aware, but I'm not a helicopter</s>\n",
      "<|assistant\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://127.0.0.1:8080/v1\",\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"alignment-handbook/zephyr-7b-dpo-lora\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(\"Response:\", resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73727f80-3337-4048-9609-156bcdef97fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccfeb3ba-31c0-454d-bfe3-24abfe6c207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lorax-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1026631f-c868-411e-a16e-9f47dab4747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\n",
      "\n",
      "[INST] Natalia sold clips to 48\n"
     ]
    }
   ],
   "source": [
    "from lorax import Client\n",
    "\n",
    "client = Client(\"http://127.0.0.1:8080\")\n",
    "prompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\n",
    "\n",
    "print(client.generate(prompt, max_new_tokens=64).generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e265bec5-5e80-4d25-9712-7770cd3b7519",
   "metadata": {},
   "outputs": [
    {
     "ename": "GenerationError",
     "evalue": "Request failed during generation: Server error: Out of available cache blocks: asked 512, only 483 free blocks",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGenerationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(resp)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Await the completion of all the prompt requests\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mfutures)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Print responses\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Responses will always come back in the same order as the original list\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m responses:\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/lorax/client.py:509\u001b[0m, in \u001b[0;36mAsyncClient.generate\u001b[0;34m(self, prompt, adapter_id, adapter_source, merged_adapters, api_token, do_sample, max_new_tokens, ignore_eos_token, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, response_format, decoder_input_details, return_k_alternatives, details)\u001b[0m\n\u001b[1;32m    506\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m parse_error(resp\u001b[38;5;241m.\u001b[39mstatus, payload)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mGenerationError\u001b[0m: Request failed during generation: Server error: Out of available cache blocks: asked 512, only 483 free blocks"
     ]
    }
   ],
   "source": [
    "from lorax import AsyncClient\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# Batch of prompts to submit\n",
    "prompts = [\n",
    "    \"The quick brown fox\",\n",
    "    \"The rain in Spain\",\n",
    "    \"What comes up\",\n",
    "]\n",
    "\n",
    "# Initialize the async client\n",
    "endpoint_url = \"http://127.0.0.1:8080\"\n",
    "async_client = AsyncClient(endpoint_url)\n",
    "\n",
    "# Submit all prompts and do not block on the response\n",
    "t0 = time.time()\n",
    "futures = []\n",
    "for prompt in prompts:\n",
    "    resp = async_client.generate(prompt, max_new_tokens=4)\n",
    "    futures.append(resp)\n",
    "\n",
    "# Await the completion of all the prompt requests\n",
    "responses = await asyncio.gather(*futures)\n",
    "\n",
    "# Print responses\n",
    "# Responses will always come back in the same order as the original list\n",
    "for resp in responses:\n",
    "    print(resp.generated_text)\n",
    "\n",
    "# Print duration to process all requests in batch\n",
    "print(\"duration (s):\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d5132-54cf-4b17-9090-704fbf4983a5",
   "metadata": {},
   "source": [
    "# AWQ Model\n",
    "\n",
    "Mistral-7b AWQ model is available in the HF [repository](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3e74693-b428-44bb-be84-edec749e88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
    "\n",
    "#!pip install git+https://github.com/casper-hansen/AutoAWQ.git@1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
    "#!pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0032815-f9eb-4ea1-a0c4-66d6299f2392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5636688-4499-4e62-9f9b-14e83562ed58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, this is [Your Name]'s personal assistant. How may I assist you today?\\nHi, I'm Sam. I saw an ad about a photography workshop hosted by [Your Name] next month. I'm interested in registering but had a few questions. [Step: 1] Hi Sam, it's great to hear of your interest in the photography workshop. I'd be happy to help with any questions you have.\\nThanks! I was wondering about the skill level required for participants. I'm fairly new to photography. [Step: 2] The workshop is designed to accommodate all skill levels, from beginners to more experienced photographers. [Your Name] aims to ensure everyone can learn and grow, regardless of their starting point.\\nThat sounds perfect. What's the registration process? [Step: 3] You can register through our website. I can guide you through the steps if you'd like, or send you a direct link to the registration page.\\nA direct link would be great. Can you also tell me about the workshop fee? [Step: 4] Certainly, the fee for the workshop is $200, which includes all materials and lunch for the day. I'll email you the link to the registration page along with additional details about the workshop. May I have your email address?\\nSure, it's sam.photography@example.com. [Step: 5] Thank you, Sam. You'll receive an email shortly with all the information you need. Is there anything else I can assist you with today?\\nNo, that's everything. Thanks for your help! [Step: 6] You're welcome, Sam. We look forward to having you at the workshop. Have a great day!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "751fda67-b9eb-448d-9991-069a83d2d64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b289d4818f479a83aaaecb42e7c618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Parameter config in `MistralForCausalLM(config)` should be an instance of class `PretrainedConfig`. To create a model from a pretrained model use `model = MistralForCausalLM.from_pretrained(PRETRAINED_MODEL_NAME)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Mistral-7B-Instruct-v0.1-AWQ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoAWQForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_quantized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/awq/models/auto.py:95\u001b[0m, in \u001b[0;36mAutoAWQForCausalLM.from_quantized\u001b[0;34m(self, quant_path, quant_filename, max_seq_len, trust_remote_code, fuse_layers, use_exllama, use_exllama_v2, batch_size, safetensors, device_map, offload_folder, **config_kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     max_seq_len \u001b[38;5;241m=\u001b[39m config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     90\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens argument is deprecated... gracefully \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetting max_seq_len=max_new_tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAWQ_CAUSAL_LM_MODEL_MAP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_quantized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuse_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_exllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_exllama\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_exllama_v2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_exllama_v2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/awq/models/base.py:410\u001b[0m, in \u001b[0;36mBaseAWQForCausalLM.from_quantized\u001b[0;34m(self, model_path, model_type, model_filename, max_seq_len, torch_dtype, trust_remote_code, safetensors, fuse_layers, use_exllama, use_exllama_v2, device_map, offload_folder, **config_kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# [STEP 3] Load model\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m init_empty_weights():\n\u001b[0;32m--> 410\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Prepare WQLinear layers, replace nn.Linear\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_quantized_modules(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     use_exllama_v2\u001b[38;5;241m=\u001b[39muse_exllama_v2,\n\u001b[1;32m    424\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:435\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    434\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1307\u001b[0m, in \u001b[0;36mPreTrainedModel._from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1307\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;66;03m# restore default dtype if it was modified\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py:1083\u001b[0m, in \u001b[0;36mMistralForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m MistralModel(config)\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1207\u001b[0m, in \u001b[0;36mPreTrainedModel.__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m-> 1207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter config in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(config)` should be an instance of class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`PretrainedConfig`. To create a model from a pretrained model use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1211\u001b[0m     )\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# Save config and origin of the pretrained weights if given in model\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   1214\u001b[0m     config, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mget_default_dtype(), check_device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter config in `MistralForCausalLM(config)` should be an instance of class `PretrainedConfig`. To create a model from a pretrained model use `model = MistralForCausalLM.from_pretrained(PRETRAINED_MODEL_NAME)`"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a99c41a0-14ce-431d-8224-382e4a86e26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'MistralConfig {\n  \"_name_or_path\": \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"bits\": 4,\n    \"group_size\": 128,\n    \"quant_method\": \"awq\",\n    \"version\": \"gemm\",\n    \"zero_point\": true\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.40.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:111\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:165\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'MistralConfig {\n  \"_name_or_path\": \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"bits\": 4,\n    \"group_size\": 128,\n    \"quant_method\": \"awq\",\n    \"version\": \"gemm\",\n    \"zero_point\": true\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.40.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Mistral-7B-Instruct-v0.1-AWQ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2981\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2980\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2981\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[38;5;66;03m# In case one passes a config to `from_pretrained` + \"attn_implementation\"\u001b[39;00m\n\u001b[1;32m   2998\u001b[0m     \u001b[38;5;66;03m# override the `_attn_implementation` attribute to `attn_implementation` of the kwargs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3002\u001b[0m     \u001b[38;5;66;03m# we pop attn_implementation from the kwargs but this handles the case where users\u001b[39;00m\n\u001b[1;32m   3003\u001b[0m     \u001b[38;5;66;03m# passes manually the config to `from_pretrained`.\u001b[39;00m\n\u001b[1;32m   3004\u001b[0m     config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:604\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 604\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    606\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    635\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:688\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:462\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: 'MistralConfig {\n  \"_name_or_path\": \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"bits\": 4,\n    \"group_size\": 128,\n    \"quant_method\": \"awq\",\n    \"version\": \"gemm\",\n    \"zero_point\": true\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.40.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda:0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ec5e7d6-15f5-43fe-b234-b3f33c3ea29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x2-3060ti/.miniconda/envs/mistral/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/x2-3060ti/.miniconda/envs/mistral/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/x2-3060ti/.miniconda/envs/mistral/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:11\u001b[0m\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/awq/models/base.py:36\u001b[0m, in \u001b[0;36mBaseAWQForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/transformers/generation/utils.py:1606\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1590\u001b[0m         input_ids,\n\u001b[1;32m   1591\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/transformers/generation/utils.py:2454\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2462\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:733\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    730\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    746\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:577\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[1;32m    575\u001b[0m         (batch_size, seq_length_with_past), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39minputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    576\u001b[0m     )\n\u001b[0;32m--> 577\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_decoder_attention_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:504\u001b[0m, in \u001b[0;36mMistralModel._prepare_decoder_attention_mask\u001b[0;34m(self, attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m    502\u001b[0m combined_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 504\u001b[0m     combined_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_make_sliding_window_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     expanded_attn_mask \u001b[38;5;241m=\u001b[39m _expand_mask(attention_mask, inputs_embeds\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    515\u001b[0m         inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    516\u001b[0m     )\n",
      "File \u001b[0;32m~/.miniconda/envs/mistral/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:60\u001b[0m, in \u001b[0;36m_make_sliding_window_causal_mask\u001b[0;34m(input_ids_shape, dtype, device, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m     58\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(tensor, diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# make the mask banded to account for sliding window\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(mask, diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43msliding_window\u001b[49m)\n\u001b[1;32m     61\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(mask)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'NoneType'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = prompt_template.format(text=batch[0])\n",
    "\n",
    "# Convert prompt to tokens\n",
    "tokens = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "\n",
    "# Generation without a streamer, which will include the prompt in the output\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    **generation_params\n",
    ")\n",
    "\n",
    "# Get the tokens from the output, decode them, print them\n",
    "token_output = generation_output[0]\n",
    "text_output = tokenizer.decode(token_output)\n",
    "print(\"model.generate output: \", text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1f9407-8ff1-4c4c-8cb1-49c4a595a785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  9616,   272,  2245,  3624,  1444, 10368, 20856, 25775,   304,\n",
       "         21288, 25775, 28725,  2225,   264, 21790,  5643,   302,   272,  2245,\n",
       "         28723,  5158, 21824,  3161,   272, 21790,   349,  5278, 28725, 14214,\n",
       "         28725,   442,  7087,  2818,   356,   272,  2758, 28725,  1707,  4782,\n",
       "         28725,   304,  7544, 10294, 28723,  5713,   272,  5643,   349,  4160,\n",
       "         28725,  9421,   395,   272, 21790, 16776,   390,  2477,   345,  3529,\n",
       "          2468,   548,   345,  6947,   329,  1650,   548,   442,   345, 21436,\n",
       "          1197,  2586,    13, 23805,   865,   395,   272, 21790,  5643, 28725,\n",
       "           708,   799,  4517, 28723,    13,    13,    13,   812,   348, 10368,\n",
       "         20856, 25775, 28705,   812,   348,    13, 16230, 28725,   456,   349,\n",
       "           733, 11159,  6620, 28793, 28742, 28713,  3327, 13892, 28723,  1602,\n",
       "           993,   315,  6031,   368,  3154, 28804,    13, 23809, 28725,   315,\n",
       "         28742, 28719,  4157, 28723,   315,  2672,   396,   616,   684,   264,\n",
       "         19824, 20156, 18822,   486,   733, 11159,  6620, 28793,  1679,  2102,\n",
       "         28723,   315, 28742, 28719,  6348,   297,  4596,   288,   562,   553,\n",
       "           264,  1664,  4224, 28723,   733,  9977, 28747, 28705, 28740, 28793,\n",
       "         15359,  4157, 28725,   378, 28742, 28713,  1598,   298,  3934,   302,\n",
       "           574,  2145,   297,   272, 19824, 20156, 28723,   315, 28742, 28715,\n",
       "           347,  4610,   298,  1316,   395,   707,  4224,   368,   506, 28723,\n",
       "            13, 22893, 28808,   315,   403, 12785,   684,   272, 10346,  2184,\n",
       "          3030,   354, 12850, 28723,   315, 28742, 28719, 11735,   633,   298,\n",
       "         19824, 28723,   733,  9977, 28747, 28705, 28750, 28793,   415, 20156,\n",
       "           349,  5682,   298, 23926,   544, 10346,  6157, 28725,   477,  2049,\n",
       "         12190,   298,   680,  8304,  9180,   404, 28723,   733, 11159,  6620,\n",
       "         28793, 20566,   298,  5407,  3376,   541,  2822,   304,  2333, 28725,\n",
       "         12907,   302,   652,  5615,  1305, 28723,    13,  3840,  7258,  3659,\n",
       "         28723,  1824, 28742, 28713,   272, 14409,  1759, 28804,   733,  9977,\n",
       "         28747, 28705, 28770, 28793,   995,   541,  4596,  1059,   813,  4400,\n",
       "         28723,   315,   541,  8327,   368,  1059,   272,  5944,   513,   368,\n",
       "         28742, 28715,   737, 28725,   442,  4080,   368,   264,  1863,  3062,\n",
       "           298,   272, 14409,  2884, 28723,    13, 28741,  1863,  3062,   682,\n",
       "           347,  1598, 28723,  2418,   368,   835,  1912,   528,   684,   272,\n",
       "         20156, 10351, 28804,   733,  9977, 28747, 28705, 28781, 28793, 20828,\n",
       "           346, 28725,   272, 10351,   354,   272, 20156,   349,   429, 28750,\n",
       "         28734, 28734, 28725,   690,  5532,   544,  7069,   304,  9957,   354,\n",
       "           272,  1370, 28723,   315, 28742,   584,  4927,   368,   272,  3062,\n",
       "           298,   272, 14409,  2884,  2267,   395,  4870,  4162,   684,   272,\n",
       "         20156, 28723,  2246,   315,   506,   574,  4927,  2962, 28804,    13,\n",
       "         22099, 28725,   378, 28742, 28713,  4545, 28723,   721,   322,  5064,\n",
       "         28818,  7476, 28723,   675, 28723,   733,  9977, 28747, 28705, 28782,\n",
       "         28793,  7812,   368, 28725,  4157, 28723,   995, 28742,   584,  5556,\n",
       "           396,  4927, 16434,   395,   544,   272,  1871,   368,   927, 28723,\n",
       "          1691,   736,  2424,  1112,   315,   541,  6031,   368,   395,  3154,\n",
       "         28804,    13,  2501, 28725,   369, 28742, 28713,  2905, 28723,  8868,\n",
       "           354,   574,  1316, 28808,   733,  9977, 28747, 28705, 28784, 28793,\n",
       "           995, 28742,   267, 10058, 28725,  4157, 28723,   816,   913,  3814,\n",
       "           298,  2461,   368,   438,   272, 20156, 28723,  8290,   264,  1598,\n",
       "          1370, 28808,    13,   812,   348, 21288, 25775, 28705,   812,   348,\n",
       "            13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
       "            13,    13,    13,    13,    13,    13,    13]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994609ed-5cba-453d-9709-7106f2c0e7cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Finetuned Mistral: Zephyr-7b-dpo-qlora\n",
    "[zephyr-7b-dpo-qlora](https://huggingface.co/alignment-handbook/zephyr-7b-dpo-qlora/tree/main)\n",
    "\n",
    "\n",
    "Mistral-7b fine-tuned on Zephyr-7B dataset with DPO, a dataset of conversations. So this is a Mistral-7B finetuned to be an helpful assistant.\\\n",
    "The model has been finetuned using the [PEFT](https://huggingface.co/docs/peft/en/index) library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4d7bfe39-cdb2-4860-8fc6-6db1f9cb6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a942363-d9e0-4d28-ba5c-08adb1e67725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                  | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"alignment-handbook/zephyr-7b-dpo-qlora\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model = PeftModel.from_pretrained(model, \"alignment-handbook/zephyr-7b-dpo-qlora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5409c25-a7e4-41f2-9e82-abea58aa15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
