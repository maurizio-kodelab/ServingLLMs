{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a97ac5",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_torchtrt_hf_bert/nvidia_logo.png\" width=\"90px\">\n",
    "\n",
    "\n",
    "# Torch-TensorRT-optimized BERT for Sentence Classificatio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fa151",
   "metadata": {},
   "source": [
    "\n",
    "####  Requirements\n",
    "\n",
    "NVIDIA's NGC provides a PyTorch Docker Container which contains PyTorch and Torch-TensorRT. Starting with version `22.05-py3`, we can make use of [latest pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) container to run this notebook.\n",
    "\n",
    "\n",
    "`sudo docker run --gpus all -it -p 8001:8888 --rm nvcr.io/nvidia/pytorch:24.03-py3`\n",
    "\n",
    "\n",
    "Otherwise, you can follow the steps in `notebooks/README` to prepare a Docker container yourself, within which you can run this demo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e687d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1104c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import timeit\n",
    "import numpy as np\n",
    "#import torch_tensorrt\n",
    "#import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e0f70d-0474-42b8-91f6-8e337211702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santamm/.miniconda/envs/lorax/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for carblacac/twitter-sentiment-analysis contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/carblacac/twitter-sentiment-analysis\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df8dc21f5254fc0ad5715ec5087ca8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8d4543198649afb6058ddfbcf3b3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b7fec564ef47c2983383ecaa647862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be087b8d912649f18b72d43de496bc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5e34748b9141f288ff6dbc4b4ab850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b9d8e0e55e4d16802f51ea816e46af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f05a78d8d048bf87f4fb0e537dbef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fad04731614849942fc94f2c165e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/149985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589c102dc3a64bedbb1d1769b387dfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638ff42ebc5c4fe7881c572f0db110a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28ac08c54c241229e53f2528f615400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4bb774aca54df38b512ae95f173caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef2aec9ff404db6a315ba2826d2754a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/119988 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64a9b830e544b7dbc0345e267e946be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/29997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2565ce15b94b4e719b007c7edeb6e0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/61998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"carblacac/twitter-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ed7f95-14ab-4257-903e-6490a3fca1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['text', 'feeling'],\n",
       " 'validation': ['text', 'feeling'],\n",
       " 'test': ['text', 'feeling']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba72446-f44e-468e-b97f-42179ecea6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"text\", \"feeling\"])\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d41d50d-b827-4511-b9ee-a9de48660438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"@kathystover Didn't go much of any where - Life took over for a while\",\n",
       " 'feeling': tensor(1)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dd87fca-eb6e-4a31-8ad8-3b3387c1541a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119988, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "536c9a25-e017-4e94-9bdf-1ceda04e2327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fa6ami86 so happy that salman won.  btw the 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@phantompoptart .......oops.... I guess I'm ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@bradleyjp decidedly undecided. Depends on the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Mountgrace lol i know! its so frustrating isn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@kathystover Didn't go much of any where - Lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119983</th>\n",
       "      <td>I so should be in bed but I can't sleep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119984</th>\n",
       "      <td>@mickeymab mine's in my profile - '77cb550 and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119985</th>\n",
       "      <td>@stacyreeves Awe... I wish I could.  I am here...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119986</th>\n",
       "      <td>Is it me or is Vodafone UK business support ru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119987</th>\n",
       "      <td>http://twitpic.com/7jmsh - My man hand with ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119988 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  feeling\n",
       "0       @fa6ami86 so happy that salman won.  btw the 1...        0\n",
       "1       @phantompoptart .......oops.... I guess I'm ki...        0\n",
       "2       @bradleyjp decidedly undecided. Depends on the...        1\n",
       "3       @Mountgrace lol i know! its so frustrating isn...        1\n",
       "4       @kathystover Didn't go much of any where - Lif...        1\n",
       "...                                                   ...      ...\n",
       "119983            I so should be in bed but I can't sleep        0\n",
       "119984  @mickeymab mine's in my profile - '77cb550 and...        1\n",
       "119985  @stacyreeves Awe... I wish I could.  I am here...        0\n",
       "119986  Is it me or is Vodafone UK business support ru...        0\n",
       "119987  http://twitpic.com/7jmsh - My man hand with ca...        0\n",
       "\n",
       "[119988 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240a6a1-d956-4063-819b-88de67f6879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778b28e-ddd0-4c2c-a551-248dcea2f86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19e711c0",
   "metadata": {},
   "source": [
    "## BERT for Sentence Classification\n",
    "\n",
    "```\n",
    "Example output:\n",
    "[[\n",
    "{'label': 'sadness', 'score': 0.0005138228880241513}, \n",
    "{'label': 'joy', 'score': 0.9972520470619202}, \n",
    "{'label': 'love', 'score': 0.0007443308713845909}, \n",
    "{'label': 'anger', 'score': 0.0007404946954920888}, \n",
    "{'label': 'fear', 'score': 0.00032938539516180754}, \n",
    "{'label': 'surprise', 'score': 0.0004197491507511586}\n",
    "]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "12f92507-a9f8-4ab2-81ac-fedd29919aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ('sadness', 'joy', 'love', 'anger', 'fear', 'surprise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56ed40dd-04d9-473b-8549-d3b9ec2769eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\", torchscript=True)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc59f996-80a9-4e09-90af-942348e82029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory footprint: 0.44G\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model memory footprint: {model.get_memory_footprint()/1e9:.2f}G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe05ee-c20b-47a1-8ba4-a4e4d9f080aa",
   "metadata": {},
   "source": [
    "### Model Tracing\n",
    "\n",
    "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation.\\\n",
    "Tracing is ideal for code that operates only on Tensor\\s and lists, dictionaries, and tuples of Tensor\\s.\n",
    "\n",
    "Using torch.jit.trace and torch.jit.trace_module, you can turn an existing module or Python function into a TorchScript ScriptFunction or ScriptModule. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.\n",
    "\n",
    "The resulting recording of a standalone function produces ScriptFunction.\\\n",
    "The resulting recording of nn.Module.forward or nn.Module produces ScriptModule.\\\n",
    "This module also contains any parameters that the original module had as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e23aa2f-b442-4c1b-b0e3-e41ce483cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token = chr(50256)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb3c4080-bd69-44a7-a39f-140d4bbf2d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@MirandaBuzz Haha! Violin Hero. Genius parody!!! Rock the violin out! Anyway, awesome show especially the cast and Dan. Good day!!!',\n",
       " '...but my hair smells like wood smoke',\n",
       " \"Hmmm...I guess TwitterFon doesn't like emoji very much. It just comes out as numbers and puctuations. Darn it!\",\n",
       " \"@NeenDhie Hey hun aww bless u. hope u have a great day (: Oh i was watchin the eurovision last night n Denmark's song got so low points\",\n",
       " 'i sooooo wish i could be at the Lakers parade tomorrow!  will someone please give me a ride? http://twurl.nl/3upnus',\n",
       " \"@boxofcrayons Let's say the 3rd type of person is he who wants to count, but needs some guidance! That's where we come in. Happy Monday!\",\n",
       " 'a long day for me as well tomorrow but a happy one  goodnight.',\n",
       " 'Want much to go to the Library Mall to show support, but am far too sick.  With you in spirit, though!! Rally starts @ 3pm...',\n",
       " '@MariahCarey hi! is it really you?',\n",
       " 'On my way to work, are we havin a summer this year? Is june and is cold']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = df.sample(10).text.values.tolist()\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28d61a09-9996-4ddc-ab45-e0d72977c690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inputs = tokenizer(batch, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "example_inputs['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9c846810-275a-46a8-aa86-b335413352d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5973ab23-b00e-4d3d-bd3d-1ff6c8b6699a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 512]), torch.Size([10, 512]), torch.Size([10, 512]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "tokens_tensor = example_inputs['input_ids']\n",
    "token_type_tensor = example_inputs['token_type_ids']\n",
    "attention_masks_tensor = example_inputs['attention_mask']\n",
    "\n",
    "tokens_tensor.size(), token_type_tensor.size(), attention_masks_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30b1762c-279c-481d-bb9f-814aae66ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, [tokens_tensor, token_type_tensor, attention_masks_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a8980446-ab5c-40a9-9845-a027d65218f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model.save('models/bert-base-uncased-emotion_traced.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "81d7bf25-c462-4849-b1c9-7ed2443d9c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.jit._trace.TopLevelTracedModule"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(traced_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca05eb-da70-48a8-83f0-069aa6ef10e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6e320d65-4b80-4641-a551-bf210e922624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MirandaBuzz Haha! Violin Hero. Genius parody!!! Rock the violin out! Anyway, awesome show especially the cast and Dan. Good day!!!\n",
      "sadness:0.00\tjoy:1.00\tlove:0.00\tanger:0.00\tfear:0.00\tsurprise:0.00\t\n",
      "...but my hair smells like wood smoke\n",
      "sadness:0.05\tjoy:0.01\tlove:0.00\tanger:0.45\tfear:0.49\tsurprise:0.00\t\n",
      "Hmmm...I guess TwitterFon doesn't like emoji very much. It just comes out as numbers and puctuations. Darn it!\n",
      "sadness:0.01\tjoy:0.22\tlove:0.04\tanger:0.72\tfear:0.01\tsurprise:0.01\t\n",
      "@NeenDhie Hey hun aww bless u. hope u have a great day (: Oh i was watchin the eurovision last night n Denmark's song got so low points\n",
      "sadness:0.04\tjoy:0.94\tlove:0.01\tanger:0.00\tfear:0.00\tsurprise:0.00\t\n",
      "i sooooo wish i could be at the Lakers parade tomorrow!  will someone please give me a ride? http://twurl.nl/3upnus\n",
      "sadness:0.04\tjoy:0.80\tlove:0.08\tanger:0.02\tfear:0.05\tsurprise:0.01\t\n",
      "@boxofcrayons Let's say the 3rd type of person is he who wants to count, but needs some guidance! That's where we come in. Happy Monday!\n",
      "sadness:0.00\tjoy:1.00\tlove:0.00\tanger:0.00\tfear:0.00\tsurprise:0.00\t\n",
      "a long day for me as well tomorrow but a happy one  goodnight.\n",
      "sadness:0.00\tjoy:1.00\tlove:0.00\tanger:0.00\tfear:0.00\tsurprise:0.00\t\n",
      "Want much to go to the Library Mall to show support, but am far too sick.  With you in spirit, though!! Rally starts @ 3pm...\n",
      "sadness:0.28\tjoy:0.68\tlove:0.01\tanger:0.02\tfear:0.01\tsurprise:0.00\t\n",
      "@MariahCarey hi! is it really you?\n",
      "sadness:0.06\tjoy:0.76\tlove:0.01\tanger:0.05\tfear:0.10\tsurprise:0.01\t\n",
      "On my way to work, are we havin a summer this year? Is june and is cold\n",
      "sadness:0.00\tjoy:0.00\tlove:0.00\tanger:0.99\tfear:0.00\tsurprise:0.00\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as nnf\n",
    "\n",
    "encoded_inputs = tokenizer(batch, return_tensors='pt', padding='max_length', max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    probs = nnf.softmax(outputs[0], dim=1)\n",
    "    for i, sentence in enumerate(batch):\n",
    "        print(f\"{sentence}\")\n",
    "        for j, prob in enumerate(probs[i].tolist()):\n",
    "            print(f\"{labels[j]}:{prob:.2f}\", end = '\\t')\n",
    "        print()\n",
    "    print()\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f10b5ac1-750d-4642-bc3a-f93c9dc6447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MirandaBuzz Haha! Violin Hero. Genius parody!!! Rock the violin out! Anyway, awesome show especially the cast and Dan. Good day!!!\n",
      "sadness:0.00\tjoy:1.00\tlove:0.00\tanger:0.00\tfear:0.00\tsurprise:0.00\t\n",
      "...but my hair smells like wood smoke\n",
      "sadness:0.05\tjoy:0.01\tlove:0.00\tanger:0.45\tfear:0.49\tsurprise:0.00\t\n",
      "Hmmm...I guess TwitterFon doesn't like emoji very much. It just comes out as numbers and puctuations. Darn it!\n",
      "sadness:0.01\tjoy:0.22\tlove:0.04\tanger:0.72\tfear:0.01\tsurprise:0.01\t\n",
      "@NeenDhie Hey hun aww bless u. hope u have a great day (: Oh i was watchin the eurovision last night n Denmark's song got so low points\n",
      "sadness:0.04\tjoy:0.94\tlove:0.01\tanger:0.00\tfear:0.00\tsurprise:0.00\t\n",
      "i sooooo wish i could be at the Lakers parade tomorrow!  will someone please give me a ride? http://twurl.nl/3upnus\n",
      "sadness:0.04\tjoy:0.80\tlove:0.08\tanger:0.02\tfear:0.05\tsurprise:0.01\t\n",
      "@boxofcrayons Let's say the 3rd type of person is he who wants to count, but needs some guidance! That's where we come in. Happy Monday!\n",
      "sadness:0.00\tjoy:1.00\tlove:0.00\tanger:0.00\tfear:0.00\tsurprise:0.00\t\n",
      "a long day for me as well tomorrow but a happy one  goodnight.\n",
      "sadness:0.00\tjoy:1.00\tlove:0.00\tanger:0.00\tfear:0.00\tsurprise:0.00\t\n",
      "Want much to go to the Library Mall to show support, but am far too sick.  With you in spirit, though!! Rally starts @ 3pm...\n",
      "sadness:0.28\tjoy:0.68\tlove:0.01\tanger:0.02\tfear:0.01\tsurprise:0.00\t\n",
      "@MariahCarey hi! is it really you?\n",
      "sadness:0.06\tjoy:0.76\tlove:0.01\tanger:0.05\tfear:0.10\tsurprise:0.01\t\n",
      "On my way to work, are we havin a summer this year? Is june and is cold\n",
      "sadness:0.00\tjoy:0.00\tlove:0.00\tanger:0.99\tfear:0.00\tsurprise:0.00\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traced model\n",
    "with torch.no_grad():\n",
    "    outputs = traced_model(**encoded_inputs)\n",
    "    probs = nnf.softmax(outputs[0], dim=1)\n",
    "    for i, sentence in enumerate(batch):\n",
    "        print(f\"{sentence}\")\n",
    "        for j, prob in enumerate(probs[i].tolist()):\n",
    "            print(f\"{labels[j]}:{prob:.2f}\", end = '\\t')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65098903-d6f3-40e0-ab80-766a26da2624",
   "metadata": {},
   "source": [
    "### Compiling with Torch-TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e2832-f8be-4320-b1c6-39075c9f53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_level = torch_tensorrt.logging.Level.Error\n",
    "torch_tensorrt.logging.set_reportable_log_level(new_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a686e6d-1245-4c33-833f-568ec5dae291",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f3f36-7ba4-44ec-98ba-1676ad113eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model = torch_tensorrt.compile(traced_model, \n",
    "    inputs= [torch_tensorrt.Input(shape=[batch_size, 512], dtype=torch.int32, device='cuda'),  # input_ids\n",
    "             torch_tensorrt.Input(shape=[batch_size, 512], dtype=torch.int32, device='cuda'),  # token_type_ids\n",
    "             torch_tensorrt.Input(shape=[batch_size, 512], dtype=torch.int32, device='cuda')], # attention_mask\n",
    "    enabled_precisions= {torch.float32}, # Run with 32-bit precision\n",
    "    workspace_size=2000000000,\n",
    "    truncate_long_and_double=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b4a5f-80c0-4be7-b1e4-909eda50add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "enc_inputs = tokenizer(batch, return_tensors='pt', padding='max_length', max_length=512)\n",
    "enc_inputs = {k: v.type(torch.int32).cuda() for k, v in enc_inputs.items()}\n",
    "output_trt = trt_model(enc_inputs['input_ids'], enc_inputs['token_type_ids'], enc_inputs['attention_mask'])\n",
    "#print(output_trt[0])\n",
    "\n",
    "most_likely_token_ids_trt = [torch.argmax(output_trt[0][i, pos, :]) for i, pos in enumerate(pos_masks)] \n",
    "unmasked_tokens_trt = enc.decode(most_likely_token_ids_trt).split(' ')\n",
    "unmasked_sentences_trt = [masked_sentences[i].replace('[MASK]', token) for i, token in enumerate(unmasked_tokens_trt)]\n",
    "for sentence in unmasked_sentences_trt:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb8e47-0d79-4b4b-8f12-e741e485bd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fb76d-a831-4b0f-a6a8-0d518368aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile again with 16 bit precision\n",
    "\n",
    "trt_model_fp16 = torch_tensorrt.compile(traced_model, \n",
    "    inputs= [torch_tensorrt.Input(shape=[batch_size, 512], dtype=torch.int32),  # input_ids\n",
    "             torch_tensorrt.Input(shape=[batch_size, 512], dtype=torch.int32),  # token_type_ids\n",
    "             torch_tensorrt.Input(shape=[batch_size, 512], dtype=torch.int32)], # attention_mask\n",
    "    enabled_precisions= {torch.half}, # Run with 16-bit precision\n",
    "    workspace_size=2000000000,\n",
    "    truncate_long_and_double=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cdc402-5eb6-4722-bfbe-b77a6c069d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678fd00-4cb5-459d-9fa1-f7b3c15920bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3790c1bb-55d7-4d84-9d5d-ed58a2f6a89d",
   "metadata": {},
   "source": [
    "## Bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4c6f6",
   "metadata": {},
   "source": [
    "First, create a pretrained BERT tokenizer from the `bert-base-uncased` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7c8721e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908a9ec2bb854ce4a80ab0e0a8287ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39ac92a51ec4fb2913611f8a06fdd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95dc6e855264dfe90d0709307ec4f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7efefb4ee34084b61f821f0f87e989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "enc = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "mlm_model_ts = BertForMaskedLM.from_pretrained('bert-base-uncased', torchscript=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1c679",
   "metadata": {},
   "source": [
    "Create dummy inputs to generate a traced TorchScript model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0842bc1-4835-4cfc-8b08-996dfd0c7620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_model_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bfe8f658-6386-4669-9497-75d5012f2358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory footprint: 0.53G\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model memory footprint: {mlm_model_ts.get_memory_footprint()/1e9:.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3827087",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "batched_indexed_tokens = [[101, 64]*64]*batch_size\n",
    "batched_segment_ids = [[0, 1]*64]*batch_size\n",
    "batched_attention_masks = [[1, 1]*64]*batch_size\n",
    "\n",
    "tokens_tensor = torch.tensor(batched_indexed_tokens)\n",
    "segments_tensor = torch.tensor(batched_segment_ids)\n",
    "attention_masks_tensor = torch.tensor(batched_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55ec3110-4afd-45b5-b47a-bc6743f06dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fee30971-631c-448d-9b8f-26cd2fbd7a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31b27f",
   "metadata": {},
   "source": [
    "Obtain a BERT masked language model from Hugging Face in the (scripted) TorchScript, then use the dummy inputs to trace it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351db861-cd02-4d18-b95d-749972afa664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "19ec3118-cb43-47c8-a1cc-6743a4ae6d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c32835765e4e549a0362943d544e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7257dbf8d954e688eec88ec9b598e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca261ec442d740ed92a048013645dd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93124e4d826e4c84825b091d8d550e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75736212384b4d8786b4b0c44363dfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e97b7dfc204d44a3cab0e86508fbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\")\n",
    "model_ts = AutoModelForSequenceClassification.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\", torchscript=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5967051-8e7a-4eeb-a9c8-76bb8d6c65b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory footprint: 0.44G\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model memory footprint: {model_ts.get_memory_footprint()/1e9:.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78a68f-cdeb-4bc7-abb6-81ef69817c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505c261-8e12-4447-be31-5a00f6dc711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model_ts = torch.jit.trace(model_ts, [tokens_tensor, segments_tensor, attention_masks_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3cd5a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "mlm_model_ts = BertForMaskedLM.from_pretrained('bert-base-uncased', torchscript=True)\n",
    "traced_mlm_model = torch.jit.trace(mlm_model_ts, [tokens_tensor, segments_tensor, attention_masks_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2217a",
   "metadata": {},
   "source": [
    "Define 4 masked sentences, with 1 word in each sentence hidden from the model. Fluent English speakers will probably be able to guess the masked words, but just in case, they are `'capital'`, `'language'`, `'innings'`, and `'mathematics'`.\n",
    "\n",
    "Also create a list containing the position of the masked word within each sentence. Given Python's 0-based indexing convention, the numbers are each higher by 1 than might be expected. This is because the token at index 0 in each sentence is a beginning-of-sentence token, denoted `[CLS]` when entered explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d1af982",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sentences = ['Paris is the [MASK] of France.', \n",
    "                    'The primary [MASK] of the United States is English.', \n",
    "                    'A baseball game consists of at least nine [MASK].', \n",
    "                    'Topology is a branch of [MASK] concerned with the properties of geometric objects that remain unchanged under continuous transformations.']\n",
    "pos_masks = [4, 3, 9, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89b4c8",
   "metadata": {},
   "source": [
    "Pass the masked sentences into the (scripted) TorchScript MLM model and verify that the unmasked sentences yield the expected results.  \n",
    "\n",
    "Because the sentences are of different lengths, we must specify the `padding` argument in calling our encoder/tokenizer. There are several possible padding strategies, but we'll use `'max_length'` padding with `max_length=128`. Later, when we compile an optimized version of the model with Torch-TensorRT, the optimized model will expect inputs of length 128, hence our choice of padding strategy and length here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d7546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the capital of France.\n",
      "The primary language of the United States is English.\n",
      "A baseball game consists of at least nine innings.\n",
      "Topology is a branch of mathematics concerned with the properties of geometric objects that remain unchanged under continuous transformations.\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = enc(masked_sentences, return_tensors='pt', padding='max_length', max_length=128)\n",
    "outputs = mlm_model_ts(**encoded_inputs)\n",
    "most_likely_token_ids = [torch.argmax(outputs[0][i, pos, :]) for i, pos in enumerate(pos_masks)]\n",
    "unmasked_tokens = enc.decode(most_likely_token_ids).split(' ')\n",
    "unmasked_sentences = [masked_sentences[i].replace('[MASK]', token) for i, token in enumerate(unmasked_tokens)]\n",
    "for sentence in unmasked_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b423ff",
   "metadata": {},
   "source": [
    "Pass the masked sentences into the traced MLM model and verify that the unmasked sentences yield the expected results. \n",
    "\n",
    "Note the difference in how the `encoded_inputs` are passed into the model in the following cell compared to the previous one. If you examine `encoded_inputs`, you'll find that it's a dictionary with 3 keys, `'input_ids'`, `'token_type_ids'`, and `'attention_mask'`, each with a PyTorch tensor as an associated value. The traced model will accept `**encoded_inputs` as an input, but the Torch-TensorRT-optimized model (to be defined later) will not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683a4a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the all of France.\n",
      "The primary all of the United States is English.\n",
      "A baseball game consists of at least nine each.\n",
      "Topology is a branch of each concerned with the properties of geometric objects that remain unchanged under continuous transformations.\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = enc(masked_sentences, return_tensors='pt', padding='max_length', max_length=128)\n",
    "outputs = traced_mlm_model(encoded_inputs['input_ids'], encoded_inputs['token_type_ids'], encoded_inputs['attention_mask'])\n",
    "most_likely_token_ids = [torch.argmax(outputs[0][i, pos, :]) for i, pos in enumerate(pos_masks)]\n",
    "unmasked_tokens = enc.decode(most_likely_token_ids).split(' ')\n",
    "unmasked_sentences = [masked_sentences[i].replace('[MASK]', token) for i, token in enumerate(unmasked_tokens)]\n",
    "for sentence in unmasked_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31b545",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Compiling with Torch-TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d8b4f",
   "metadata": {},
   "source": [
    "Change the logging level to avoid long printouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42862893",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_level = torch_tensorrt.logging.Level.Error\n",
    "torch_tensorrt.logging.set_reportable_log_level(new_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d6d59",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f4b1d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  original_name=BertForMaskedLM\n",
       "  (bert): BertModel(\n",
       "    original_name=BertModel\n",
       "    (embeddings): BertEmbeddings(\n",
       "      original_name=BertEmbeddings\n",
       "      (word_embeddings): Embedding(original_name=Embedding)\n",
       "      (position_embeddings): Embedding(original_name=Embedding)\n",
       "      (token_type_embeddings): Embedding(original_name=Embedding)\n",
       "      (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "      (dropout): Dropout(original_name=Dropout)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      original_name=BertEncoder\n",
       "      (layer): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          original_name=BertLayer\n",
       "          (attention): BertAttention(\n",
       "            original_name=BertAttention\n",
       "            (self): BertSelfAttention(\n",
       "              original_name=BertSelfAttention\n",
       "              (query): Linear(original_name=Linear)\n",
       "              (key): Linear(original_name=Linear)\n",
       "              (value): Linear(original_name=Linear)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              original_name=BertSelfOutput\n",
       "              (dense): Linear(original_name=Linear)\n",
       "              (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "              (dropout): Dropout(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            original_name=BertIntermediate\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            original_name=BertOutput\n",
       "            (dense): Linear(original_name=Linear)\n",
       "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "            (dropout): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    original_name=BertOnlyMLMHead\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      original_name=BertLMPredictionHead\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        original_name=BertPredictionHeadTransform\n",
       "        (dense): Linear(original_name=Linear)\n",
       "        (transform_act_fn): GELUActivation(original_name=GELUActivation)\n",
       "        (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
       "      )\n",
       "      (decoder): Linear(original_name=Linear)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_mlm_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eab90150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_tensorrt._compile:Input graph is a Torchscript module but the ir provided is default (dynamo). Please set ir=torchscript to suppress the warning. Compiling the module with ir=torchscript\n"
     ]
    }
   ],
   "source": [
    "trt_model = torch_tensorrt.compile(traced_mlm_model, \n",
    "    inputs= [torch_tensorrt.Input(shape=[batch_size, 128], dtype=torch.int32, device='cuda'),  # input_ids\n",
    "             torch_tensorrt.Input(shape=[batch_size, 128], dtype=torch.int32, device='cuda'),  # token_type_ids\n",
    "             torch_tensorrt.Input(shape=[batch_size, 128], dtype=torch.int32, device='cuda')], # attention_mask\n",
    "    enabled_precisions= {torch.float32}, # Run with 32-bit precision\n",
    "    workspace_size=2000000000,\n",
    "    truncate_long_and_double=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96751ce",
   "metadata": {},
   "source": [
    "Pass the masked sentences into the compiled model and verify that the unmasked sentences yield the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "097ea381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the all of France.\n",
      "The primary all of the United States is English.\n",
      "A baseball game consists of at least nine each.\n",
      "Topology is a branch of each concerned with the properties of geometric objects that remain unchanged under continuous transformations.\n"
     ]
    }
   ],
   "source": [
    "enc_inputs = enc(masked_sentences, return_tensors='pt', padding='max_length', max_length=128)\n",
    "enc_inputs = {k: v.type(torch.int32).cuda() for k, v in enc_inputs.items()}\n",
    "output_trt = trt_model(enc_inputs['input_ids'], enc_inputs['token_type_ids'], enc_inputs['attention_mask'])\n",
    "#print(output_trt[0])\n",
    "\n",
    "most_likely_token_ids_trt = [torch.argmax(output_trt[0][i, pos, :]) for i, pos in enumerate(pos_masks)] \n",
    "unmasked_tokens_trt = enc.decode(most_likely_token_ids_trt).split(' ')\n",
    "unmasked_sentences_trt = [masked_sentences[i].replace('[MASK]', token) for i, token in enumerate(unmasked_tokens_trt)]\n",
    "for sentence in unmasked_sentences_trt:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398271d",
   "metadata": {},
   "source": [
    "Compile the model again, this time with 16-bit precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a063dee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_tensorrt._compile:Input graph is a Torchscript module but the ir provided is default (dynamo). Please set ir=torchscript to suppress the warning. Compiling the module with ir=torchscript\n"
     ]
    }
   ],
   "source": [
    "trt_model_fp16 = torch_tensorrt.compile(traced_mlm_model, \n",
    "    inputs= [torch_tensorrt.Input(shape=[batch_size, 128], dtype=torch.int32),  # input_ids\n",
    "             torch_tensorrt.Input(shape=[batch_size, 128], dtype=torch.int32),  # token_type_ids\n",
    "             torch_tensorrt.Input(shape=[batch_size, 128], dtype=torch.int32)], # attention_mask\n",
    "    enabled_precisions= {torch.half}, # Run with 16-bit precision\n",
    "    workspace_size=2000000000,\n",
    "    truncate_long_and_double=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926334a",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Benchmarking\n",
    "\n",
    "In developing this notebook, we conducted our benchmarking on a single NVIDIA A100 GPU. Your results may differ from those shown, particularly on a different GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c6fb9",
   "metadata": {},
   "source": [
    "This function passes the inputs into the model and runs inference `num_loops` times, then returns a list of length containing the amount of time in seconds that each instance of inference took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b72a091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeGraph(model, input_tensor1, input_tensor2, input_tensor3, num_loops=50):\n",
    "    print(\"Warm up ...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(20):\n",
    "            features = model(input_tensor1, input_tensor2, input_tensor3)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_loops):\n",
    "            start_time = timeit.default_timer()\n",
    "            features = model(input_tensor1, input_tensor2, input_tensor3)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = timeit.default_timer()\n",
    "            timings.append(end_time - start_time)\n",
    "            # print(\"Iteration {}: {:.6f} s\".format(i, end_time - start_time))\n",
    "\n",
    "    return timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44dcf8",
   "metadata": {},
   "source": [
    "This function prints the number of input batches the model is able to process each second and summary statistics of the model's latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ef71ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStats(graphName, timings, batch_size):\n",
    "    times = np.array(timings)\n",
    "    steps = len(times)\n",
    "    speeds = batch_size / times\n",
    "    time_mean = np.mean(times)\n",
    "    time_med = np.median(times)\n",
    "    time_99th = np.percentile(times, 99)\n",
    "    time_std = np.std(times, ddof=0)\n",
    "    speed_mean = np.mean(speeds)\n",
    "    speed_med = np.median(speeds)\n",
    "\n",
    "    msg = (\"\\n%s =================================\\n\"\n",
    "            \"batch size=%d, num iterations=%d\\n\"\n",
    "            \"  Median text batches/second: %.1f, mean: %.1f\\n\"\n",
    "            \"  Median latency: %.6f, mean: %.6f, 99th_p: %.6f, std_dev: %.6f\\n\"\n",
    "            ) % (graphName,\n",
    "                batch_size, steps,\n",
    "                speed_med, speed_mean,\n",
    "                time_med, time_mean, time_99th, time_std)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afe97b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba98b24",
   "metadata": {},
   "source": [
    "Benchmark the (scripted) TorchScript model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bab5fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT =================================\n",
      "batch size=4, num iterations=50\n",
      "  Median text batches/second: 448.6, mean: 446.3\n",
      "  Median latency: 0.008916, mean: 0.008965, 99th_p: 0.009542, std_dev: 0.000154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(model.cuda(), enc_inputs['input_ids'], enc_inputs['token_type_ids'], enc_inputs['attention_mask'])\n",
    "\n",
    "printStats(\"BERT\", timings, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79c452",
   "metadata": {},
   "source": [
    "Benchmark the traced model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c0bd8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT =================================\n",
      "batch size=4, num iterations=50\n",
      "  Median text batches/second: 621.5, mean: 610.9\n",
      "  Median latency: 0.006436, mean: 0.006558, 99th_p: 0.007502, std_dev: 0.000283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(traced_model.cuda(), enc_inputs['input_ids'], enc_inputs['token_type_ids'], enc_inputs['attention_mask'])\n",
    "\n",
    "printStats(\"BERT\", timings, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db22a1",
   "metadata": {},
   "source": [
    "Benchmark the compiled FP32 model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ade7b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT =================================\n",
      "batch size=4, num iterations=50\n",
      "  Median text batches/second: 754.3, mean: 754.3\n",
      "  Median latency: 0.005303, mean: 0.005303, 99th_p: 0.005326, std_dev: 0.000008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(trt_model, enc_inputs['input_ids'], enc_inputs['token_type_ids'], enc_inputs['attention_mask'])\n",
    "\n",
    "printStats(\"BERT\", timings, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b696de",
   "metadata": {},
   "source": [
    "Benchmark the compiled FP16 model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f61b83fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT =================================\n",
      "batch size=4, num iterations=50\n",
      "  Median text batches/second: 1688.7, mean: 1692.5\n",
      "  Median latency: 0.002369, mean: 0.002363, 99th_p: 0.002384, std_dev: 0.000013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(trt_model_fp16, enc_inputs['input_ids'], enc_inputs['token_type_ids'], enc_inputs['attention_mask'])\n",
    "\n",
    "printStats(\"BERT\", timings, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f67ba3",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. Conclusion\n",
    "\n",
    "In this notebook, we have walked through the complete process of compiling TorchScript models with Torch-TensorRT for Masked Language Modeling with Hugging Face's `bert-base-uncased` transformer and testing the performance impact of the optimization. With Torch-TensorRT on an NVIDIA A100 GPU, we observe the speedups indicated below. These acceleration numbers will vary from GPU to GPU (as well as implementation to implementation based on the ops used) and we encorage you to try out latest generation of Data center compute cards for maximum acceleration.\n",
    "\n",
    "Scripted (GPU): 1.0x\n",
    "Traced (GPU): 1.62x\n",
    "Torch-TensorRT (FP32): 2.14x\n",
    "Torch-TensorRT (FP16): 3.15x\n",
    "\n",
    "### What's next\n",
    "Now it's time to try Torch-TensorRT on your own model. If you run into any issues, you can fill them at https://github.com/pytorch/TensorRT. Your involvement will help future development of Torch-TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd152d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6959f38-81d8-4c52-97a4-614d9760b047",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
