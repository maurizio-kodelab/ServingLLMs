{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab26a81a-8f1a-4f52-ad32-1a16aa9216d7",
   "metadata": {},
   "source": [
    "# Lesson 2 - Batching\n",
    "\n",
    "In this lesson, we'll discuss the concept of \"batching\" in LLM inference.\n",
    "\n",
    "- What is batching?\n",
    "- Throughput vs latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec47940-03d0-4312-b767-cacf262c448f",
   "metadata": {},
   "source": [
    "### Import required packages and load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58366b4d-d8ae-4d54-9db1-98f366838a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ebae63-7e4f-42ce-938e-6171b88b6af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743ac31188fe4abab0dab1d18a08f47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd89ac5beeee44c99bdaefa770e3d57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31dd4fd8ba441a6947ee95524d4fcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b162cc56a154a11bbc141d284977034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07199f0ca55349a5816e2297353084ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25768a4a5474696b9eb3c5810bb46f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df3789e6e394c8c915cc01952e212af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_name = \"./models/gpt2\"\n",
    "model_name = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81202630-61a9-413f-9bcc-793c79c9a759",
   "metadata": {},
   "source": [
    "### Reuse KV-cache text generation function from Lesson 1\n",
    "- Use the same prompt as the previous lesson to verify everything is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cbed1d0-587c-4baa-b212-8da71fa50390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fence and ran to the other side of the fence\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The quick brown fox jumped over the\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "\n",
    "\n",
    "def generate(inputs, max_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_id, past_key_values = \\\n",
    "        generate_token_with_past(next_inputs)\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "                dim=1\n",
    "            ),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens)\n",
    "\n",
    "\n",
    "tokens = generate(inputs, max_tokens=10)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216eccb9-32f4-412c-acb5-f2722a95dbf4",
   "metadata": {},
   "source": [
    "### Add padding tokens to the model to prepare batches of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa74503-ae66-416e-b8bd-1d61e2dbb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc33a3f-686d-45d9-b741-94e328188ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276602a0-4b17-4d00-ad0b-576193b593a3",
   "metadata": {},
   "source": [
    "- Tokenize list of prompts\n",
    "- Add padding so that all prompts have the same number of tokens as the longest prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e18331-0cf4-4223-b2d0-7b3768a88f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple prompts of varying lengths to send\n",
    "# to the model at once\n",
    "prompts = [\n",
    "    \"The quick brown fox jumped over the\",\n",
    "    \"The rain in Spain falls\",\n",
    "    \"What comes up must\",\n",
    "]\n",
    "\n",
    "# note: padding=True ensures the padding token\n",
    "# will be inserted into the tokenized tensors\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf42563-b375-49e6-b22f-bffa1c18faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input_ids:\", inputs[\"input_ids\"])\n",
    "print(\"shape:\", inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cb5c0-7ed5-439a-9ce4-83e394b3aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"attention_mask:\", inputs[\"attention_mask\"])\n",
    "print(\"shape:\", inputs[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5734109-1fe6-418a-9712-ac062f8961a0",
   "metadata": {},
   "source": [
    "- Add position ids to track original order of tokens in each prompt\n",
    "- Padding tokens are set to `1` and then first real token starts with position `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c3699-a86d-4345-b4ff-b0f0ac8bae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position_ids tell the transformer the ordinal position\n",
    "# of each token in the input sequence\n",
    "# for single input inference, this is just [0 .. n]\n",
    "# for n tokens, but for batch inference,\n",
    "# we need to 0 out the padding tokens at the start of the sequence\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "position_ids.masked_fill_(attention_mask == 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7e2d9-4563-4bd3-86ab-0253c736aed1",
   "metadata": {},
   "source": [
    "- Pass tokens to model to calculate logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4e2a8-804e-4b32-87e4-3ca2d382f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as before, but include the position_ids\n",
    "with torch.no_grad():\n",
    "    outputs = model(position_ids=position_ids, **inputs)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0756d61-22d4-43f0-b741-bf3a8d3df69e",
   "metadata": {},
   "source": [
    "- Retrieve most likely token for each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de3048-6bbb-435c-9376-e841d1f14273",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_logits = logits[:, -1, :] \n",
    "next_token_ids = last_logits.argmax(dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a19acb-35bb-4fef-985b-36fb64b400cf",
   "metadata": {},
   "source": [
    "- Print the next token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f933a-c626-414f-bd2f-922c287a01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b0980-f6e5-4c00-845c-102b576dac65",
   "metadata": {},
   "source": [
    "- Convert the token ids into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b80414-3dd7-42f6-ac7d-92607892157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "next_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca75f57-2613-4164-84f5-cf0355b3e264",
   "metadata": {},
   "source": [
    "### Let's put it all together!\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56b4db9-14d9-40c0-9821-ff94504dff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n tokens with past\n",
    "\n",
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8907baf-9d34-4d05-aca7-3fd47943fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all tokens for some max tokens\n",
    "def generate_batch(inputs, max_tokens):\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = \\\n",
    "            generate_batch_tokens_with_past(next_inputs)\n",
    "\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"],\n",
    "                torch.ones((next_token_ids.shape[0], 1)),  \n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return [\"\".join(tokens) for tokens in generated_tokens]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a07baf-1fda-41c4-ab20-94247d725a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the generate_batch function and print out the generated tokens\n",
    "generated_tokens = generate_batch(inputs, max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ec385-ebbd-4458-ad29-15e7deece99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, generated in zip(prompts, generated_tokens):\n",
    "    print(prompt, f\"\\x1b[31m{generated}\\x1b[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790dd60a-228d-48fc-bff0-01983f743aaf",
   "metadata": {},
   "source": [
    "### Throughput vs Latency\n",
    "\n",
    "- Explore the effect of batching on latency (how long it takes to generate each token). \n",
    "- Observe the fundamental tradeoff that exists between throughput and latency.\n",
    "\n",
    "**Note:** Your results might differ somewhat from those shown in the video, but they will still follow the same pattern as explained by the instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12114de7-d7da-4e33-9f6c-48f5406bfb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "max_tokens = 10\n",
    "\n",
    "# observations\n",
    "durations = []\n",
    "throughputs = []\n",
    "latencies = []\n",
    "\n",
    "batch_sizes = [2**p for p in range(8)]\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"bs= {batch_size}\")\n",
    "\n",
    "    # generate tokens for batch and record duration\n",
    "    t0 = time.time()\n",
    "    batch_prompts = [\n",
    "        prompts[i % len(prompts)] for i in range(batch_size)\n",
    "    ]\n",
    "    inputs = tokenizer(\n",
    "        batch_prompts, padding=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    generated_tokens = generate_batch(inputs, max_tokens=max_tokens)\n",
    "    duration_s = time.time() - t0\n",
    "\n",
    "    ntokens = batch_size * max_tokens\n",
    "    throughput = ntokens / duration_s\n",
    "    avg_latency = duration_s / max_tokens\n",
    "    print(\"duration\", duration_s)\n",
    "    print(\"throughput\", throughput)\n",
    "    print(\"avg latency\", avg_latency)    \n",
    "    print()\n",
    "\n",
    "    durations.append(duration_s)\n",
    "    throughputs.append(throughput)\n",
    "    latencies.append(avg_latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7867f-300f-47ac-a34c-7e35e4260b53",
   "metadata": {},
   "source": [
    "### Let's plot the throughput and latency observations against the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc8da6-0969-4b79-a28a-c2fdd5b8af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_plot(x, y1, y2, x_label, y1_label, y2_label):\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot the first line (throughput)\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(y1_label, color=color)\n",
    "    ax1.plot(x, y1, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Set the x-axis to be log-scaled\n",
    "    ax1.set_xscale('log', base=2)\n",
    "\n",
    "    # Instantiate a second axes that shares the same x-axis\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(y2_label, color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(x, y2, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7137d-f5f4-45e5-aa28-d40d6a6eca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Note**: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "render_plot(\n",
    "    batch_sizes,\n",
    "    throughputs,\n",
    "    latencies,\n",
    "    \"Batch Size\",\n",
    "    \"Throughput\",\n",
    "    \"Latency\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92bd38-c3fb-419c-9b3c-27cbfcadf203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39d1bd-91da-4b2c-b7a6-157b9fe08ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a7303-b8cd-443c-bed8-639510690c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
