{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46839b74-a181-4cc3-a27c-b1690d2c03f4",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "Welcome to Lesson 1.\n",
    "\n",
    "To access the `requirements.txt` file, go to `File` and click on `Open`.\n",
    "\n",
    "In this lesson, we'll cover the following:\n",
    "\n",
    "1. How to load an LLM from HuggingFace?\n",
    "2. How to generate a token from the model output tensors?\n",
    "3. Prefill and decode: optimizing token generation over multiple steps\n",
    "\n",
    "I hope you enjoy this course!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776991f7-6366-473f-975a-aa69b3ca2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c3520-1590-43ee-8ec3-6df9b26f0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. How to load an LLM from HuggingFace?\n",
    "\n",
    "- Load the LLM\n",
    "- You'll use GPT2 throughout this course\n",
    "\n",
    "model_name = \"./models/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "- Examine the model's architecture\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76b6a3-998f-4468-9d62-0cabf0a59087",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. How to generate a token from the model output tensors?\n",
    "\n",
    "### Text Generation\n",
    "- Start by tokenizing the input prompt\n",
    "\n",
    "prompt = \"The quick brown fox jumped over the\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3f807-95ec-429a-94ba-40ea8d67fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Pass the inputs to the model and retrieve the logits to find the most likely next token\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "print(logits.shape)\n",
    "\n",
    "last_logits = logits[0, -1, :]\n",
    "next_token_id = last_logits.argmax()\n",
    "next_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d68ef1-e2c8-4c05-9f83-a117ea85d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Decode the most likely token\n",
    "\n",
    "tokenizer.decode(next_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82e0df-4a7b-4e96-93e0-7d578fd8e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Print the 10 most likely next words\n",
    "\n",
    "top_k = torch.topk(last_logits, k=10)\n",
    "tokens = [tokenizer.decode(tk) for tk in top_k.indices]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b703d-7074-4812-bd5d-9f729c9a0ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Concatenate the input and most likely tokens\n",
    "\n",
    "next_inputs = {\n",
    "    \"input_ids\": torch.cat(\n",
    "        [inputs[\"input_ids\"], next_token_id.reshape((1, 1))],\n",
    "        dim=1\n",
    "    ),\n",
    "    \"attention_mask\": torch.cat(\n",
    "        [inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "        dim=1\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(next_inputs[\"input_ids\"],\n",
    "      next_inputs[\"input_ids\"].shape)\n",
    "print(next_inputs[\"attention_mask\"],\n",
    "      next_inputs[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a0d5e-745a-4adb-b6ab-e0f671c018f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Prefill and decode: optimizing token generation over multiple steps\n",
    "\n",
    "### Text generation helper function\n",
    "- The following helper function generates the next tokens given a set of input tokens\n",
    "\n",
    "def generate_token(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b237a-43bd-4b33-b314-83e4e35f9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Use the helper function to generate multiple tokens in a loop\n",
    "- Track the time it takes to generate each token\n",
    "\n",
    "generated_tokens = []\n",
    "next_inputs = inputs\n",
    "durations_s = []\n",
    "for _ in range(10):\n",
    "    t0 = time.time()\n",
    "    next_token_id = generate_token(next_inputs)\n",
    "    durations_s += [time.time() - t0]\n",
    "    \n",
    "    next_inputs = {\n",
    "        \"input_ids\": torch.cat(\n",
    "            [next_inputs[\"input_ids\"], next_token_id.reshape((1, 1))],\n",
    "            dim=1),\n",
    "        \"attention_mask\": torch.cat(\n",
    "            [next_inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "            dim=1),\n",
    "    }\n",
    "    \n",
    "    next_token = tokenizer.decode(next_token_id)\n",
    "    generated_tokens.append(next_token)\n",
    "\n",
    "print(f\"{sum(durations_s)} s\")\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f60c88-7103-409e-9aad-7b8ac466169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Plot token generation time\n",
    "- The x-axis here is the token number\n",
    "- The y-axis is the time to generate a token in millisenconds (ms)\n",
    "**Note**: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "plt.plot(durations_s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3bcc0-bb63-4c96-bf5e-a67cd1fd11a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speeding up text generation with KV-caching\n",
    "KV-caching is a technique to speed up token generation by storing some of the tensors in the attention head for use in subsequent generation steps\n",
    "- Modify the generate helper function to return the next token and the key/value tensors\n",
    "\n",
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e93e74-a9e5-4486-8e6d-effa8954ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Generate 10 tokens using the updated helper function\n",
    "\n",
    "generated_tokens = []\n",
    "next_inputs = inputs\n",
    "durations_cached_s = []\n",
    "for _ in range(10):\n",
    "    t0 = time.time()\n",
    "    next_token_id, past_key_values = \\\n",
    "        generate_token_with_past(next_inputs)\n",
    "    durations_cached_s += [time.time() - t0]\n",
    "    \n",
    "    next_inputs = {\n",
    "        \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "        \"attention_mask\": torch.cat(\n",
    "            [next_inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "            dim=1),\n",
    "        \"past_key_values\": past_key_values,\n",
    "    }\n",
    "    \n",
    "    next_token = tokenizer.decode(next_token_id)\n",
    "    generated_tokens.append(next_token)\n",
    "\n",
    "print(f\"{sum(durations_cached_s)} s\")\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de076d80-d11d-4d61-aed6-cbd9529cc115",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Compare the execution time for the KV-cache function with the original helper function\n",
    "\n",
    "**Note**: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "plt.plot(durations_s)\n",
    "plt.plot(durations_cached_s)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
